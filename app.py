# app.py
# -*- coding: utf-8 -*-
"""
ğŸ§ª Tetkik Analiz ArayÃ¼zÃ¼ â€” Ã‡oklu Dosya (Optimize, Revize)
- Ã‡oklu dosya hÄ±zlÄ± okuma (paralel + cache)
- Bellek optimizasyonu (downcast, categorical)
- Ä°steÄŸe baÄŸlÄ± Polars hÄ±zlandÄ±rma
- BÃ¼yÃ¼k tablolarÄ± gÃ¼venli gÃ¶stermeye yÃ¶nelik limitler
- Grafikler isteÄŸe baÄŸlÄ± (matplotlib; renk set edilmez)
- Kategorik analizlerde SAÄLAM normalizasyon:
Â  Â  â€¢ Kan Grubu: A/B/AB/O/0 + Rh(+/-/poz/neg/rh+/rh-) â†’ tek tipe
Â  Â  â€¢ Anormal Hb: HbS/HbC/HbD/HbE/HbA2â†‘/HbFâ†‘/Normal
- Hem ham yazÄ±mlar hem normalize edilmiÅŸ kategoriler ayrÄ± tablolar/CSV
- Ham yazÄ±mdan hasta/protokol seÃ§erek hastanÄ±n/protokolÃ¼n tÃ¼m tetkiklerini gÃ¶ster

Ã‡alÄ±ÅŸtÄ±rma:
Â  Â  streamlit run app.py
"""

import io
import re
import math
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from scipy import stats
from concurrent.futures import ThreadPoolExecutor

# ============== Ayarlar ============== #
st.set_page_config(page_title="Tetkik Analiz â€” Optimize", layout="wide")

REQ_COLS = ["PROTOKOL_NO", "TCKIMLIK_NO", "TETKIK_ISMI", "TEST_DEGERI", "CINSIYET"]

# Kategorik (metin) testler
CATEGORICAL_TESTS = {"Kan Grubu/", "Anormal Hb/"}

# --- EriÅŸkin pozitiflik eÅŸikleri (TETKIK_ISMI anahtarlarÄ±) ---
THRESHOLDS = {
Â  Â  "HbA2 (%)": (">=", 3.5),
Â  Â  "A2/":Â  Â  Â  (">=", 3.5),
Â  Â  "HbF (%)":Â  (">",Â  2.0),
Â  Â  "F/":Â  Â  Â  Â (">",Â  2.0),
Â  Â  "HbS (%)":Â  (">",Â  0.0),
Â  Â  "HbC (%)":Â  (">",Â  0.0),
Â  Â  "HbD (%)":Â  (">",Â  0.0),
Â  Â  "HbE (%)":Â  (">",Â  0.0),
}
GT_ZERO_DEFAULT = {
Â  Â  "HbS (%)","HbC (%)","HbD (%)","HbE (%)","HbF (%)","HbA2 (%)","A2/","F/",
Â  Â  "C/","D/","E/","S/"
}
VARIANT_NUMERIC_TESTS = {
Â  Â  "HbS (%)","HbC (%)","HbD (%)","HbE (%)","HbF (%)","HbA2 (%)","Anormal Hb/"
}

DISPLAY_LIMIT = 400

MALE_TOKENSÂ  Â = {"e","erkek","m","male","bay"}
FEMALE_TOKENS = {"k","kadÄ±n","kadin","f","female","bayan"}

# Polars mevcut mu?
try:
Â  Â  import polars as pl
Â  Â  HAS_POLARS = True
except Exception:
Â  Â  HAS_POLARS = False


# ============== YardÄ±mcÄ±lar ============== #
def coerce_numeric(series: pd.Series) -> pd.Series:
Â  Â  s = series.astype(str).str.replace(",", ".", regex=False).str.replace(" ", "", regex=False)
Â  Â  return pd.to_numeric(s, errors="coerce")

# ----- P deÄŸeri yazÄ±m kuralÄ± (TÃ¼rkÃ§e ondalÄ±k) -----
def _fmt_p(p: float) -> str:
Â  Â  if p is None or np.isnan(p):
Â  Â  Â  Â  return "â€”"
Â  Â  if p < 0.001:
Â  Â  Â  Â  return "<0,001"
Â  Â  if p < 0.05:
Â  Â  Â  Â  return "<0,05"
Â  Â  return f"{p:.3f}".replace(".", ",")

# ----- Normalite testi: n<=5000 Shapiro; bÃ¼yÃ¼k n KS (N(Î¼,Ïƒ)) -----
def normality_test_with_p(series: pd.Series, alpha: float = 0.05):
Â  Â  x = pd.to_numeric(series, errors="coerce").dropna()
Â  Â  n = len(x)
Â  Â  if n < 3:
Â  Â  Â  Â  return "yetersiz", "â€”"

Â  Â  try:
Â  Â  Â  Â  if n <= 5000:
Â  Â  Â  Â  Â  Â  stat, p = stats.shapiro(x)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  mu = float(np.mean(x))
Â  Â  Â  Â  Â  Â  sd = float(np.std(x, ddof=1))
Â  Â  Â  Â  Â  Â  if sd == 0:
Â  Â  Â  Â  Â  Â  Â  Â  return "yetersiz", "â€”"
Â  Â  Â  Â  Â  Â  # H0: veri ~ N(mu, sd)
Â  Â  Â  Â  Â  Â  stat, p = stats.kstest(x, 'norm', args=(mu, sd))

Â  Â  Â  Â  label = "normal" if p >= alpha else "non-normal"
Â  Â  Â  Â  return label, _fmt_p(p)
Â  Â  except Exception:
Â  Â  Â  Â  return "bilinmiyor", "â€”"

def add_numeric_copy(frame, src_col="TEST_DEGERI", out_col="__VAL_NUM__"):
Â  Â  if out_col not in frame.columns:
Â  Â  Â  Â  tmp = (frame[src_col].astype(str)
Â  Â  Â  Â  Â  Â  Â  Â .str.replace(",", ".", regex=False)
Â  Â  Â  Â  Â  Â  Â  Â .str.replace(" ", "", regex=False))
Â  Â  Â  Â  frame[out_col] = pd.to_numeric(tmp, errors="coerce")
Â  Â  return frame


def check_columns(df: pd.DataFrame):
Â  Â  return [c for c in REQ_COLS if c not in df.columns]


def normalize_sex_label(value):
Â  Â  if not isinstance(value, str): return None
Â  Â  trimmed = value.strip()
Â  Â  if not trimmed: return None
Â  Â  low = trimmed.lower()
Â  Â  if low in MALE_TOKENS: return "Erkek"
Â  Â  if low in FEMALE_TOKENS: return "KadÄ±n"
Â  Â  return trimmed


def _resolve_patient_sex(series: pd.Series) -> str:
Â  Â  values = [v for v in pd.unique(series.dropna()) if isinstance(v, str) and v]
Â  Â  if not values: return "Bilinmiyor"
Â  Â  if len(values) == 1: return values[0]
Â  Â  return "Ã‡akÄ±ÅŸma"


def summarize_sex_counts(frame: pd.DataFrame) -> pd.DataFrame:
Â  Â  tmp = frame[["TCKIMLIK_NO", "CINSIYET"]].copy()
Â  Â  tmp["CINSIYET"] = tmp["CINSIYET"].astype(str)
Â  Â  tmp["__SEX_CANON__"] = tmp["CINSIYET"].map(normalize_sex_label).astype(object)
Â  Â  s_rows = tmp["__SEX_CANON__"].where(tmp["__SEX_CANON__"].notna(), "Bilinmiyor")
Â  Â  row_counts = (
Â  Â  Â  Â  s_rows.value_counts(dropna=False)
Â  Â  Â  Â  .rename_axis("CINSIYET").to_frame("SatÄ±r SayÄ±sÄ±")
Â  Â  )
Â  Â  with_id = tmp[tmp["TCKIMLIK_NO"].notna()].copy()
Â  Â  if not with_id.empty:
Â  Â  Â  Â  w = with_id.copy()
Â  Â  Â  Â  w["__SEX_CANON__"] = w["__SEX_CANON__"].astype(object)
Â  Â  Â  Â  patient_gender = (
Â  Â  Â  Â  Â  Â  w.groupby("TCKIMLIK_NO")["__SEX_CANON__"]
Â  Â  Â  Â  Â  Â  Â .apply(lambda s: _resolve_patient_sex(pd.Series(pd.unique(s.dropna()))))
Â  Â  Â  Â  Â  Â  Â .reset_index(name="__SEX_RESOLVED__")
Â  Â  Â  Â  )
Â  Â  Â  Â  patient_counts = (
Â  Â  Â  Â  Â  Â  patient_gender["__SEX_RESOLVED__"].fillna("Bilinmiyor")
Â  Â  Â  Â  Â  Â  .value_counts(dropna=False)
Â  Â  Â  Â  Â  Â  .rename_axis("CINSIYET").to_frame("Hasta (Benzersiz)")
Â  Â  Â  Â  )
Â  Â  else:
Â  Â  Â  Â  patient_counts = pd.DataFrame(columns=["Hasta (Benzersiz)"])
Â  Â  summary = row_counts.join(patient_counts, how="outer").fillna(0)
Â  Â  summary["SatÄ±r SayÄ±sÄ±"] = summary["SatÄ±r SayÄ±sÄ±"].astype(int)
Â  Â  if "Hasta (Benzersiz)" in summary.columns:
Â  Â  Â  Â  summary["Hasta (Benzersiz)"] = summary["Hasta (Benzersiz)"].astype(int)
Â  Â  else:
Â  Â  Â  Â  summary["Hasta (Benzersiz)"] = 0
Â  Â  total_rows = int(summary["SatÄ±r SayÄ±sÄ±"].sum())
Â  Â  total_patients = int(summary["Hasta (Benzersiz)"].sum())
Â  Â  summary["% SatÄ±r"]Â  = (summary["SatÄ±r SayÄ±sÄ±"] / total_rows * 100).round(2) if total_rows else np.nan
Â  Â  summary["% Hasta"] = (summary["Hasta (Benzersiz)"] / total_patients * 100).round(2) if total_patients else np.nan
Â  Â  summary = summary.reset_index()
Â  Â  summary = summary[["CINSIYET","Hasta (Benzersiz)","% Hasta","SatÄ±r SayÄ±sÄ±","% SatÄ±r"]]
Â  Â  return summary.sort_values("Hasta (Benzersiz)", ascending=False).reset_index(drop=True)


def downcast_df(df: pd.DataFrame) -> pd.DataFrame:
Â  Â  if "TEST_DEGERI" in df.columns:
Â  Â  Â  Â  df["TEST_DEGERI"] = df["TEST_DEGERI"].astype(str)
Â  Â  for col in ["TETKIK_ISMI", "CINSIYET", "SOURCE_FILE"]:
Â  Â  Â  Â  if col in df.columns:
Â  Â  Â  Â  Â  Â  df[col] = df[col].astype("category")
Â  Â  return df


def descr_stats_fast(x: pd.Series) -> dict:
Â  Â  x = pd.to_numeric(x, errors="coerce")
Â  Â  x = x[~x.isna()]
Â  Â  if x.empty:
Â  Â  Â  Â  return {"count":0,"mean":np.nan,"std":np.nan,"min":np.nan,"q1":np.nan,"median":np.nan,"q3":np.nan,"max":np.nan,"cv%":np.nan,"iqr":np.nan}
Â  Â  q = np.percentile(x, [25, 50, 75])
Â  Â  mean = float(x.mean())
Â  Â  std = float(x.std(ddof=1)) if len(x) > 1 else 0.0
Â  Â  cvÂ  = (std/mean)*100 if mean!=0 else np.nan
Â  Â  return {"count":int(x.size),"mean":mean,"std":std,"min":float(x.min()),"q1":float(q[0]),"median":float(q[1]),"q3":float(q[2]),"max":float(x.max()),"cv%":float(cv),"iqr":float(q[2]-q[0])}


def normality_flag(x: pd.Series, alpha=0.05) -> str:
Â  Â  x = pd.to_numeric(x, errors="coerce").dropna()
Â  Â  if len(x) < 3: return "yetersiz"
Â  Â  try:
Â  Â  Â  Â  if len(x) <= 5000:
Â  Â  Â  Â  Â  Â  stat, p = stats.shapiro(x)
Â  Â  Â  Â  Â  Â  return "normal" if p >= alpha else "non-normal"
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  res = stats.anderson(x, dist="norm")
Â  Â  Â  Â  Â  Â  crit = res.critical_values[2]
Â  Â  Â  Â  Â  Â  return "normal" if res.statistic < crit else "non-normal"
Â  Â  except Exception:
Â  Â  Â  Â  return "bilinmiyor"


def apply_threshold(series, rule):
Â  Â  op, cut = rule
Â  Â  if op == ">=": return series >= cut
Â  Â  if op == ">":Â  return series >Â  cut
Â  Â  if op == "<=": return series <= cut
Â  Â  if op == "<":Â  return series <Â  cut
Â  Â  return series.notna()


def nonparametric_test_by_group(df, val_col, grp_col):
Â  Â  groups = [g.dropna() for _, g in df.groupby(grp_col)[val_col]]
Â  Â  groups = [pd.to_numeric(g, errors="coerce").dropna() for g in groups]
Â  Â  groups = [g for g in groups if len(g) > 0]
Â  Â  unique_groups = df[grp_col].dropna().unique()
Â  Â  unique_groups = [g for g in unique_groups if df[df[grp_col] == g][val_col].notna().sum() > 0]
Â  Â  if len(unique_groups) < 2:
Â  Â  Â  Â  return "KarÅŸÄ±laÅŸtÄ±rma iÃ§in en az 2 grup gerekli.", None
Â  Â  if len(unique_groups) == 2:
Â  Â  Â  Â  gnames = list(unique_groups)
Â  Â  Â  Â  x = pd.to_numeric(df[df[grp_col] == gnames[0]][val_col], errors="coerce").dropna()
Â  Â  Â  Â  y = pd.to_numeric(df[df[grp_col] == gnames[1]][val_col], errors="coerce").dropna()
Â  Â  Â  Â  if len(x) >= 1 and len(y) >= 1:
Â  Â  Â  Â  Â  Â  stat, p = stats.mannwhitneyu(x, y, alternative="two-sided")
Â  Â  Â  Â  Â  Â  return f"Mannâ€“Whitney U: U={stat:.2f}, p={p:.4g} ({gnames[0]} vs {gnames[1]})", ("MWU", stat, p, gnames[0], gnames[1])
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  return "Gruplarda yeterli gÃ¶zlem yok.", None
Â  Â  else:
Â  Â  Â  Â  stat, p = stats.kruskal(*groups)
Â  Â  Â  Â  return f"Kruskalâ€“Wallis: H={stat:.2f}, p={p:.4g} (grup sayÄ±sÄ±: {len(unique_groups)})", ("KW", stat, p, unique_groups)


def make_boxplot(df, x_col, y_col, title="Kutu GrafiÄŸi"):
Â  Â  valid = df[[x_col, y_col]].copy()
Â  Â  valid[y_col] = pd.to_numeric(valid[y_col], errors="coerce")
Â  Â  valid = valid.dropna()
Â  Â  if valid.empty:
Â  Â  Â  Â  st.info("Grafik iÃ§in yeterli veri yok."); return
Â  Â  cats = list(valid[x_col].astype(str).unique())
Â  Â  data = [valid[valid[x_col].astype(str) == c][y_col].values for c in cats]
Â  Â  fig, ax = plt.subplots()
Â  Â  ax.boxplot(data, labels=cats, showmeans=True)
Â  Â  ax.set_title(title); ax.set_xlabel(x_col); ax.set_ylabel(y_col)
Â  Â  st.pyplot(fig)


def make_hist(df, col, bins=30, title="Histogram"):
Â  Â  x = pd.to_numeric(df[col], errors="coerce").dropna()
Â  Â  if x.empty:
Â  Â  Â  Â  st.info("Histogram iÃ§in yeterli veri yok."); return
Â  Â  fig, ax = plt.subplots()
Â  Â  ax.hist(x, bins=bins)
Â  Â  ax.set_title(title); ax.set_xlabel(col); ax.set_ylabel("Frekans")
Â  Â  st.pyplot(fig)


def export_df(df, name="export.csv"):
Â  Â  csv = df.to_csv(index=False).encode("utf-8-sig")
Â  Â  st.download_button("â¬‡ï¸ CSV indir", data=csv, file_name=name, mime="text/csv")


# ======== Ã–ZEL: Kategorik normalizasyon fonksiyonlarÄ± ======== #
def normalize_blood_group(x: str | None):
Â  Â  """
Â  Â  'A Rh (+) Pozitif' -> 'A Rh(+)', 'O Rh -' -> 'O Rh(-)', '0 +' -> 'O Rh(+)'
Â  Â  metin anlaÅŸÄ±lmazsa None dÃ¶ner.
Â  Â  """
Â  Â  if not isinstance(x, str): return None
Â  Â  u = x.strip().upper().replace("Ä°", "I")
Â  Â  if not u: return None

Â  Â  # ABO (AB, A, B, O/0)
Â  Â  abo = None
Â  Â  if re.search(r"\bAB\b", u):
Â  Â  Â  Â  abo = "AB"
Â  Â  elif re.search(r"\bA\b", u):
Â  Â  Â  Â  abo = "A"
Â  Â  elif re.search(r"\bB\b", u):
Â  Â  Â  Â  abo = "B"
Â  Â  elif re.search(r"\bO\b|\b0\b", u):
Â  Â  Â  Â  abo = "O"

Â  Â  # Rh (+ / - / POS/POZ / NEG / RH+ / RH- / + / -)
Â  Â  rh = None
Â  Â  if re.search(r"\+|\bPOS(ITIVE)?\b|\bPOZ(ITIF)?\b|\bRH\+\b", u):
Â  Â  Â  Â  rh = "Rh(+)"
Â  Â  elif re.search(r"-|\bNEG(ATIVE)?\b|\bRH-\b", u):
Â  Â  Â  Â  rh = "Rh(-)"

Â  Â  if abo is None and rh is None:
Â  Â  Â  Â  return None
Â  Â  return f"{abo or ''} {rh or ''}".strip()


def norm_anormal_hb_text(x: str | None):
Â  Â  if not isinstance(x, str): return None
Â  Â  s = x.upper().replace("Ä°","I").strip()
Â  Â Â 
Â  Â  # YENÄ° EKLENEN KONTROL (Genellikle USV veya benzeri metinleri yakalar)
Â  Â  if re.search(r"\bUSV\b|UNIDENTIFIED|TANIMLANAMAYAN", s): return "USV"
Â  Â Â 
Â  Â  # --- MEVCUT KOD ---
Â  Â  if re.search(r"S-?BETA|S ?Î²", s): return "Hb S-Î²-thal"
Â  Â  if re.search(r"\bHBS\b|S TRAIT|S HET|HBS HET|HBS TAS|S-TASIY", s): return "HbS"
Â  Â  if re.search(r"\bHBC\b", s): return "HbC"
Â  Â  if re.search(r"\bHBD\b", s): return "HbD"
Â  Â  if re.search(r"\bHBE\b", s): return "HbE"
Â  Â  if re.search(r"\bA2\b|HBA2", s): return "HbA2â†‘"
Â  Â  if re.search(r"\bF\b|HBF", s): return "HbFâ†‘"
Â  Â  if re.search(r"\bNORMAL\b|NEG", s): return "Normal"
Â  Â  return None


# ============== Cache'li Dosya Okuma ============== #
@st.cache_data(show_spinner=False)
def read_one_excel_cached(file_bytes: bytes, engine_hint: str = "openpyxl") -> pd.DataFrame:
Â  Â  bio = io.BytesIO(file_bytes)
Â  Â  return pd.read_excel(bio, engine=engine_hint)


def read_many_excels(files):
Â  Â  def _read(upl):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  data = upl.read()
Â  Â  Â  Â  Â  Â  df = read_one_excel_cached(data)
Â  Â  Â  Â  Â  Â  return (upl.name, df, None)
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  return (upl.name, None, str(e))
Â  Â  out = []
Â  Â  with ThreadPoolExecutor(max_workers=min(8, len(files))) as ex:
Â  Â  Â  Â  for name, df, err in ex.map(_read, files):
Â  Â  Â  Â  Â  Â  out.append((name, df, err))
Â  Â  return out


# ============== UI BaÅŸlangÄ±Ã§ ============== #
st.title("âš¡ Tetkik Analiz â€” Ã‡oklu Dosya (Optimize, Revize)")
st.caption("BÃ¼yÃ¼k veri ve Ã§oklu dosya iÃ§in hÄ±zlandÄ±rÄ±lmÄ±ÅŸ sÃ¼rÃ¼m (kan grubu/anormal Hb normalizasyonu dÃ¢hil).")

uploads = st.file_uploader("Excel dosyalarÄ± (.xlsx, .xls) â€” Ã‡oklu seÃ§im", type=["xlsx", "xls"], accept_multiple_files=True)

use_polars = st.checkbox("Polars hÄ±zlandÄ±rmayÄ± dene (kuruluysa)", value=('pl' in globals() and HAS_POLARS),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â help="Polars kurulu deÄŸilse otomatik devre dÄ±ÅŸÄ± kalÄ±r.")

if not uploads:
Â  Â  st.info("Birden Ã§ok dosyayÄ± aynÄ± anda seÃ§in (Ã¶rn. 12 dosya).")
Â  Â  st.stop()

with st.spinner("Dosyalar okunuyor..."):
Â  Â  results = read_many_excels(uploads)

frames, skipped = [], []
for name, tmp, err in results:
Â  Â  if err:
Â  Â  Â  Â  skipped.append((name, f"Okuma hatasÄ±: {err}")); continue
Â  Â  miss = check_columns(tmp)
Â  Â  if miss:
Â  Â  Â  Â  skipped.append((name, f"Eksik sÃ¼tun: {miss}")); continue
Â  Â  tmp["SOURCE_FILE"] = name
Â  Â  frames.append(tmp)

if skipped:
Â  Â  for nm, msg in skipped:
Â  Â  Â  Â  st.warning(f"'{nm}' atlandÄ± â†’ {msg}")

if not frames:
Â  Â  st.error("Uygun veri iÃ§eren dosya bulunamadÄ±."); st.stop()

df = pd.concat(frames, ignore_index=True)
df = downcast_df(df)

if use_polars and HAS_POLARS:
Â  Â  try: pl_df = pl.from_pandas(df)
Â  Â  except Exception:Â 
Â  Â  Â  Â  use_polars, pl_df = False, None
else:
Â  Â  pl_df = None


# ================= Filtreler ================= #
left, right = st.columns([3, 2])
with left:
Â  Â  unique_tests = sorted([str(x) for x in df["TETKIK_ISMI"].dropna().unique()])
Â  Â  selected_tests = st.multiselect("Analiz edilecek tetkikler", options=unique_tests, default=unique_tests)
with right:
Â  Â  sexes = [str(x) for x in df["CINSIYET"].dropna().unique()]
Â  Â  chosen_sex = st.multiselect("Cinsiyet filtresi", options=sexes, default=sexes)
Â  Â  files = [str(x) for x in df["SOURCE_FILE"].dropna().unique()]
Â  Â  chosen_files = st.multiselect("Dosya filtresi", options=files, default=files)

# --- 99 ile baÅŸlayan TCKN filtreleme kontrolÃ¼ ---
st.markdown("### ğŸ§¾ TCKN Filtre SeÃ§imi")

tckn_filter = st.selectbox(
Â  Â  "TCKN filtrele:",
Â  Â  ["Hepsi", "Sadece gerÃ§ek TCKN", "Sadece 99'lu TCKN"],
Â  Â  index=1,Â  # VarsayÄ±lan: Sadece gerÃ§ek TCKN
Â  Â  help="99 ile baÅŸlayanlar genelde geÃ§ici kayÄ±tlardÄ±r."
)

work = df.copy()
# --- TCKN filtreleme ---
if "TCKIMLIK_NO" in work.columns:
Â  Â  tckn_str = work["TCKIMLIK_NO"].astype(str)

Â  Â  if tckn_filter == "Sadece gerÃ§ek TCKN":
Â  Â  Â  Â  work = work[~tckn_str.str.startswith("99", na=False)]

Â  Â  elif tckn_filter == "Sadece 99'lu TCKN":
Â  Â  Â  Â  work = work[tckn_str.str.startswith("99", na=False)]

if chosen_sex:
Â  Â  work = work[work["CINSIYET"].astype(str).isin(chosen_sex)]
if chosen_files:
Â  Â  work = work[work["SOURCE_FILE"].astype(str).isin(chosen_files)]
if selected_tests:
Â  Â  work = work[work["TETKIK_ISMI"].astype(str).isin(selected_tests)]

# GÃ¼vence: numeric kopya olsun
work = add_numeric_copy(work)


# ================= VARYANT Ã–ZETÄ° (etiketleme) ================= #
A2_KEYS = {"A2/","HbA2","HbA2 (%)","Hb A2","Hb A2 (%)"}
F_KEYSÂ  = {"F/","HbF","HbF (%)","Hb F","Hb F (%)"}
NUMVAR_FROM_TEST = {"C/":"HbC", "D/":"HbD", "E/":"HbE", "S/":"HbS"}

def pick_variant_tag(g: pd.DataFrame) -> str | None:
Â  Â  g = add_numeric_copy(g.copy())
Â  Â  g["TETKIK_ISMI"] = g["TETKIK_ISMI"].astype(str)
Â  Â Â 
Â  Â  # YENÄ° - ADIM 1: Ã–nce CLEAN (temizlenmiÅŸ) sÃ¼tununa bak
Â  Â  # KullanÄ±cÄ± "DÃ¼zenlenebilir tablo"ya "USV" gibi bir deÄŸer yazdÄ±ysa,
Â  Â  # bu, diÄŸer tÃ¼m kurallarÄ± ezer (en yÃ¼ksek Ã¶ncelik).
Â  Â  clean_col = "ANORMAL_HB_CLEAN"
Â  Â  if clean_col in g.columns:
Â  Â  Â  Â  # Bu protokole ait CLEAN deÄŸerlerini al
Â  Â  Â  Â  clean_values = g[clean_col].dropna().astype(str)
Â  Â  Â  Â  clean_values = clean_values[clean_values != ""]
Â  Â  Â  Â  if not clean_values.empty:
Â  Â  Â  Â  Â  Â  # TemizlenmiÅŸ deÄŸeri (Ã¶rn. "USV", "HbS", "HbC") doÄŸrudan etiket olarak dÃ¶ndÃ¼r
Â  Â  Â  Â  Â  Â  return clean_values.iloc[0]Â 

Â  Â  # ADIM 2: CLEAN yoksa, eski mantÄ±ÄŸa (otomatik sÄ±nÄ±flandÄ±rma) devam et
Â  Â  tags = []
Â  Â Â 
Â  Â  # 1) Anormal Hb/ metinlerinden (norm_anormal_hb_text "USV"yi zaten tanÄ±yor olmalÄ±)
Â  Â  txt = g.loc[g["TETKIK_ISMI"] == "Anormal Hb/", "TEST_DEGERI"].dropna().astype(str)
Â  Â  for v in txt:
Â  Â  Â  Â  t = norm_anormal_hb_text(v)
Â  Â  Â  Â  if t: tags.append(t)
Â  Â  Â  Â Â 
Â  Â  # 2) A2/F eriÅŸkin eÅŸikleri
Â  Â  if g["TETKIK_ISMI"].isin(A2_KEYS).any():
Â  Â  Â  Â  a2 = g.loc[g["TETKIK_ISMI"].isin(A2_KEYS), "__VAL_NUM__"].dropna()
Â  Â  Â  Â  if not a2.empty and a2.max() >= 3.5: tags.append("HbA2â†‘")
Â  Â  if g["TETKIK_ISMI"].isin(F_KEYS).any():
Â  Â  Â  Â  f = g.loc[g["TETKIK_ISMI"].isin(F_KEYS), "__VAL_NUM__"].dropna()
Â  Â  Â  Â  if not f.empty and f.max() > 2.0: tags.append("HbFâ†‘")
Â  Â  Â  Â Â 
Â  Â  # 3) HPLC pikleri
Â  Â  for k, var_name in NUMVAR_FROM_TEST.items():
Â  Â  Â  Â  m = g["TETKIK_ISMI"] == k
Â  Â  Â  Â  if m.any():
Â  Â  Â  Â  Â  Â  vv = g.loc[m, "__VAL_NUM__"].dropna()
Â  Â  Â  Â  Â  Â  if not vv.empty and (vv > 0).any():
Â  Â  Â  Â  Â  Â  Â  Â  tags.append(var_name)
Â  Â Â 
Â  Â  if not tags: return None
Â  Â  # Ã–ncelik listesi (USV'yi iÃ§ermeli)
Â  Â  for p in ["Hb S-Î²-thal","HbS","HbC","HbD","HbE","USV","HbA2â†‘","HbFâ†‘","Normal"]:
Â  Â  Â  Â  if p in tags: return p
Â  Â  return tags[0]

if "VARIANT_TAG" not in work.columns:
Â  Â  var_map = (work.groupby("PROTOKOL_NO", group_keys=False)
Â  Â  Â  Â  Â  Â  Â  Â  Â  .apply(lambda g: pd.Series({"VARIANT_TAG": pick_variant_tag(g)}))
Â  Â  Â  Â  Â  Â  Â  Â  Â  .reset_index())
Â  Â  work = work.merge(var_map, on="PROTOKOL_NO", how="left")

st.header("ğŸ“‹ Varyant Ã–zeti â€” eriÅŸkin eÅŸikleri ile")
present = [t for t in ["Hb S-Î²-thal","HbS","HbC","HbD","HbE","HbA2â†‘","HbFâ†‘","Normal"]
Â  Â  Â  Â  _if t in set(work["VARIANT_TAG"].dropna())]
variant_choice = st.selectbox("Varyant seÃ§:", ["(TÃ¼mÃ¼)"] + present, index=0)

base_v = work.copy()
if variant_choice != "(TÃ¼mÃ¼)":
Â  Â  base_v = base_v[base_v["VARIANT_TAG"] == variant_choice]

# 1) TÃ¼mÃ¼ iÃ§in frekans
if variant_choice == "(TÃ¼mÃ¼)":
Â  Â  freq = (work["VARIANT_TAG"].value_counts(dropna=True)
Â  Â  Â  Â  Â  Â  .rename_axis("Varyant").to_frame("N").reset_index())
Â  Â  total = int(freq["N"].sum()) if not freq.empty else 0
Â  Â  if total > 0: freq["%"] = (freq["N"]/total*100).round(2)
Â  Â  st.subheader("Varyant FrekanslarÄ±")
Â  Â  st.dataframe(freq, use_container_width=True)
Â  Â  st.download_button("â¬‡ï¸ Varyant frekanslarÄ± (CSV)",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â data=freq.to_csv(index=False).encode("utf-8-sig"),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â file_name="varyant_frekans.csv", mime="text/csv")

# 2) SeÃ§ilen varyant iÃ§in â™€/â™‚ Mean Â± SD tablosu
def _mean_sd(s: pd.Series):
Â  Â  s = pd.to_numeric(s, errors="coerce").dropna()
Â  Â  return "â€”" if s.empty else f"{s.mean():.2f} Â± {s.std(ddof=1):.2f}"

# --- YENÄ° VE GENÄ°ÅLETÄ°LMÄ°Å HALÄ° ---
PARAMS = {
Â  Â  # --- Hemogram Parametreleri ---
Â  Â  "Hemogram/HGB":Â  ("Hb (g/dL)",Â  Â  "F: 11â€“15; M: 12â€“17"),
Â  Â  "Hemogram/HCT":Â  ("HCT (%)",Â  Â  Â  "F: 36â€“46; M: 40â€“53"),
Â  Â  "Hemogram/RBC":Â  ("RBC (Ã—10â¶)",Â  Â "F: 3.9â€“5.6; M: 4.5â€“6.0"),
Â  Â  "Hemogram/RDW":Â  ("RDW (%)",Â  Â  Â  "11â€“16"),
Â  Â  "Hemogram/MCV":Â  ("MCV (fL)",Â  Â  Â "80â€“100"),
Â  Â  "Hemogram/MCH":Â  ("MCH (pg)",Â  Â  Â "27â€“34"),
Â  Â  "Hemogram/MCHC": ("MCHC (g/dL)", "32â€“36"),
Â  Â  # --- Buraya diÄŸer hemogram parametrelerini ekleyin (Ã–RN) ---
Â  Â  "Hemogram/PLT":Â  ("PLT (Ã—10Â³)",Â  Â "150-450"),
Â  Â  "Hemogram/WBC":Â  ("WBC (Ã—10Â³)",Â  Â "4.0-11.0"),
Â  Â Â 
Â  Â  # --- HPLC Parametreleri (Mevcut) ---
Â  Â  "Talasemi(HPLC) (A0)/":Â  Â  Â  Â  Â  Â ("HbA0 (%)",Â  Â  Â  "94â€“98"),
Â  Â  "HbA0 (%)":Â  Â  Â  ("HbAâ‚‚ (%)",Â  Â  Â "94â€“98"),
Â  Â  "A0/":Â  Â  Â  Â  Â  Â ("HbAâ‚‚ (%)",Â  Â  Â "94â€“98"), # A0 iÃ§in alternatif isim
Â  Â  "HbA":Â  Â  Â  Â  Â  Â ("HbA (%)",Â  Â  Â  "94â€“98"),
Â  Â  "HbA2 (%)":Â  Â  Â  ("HbAâ‚‚ (%)",Â  Â  Â "2â€“3.5"),
Â  Â  "A2/":Â  Â  Â  Â  Â  Â ("HbAâ‚‚ (%)",Â  Â  Â "2â€“3.5"), # A2 iÃ§in alternatif isim
Â  Â  "HbF (%)":Â  Â  Â  Â ("Hb F (%)",Â  Â  Â "0â€“2"),
Â  Â  "F/":Â  Â  Â  Â  Â  Â  ("Hb F (%)",Â  Â  Â "0â€“2"),Â  Â # F iÃ§in alternatif isim
Â  Â Â 
Â  Â  # --- YENÄ° EKLENEN HPLC VARYANTLARI ---
Â  Â  "HbS (%)":Â  Â  Â  Â ("HbS (%)",Â  Â  Â  "0"),
Â  Â  "S/":Â  Â  Â  Â  Â  Â  ("HbS (%)",Â  Â  Â  "0"),Â  Â # S iÃ§in alternatif isim
Â  Â  "HbC (%)":Â  Â  Â  Â ("HbC (%)",Â  Â  Â  "0"),
Â  Â  "C/":Â  Â  Â  Â  Â  Â  ("HbC (%)",Â  Â  Â  "0"),Â  Â # C iÃ§in alternatif isim
Â  Â  "HbD (%)":Â  Â  Â  Â ("HbD (%)",Â  Â  Â  "0"),
Â  Â  "D/":Â  Â  Â  Â  Â  Â  ("HbD (%)",Â  Â  Â  "0"),Â  Â # D iÃ§in alternatif isim
Â  Â  "HbE (%)":Â  Â  Â  Â ("HbE (%)",Â  Â  Â  "0"),
Â  Â  "E/":Â  Â  Â  _Â  Â  Â  ("HbE (%)",Â  Â  Â  "0"),Â  Â # E iÃ§in alternatif isim
Â  Â  # YENÄ° EKLENEN USV SATIRI (EÄŸer verinizde "USV/" gibi bir test ismi varsa)
Â  Â  "USV/":Â  Â  Â  Â  Â  ("USV (%)",Â  Â  Â  "â€”"),
Â  Â  "USV (%)":Â  Â  Â  Â ("USV (%)",Â  Â  Â  "â€”"),
}

table_fm = pd.DataFrame()
if variant_choice != "(TÃ¼mÃ¼)":
Â  Â  rows = []
Â  Â  for tetkik_key, (disp, ref) in PARAMS.items():
Â  Â  Â  Â  subp = base_v[base_v["TETKIK_ISMI"] == tetkik_key].copy()
Â  Â  Â  Â  if subp.empty:Â 
Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  subp = add_numeric_copy(subp)Â  # __VAL_NUM__ gÃ¼vence
Â  Â  Â  Â  fem = _mean_sd(subp.loc[subp["CINSIYET"].astype(str).str.lower().str.startswith(("k","f")), "__VAL_NUM__"])
Â  Â  Â  Â  male= _mean_sd(subp.loc[subp["CINSIYET"].astype(str).str.lower().str.startswith(("e","m")), "__VAL_NUM__"])
Â  Â  Â  Â  rows.append({"Parameter": disp, "Female (Mean Â± SD)": fem, "Male (Mean Â± SD)": male, "Reference range": ref})
Â  Â  table_fm = pd.DataFrame(rows)
Â  Â  st.subheader("â™€/â™‚ Mean Â± SD (seÃ§ilen varyant)")
Â  Â  if table_fm.empty:
Â  Â  Â  Â  st.info("Bu varyant iÃ§in parametrik veri bulunamadÄ±.")
Â  Â  else:
Â  Â  Â  Â  st.dataframe(table_fm, use_container_width=True)
Â  Â  Â  Â  st.download_button("â¬‡ï¸ Tablo #1 (CSV)",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â data=table_fm.to_csv(index=False).encode("utf-8-sig"),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â file_name="varyant_ozet_female_male.csv", mime="text/csv")

# 3) BirleÅŸik tablo (Varyant FrekanslarÄ± + MeanÂ±SD)
if variant_choice != "(TÃ¼mÃ¼)":
Â  Â  freq_part = locals().get("freq", pd.DataFrame(columns=["Varyant","N","%"])).copy()
Â  Â  if not freq_part.empty:
Â  Â  Â  Â  freq_part = freq_part.rename(columns={"Varyant":"BaÅŸlÄ±k"})
Â  Â  Â  Â  freq_part.insert(0,"BÃ¶lÃ¼m","Varyant FrekanslarÄ±")
Â  Â  msd_part = table_fm.copy()
Â  Â  if not msd_part.empty:
Â  Â  Â  Â  msd_part = msd_part.rename(columns={"Parameter":"BaÅŸlÄ±k"})
Â  Â  Â  Â  msd_part.insert(0,"BÃ¶lÃ¼m","â™€/â™‚ Mean Â± SD")
Â  Â  cols = ["BÃ¶lÃ¼m","BaÅŸlÄ±k","N","%","Female (Mean Â± SD)","Male (Mean Â± SD)","Reference range"]
Â  Â  for dfc in (freq_part, msd_part):
Â  Â  Â  Â  for c in cols:
Â  Â  Â  Â  Â  Â  if c not in dfc.columns: dfc[c] = None
Â  Â  combined_df = pd.concat([freq_part[cols], msd_part[cols]], ignore_index=True)
Â  Â  st.subheader("ğŸ§© BirleÅŸik Tablo")
Â  Â  st.dataframe(combined_df, use_container_width=True)
Â  Â  st.download_button("â¬‡ï¸ BirleÅŸik tablo (CSV)",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â data=combined_df.to_csv(index=False).encode("utf-8-sig"),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â file_name=f"birlesik_{variant_choice}.csv",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â mime="text/csv")


# ================= Kategorik Veri Analizi â€” Benzersiz DeÄŸerler ================= #
st.header("ğŸ§¬ Kategorik Veri Analizi â€” Benzersiz DeÄŸerler")
for test_name in ["Kan Grubu/", "Anormal Hb/"]:
Â  Â  sub = work[work["TETKIK_ISMI"].astype(str) == test_name].copy()
Â  Â  if sub.empty:
Â  Â  Â  Â  st.warning(f"{test_name} verisi bulunamadÄ±.")
Â  Â  Â  Â  continue

Â  Â  st.subheader(f"ğŸ” {test_name}")

Â  Â  raw_text = sub["TEST_DEGERI"].astype(str).str.strip()
Â  Â  if test_name == "Kan Grubu/":
Â  Â  Â  Â  normalized = raw_text.map(normalize_blood_group)
Â  Â  else:
Â  Â  Â  Â  normalized = raw_text.map(norm_anormal_hb_text)

Â  Â  # ============ Ã–ZEL AKIÅ: ANORMAL Hb/ ============
Â  Â  if test_name == "Anormal Hb/":
Â  Â  Â  Â  # 1) Ham yazÄ±m â†’ TC listesi (Frekans yerine)
Â  Â  Â  Â  sub_nonempty = sub[raw_text.ne("") & sub["TEST_DEGERI"].notna()].copy()
Â  Â  Â  Â  if sub_nonempty.empty:
Â  Â  Â  Â  Â  Â  st.info("Anormal Hb/ iÃ§in dolu metin bulunamadÄ±.")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  # Her ham deÄŸer iÃ§in benzersiz TCKIMLIK_NO listesini Ã§Ä±kar
Â  Â  Â  Â  Â  Â  map_tc = (
Â  Â  Â  Â  Â  Â  Â  Â  sub_nonempty
Â  Â  Â  Â  Â  Â  Â  Â  .assign(_val=raw_text.loc[sub_nonempty.index])
Â  Â  Â  Â  Â  Â  Â  Â  .groupby("_val", dropna=False)["TCKIMLIK_NO"]
Â  Â  Â  Â  Â  Â  Â  Â  .apply(lambda s: ", ".join(sorted({str(x) for x in s.dropna().astype(str)})) or "â€”")
Â  Â  Â  Â  Â  Â  Â  Â  .reset_index()
Â  Â  Â  Â  Â  Â  Â  Â  .rename(columns={"_val": "Ham DeÄŸer", "TCKIMLIK_NO": "TCKIMLIK_NO (liste)"})
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  st.markdown("**Ham yazÄ±mlar (TC listeli)**")
Â  Â  Â  Â  Â  Â  st.dataframe(map_tc, use_container_width=True)
Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  "â¬‡ï¸ AnormalHb_ham_yazim_TC_listesi.csv",
Â  Â  Â  Â  Â  Â  Â  Â  data=map_tc.to_csv(index=False).encode("utf-8-sig"),
Â  Â  Â  Â  Â  Â  Â  Â  file_name="AnormalHb_ham_yazim_TC_listesi.csv",
Â  Â  Â  Â  Â  Â  Â  Â  mime="text/csv",
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  # 2) DÃ¼zenlenebilir tablo (CLEAN kolonu)
Â  Â  Â  Â  edit_cols = [c for c in ["PROTOKOL_NO","TCKIMLIK_NO","CINSIYET","SOURCE_FILE","TEST_DEGERI"] if c in sub_nonempty.columns]
Â  Â  Â  Â  edit_df = sub_nonempty[edit_cols].copy()
Â  Â  Â  Â  clean_col = "ANORMAL_HB_CLEAN"
Â  Â  Â  Â  # Daha Ã¶nce varsa koru; yoksa normalize Ã¶neriyi doldur
Â  Â  Â  Â  if clean_col in sub_nonempty.columns:
Â  Â  Â  Â  Â  Â  edit_df[clean_col] = sub_nonempty[clean_col].astype(str)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  edit_df[clean_col] = normalized.loc[sub_nonempty.index].fillna("").astype(str)

Â  Â  Â  Â  st.markdown("**DÃ¼zenlenebilir tablo (CLEAN deÄŸerini yazÄ±n)**")
Â  Â  Â  Â  edited = st.data_editor(
Â  Â  Â  Â  Â  Â  edit_df,
Â  Â  Â  Â  Â  Â  use_container_width=True,
Â  Â  Â  Â  Â  Â  key="anormalhb_editor",
Â  Â  Â  Â  Â  Â  column_config={
Â  Â  Â  Â  Â  Â  Â  Â  "TEST_DEGERI": st.column_config.TextColumn(label="ORIGINAL", help="Ham deÄŸer", disabled=True),
Â  Â  Â  Â  Â  Â  Â  Â  clean_col: st.column_config.TextColumn(label="CLEAN (dÃ¼zenlenebilir)"),
Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  )
Â  Â  Â  Â  col_apply, col_over = st.columns([1,1])
Â  Â  Â  Â  with col_apply:
Â  Â  Â  Â  Â  Â  apply_now = st.button("âœ… Uygula ve kaydet (oturum iÃ§i)", key="apply_anormalhb")
Â  Â  Â  Â  with col_over:
Â  Â  Â  Â  Â  Â  overwrite_main = st.checkbox("ORIGINAL sÃ¼tununu da CLEAN ile deÄŸiÅŸtir", value=False, key="over_anormalhb")

Â  Â  Â  Â  # --- DÃœZELTÄ°LMÄ°Å GÄ°RÄ°NTÄ° BLOKU BAÅLANGICI ---
Â  Â  Â  Â  if apply_now and not edited.empty:
Â  Â  Â  Â  Â  Â  # 1. DÃ¼zenlenen satÄ±rlarÄ± (edited) al
Â  Â  Â  Â  Â  Â  upd = edited[[c for c in ["PROTOKOL_NO","TEST_DEGERI",clean_col] if c in edited.columns]].copy()
Â  Â  Â  Â  Â  Â  upd.rename(columns={clean_col: "__CLEAN_TMP__"}, inplace=True)

Â  Â  Â  Â  Â  Â  key_proto = work["PROTOKOL_NO"].astype(str) if "PROTOKOL_NO" in work.columns else pd.Series("", index=work.index)
Â  Â  Â  Â  Â  Â  key_testÂ  = work["TEST_DEGERI"].astype(str).str.strip()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 2. 'work' dataframe'inde ilgili satÄ±rlarÄ±n CLEAN sÃ¼tununu gÃ¼ncelle
Â  Â  Â  Â  Â  Â  for _, r in upd.iterrows():
Â  Â  Â  Â  Â  Â  Â  Â  proto = str(r.get("PROTOKOL_NO",""))
Â  Â  Â  Â  Â  Â  Â  Â  origÂ  = str(r.get("TEST_DEGERI","")).strip()
Â  Â  Â  Â  Â  Â  Â  Â  # Anahtar: Protokol NO ve Orijinal TEST_DEGERI
Â  Â  Â  Â  Â  Â  Â  Â  mask = (key_proto == proto) & (key_test == orig)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  work.loc[mask, clean_col] = r["__CLEAN_TMP__"]
Â  Â  Â  Â  Â  Â  Â  Â  if overwrite_main:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Orijinali de (isteÄŸe baÄŸlÄ±) deÄŸiÅŸtir
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  work.loc[mask, "TEST_DEGERI"] = r["__CLEAN_TMP__"]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 3. YENÄ° ve Ã–NEMLÄ°: VARIANT_TAG'Ä° YENÄ°DEN HESAPLA
Â  Â  Â  Â  Â  Â  #Â  Â  CLEAN sÃ¼tunu gÃ¼ncellendiÄŸi iÃ§in, tÃ¼m 'work' dataframe'iÂ 
Â  Â  Â  Â  Â  Â  #Â  Â  iÃ§in 'VARIANT_TAG' sÃ¼tununu SÄ°LÄ°P, yeniÂ 
Â  Â  Â  Â  Â  Â  #Â  Â  pick_variant_tag (AdÄ±m 1'de gÃ¼ncellenen) fonksiyonu ileÂ 
Â  Â  Â  Â  Â  Â  #Â  Â  baÅŸtan hesaplatÄ±yoruz.
Â  Â  Â  Â  Â  Â  st.info("CLEAN deÄŸerleri uygulandÄ±, tÃ¼m VARIANT_TAG'ler yeniden hesaplanÄ±yor...")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if "VARIANT_TAG" in work.columns:
Â  Â  Â  Â  Â  Â  Â  Â  work = work.drop(columns="VARIANT_TAG") # Eski tag'leri sil
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # TÃ¼m 'work' Ã¼zerinden tag'leri yeniden hesapla
Â  Â  Â  Â  Â  Â  var_map = (work.groupby("PROTOKOL_NO", group_keys=False)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â .apply(lambda g: pd.Series({"VARIANT_TAG": pick_variant_tag(g)}))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â .reset_index())
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 'work'e yeni VARIANT_TAG sÃ¼tununu ekle
Â  Â  Â  Â  Â  Â  work = work.merge(var_map, on="PROTOKOL_NO", how="left")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # USV Hesaplama bloÄŸunun da (varsa) yeniden Ã§alÄ±ÅŸmasÄ± iÃ§in
Â  Â  Â  Â  Â  Â  # 'work'Ã¼ tekrar gÃ¼ncelliyoruz.
Â  Â  Â  Â  Â  Â  # (Not: Bu kod, bir sonraki adÄ±mdaki 'USV (%)' hesaplamasÄ±nÄ±
Â  Â  Â  Â  Â  Â  #Â  tetiklemez, ancak pivot tabloyu doÄŸru 'work' ile besler.)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.success("VARIANT_TAG'ler baÅŸarÄ±yla gÃ¼ncellendi! Pivot tabloyu kontrol edebilirsiniz.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 4. GÃ¼ncellenmiÅŸ veriyi indirme
Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  "â¬‡ï¸ GÃ¼ncellenmiÅŸ veri (CSV)",
Â  Â  Â  Â  Â  Â  Â  Â  data=work.to_csv(index=False).encode("utf-8-sig"),
Â  Â  Â  Â  Â  Â  Â  Â  file_name="guncellenmis_veri_VE_tagler.csv",
Â  Â  Â  Â  Â  Â  Â  Â  mime="text/csv",
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  # --- DÃœZELTÄ°LMÄ°Å GÄ°RÄ°NTÄ° BLOKU SONU ---

Â  Â  Â  Â  # 3) SeÃ§ince hastanÄ±n/protokolÃ¼n tÃ¼m tetkikleri
Â  Â  Â  Â  # --- DÃœZELTÄ°LMÄ°Å GÄ°RÄ°NTÄ° ---
Â  Â  Â  Â  st.markdown("**HÄ±zlÄ± inceleme: bir hasta veya protokol seÃ§in**")
Â  Â  Â  Â  tcsÂ  = sorted({str(x) for x in sub_nonempty.get("TCKIMLIK_NO", pd.Series(dtype=object)).dropna().astype(str)})
Â  Â  Â  Â  prot = sorted({str(x) for x in sub_nonempty.get("PROTOKOL_NO", pd.Series(dtype=object)).dropna().astype(str)})

Â  Â  Â  Â  tabs_sel = st.tabs(["Hasta ile seÃ§", "Protokol ile seÃ§"])
Â  Â  Â  Â  with tabs_sel[0]:
Â  Â  Â  Â  Â  Â  if tcs:
Â  Â  Â  Â  Â  Â  Â  Â  sel_tc = st.selectbox("TCKIMLIK_NO", options=tcs, key="sel_tc_anormalhb")
Â  Â  Â  Â  Â  Â  Â  Â  proto_for_tc = (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sub_nonempty.loc[sub_nonempty["TCKIMLIK_NO"].astype(str) == sel_tc, "PROTOKOL_NO"]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .astype(str).unique().tolist()
Â  Â  Â  Â  Â  Â  Â  Â  ) if "PROTOKOL_NO" in sub_nonempty.columns else []
Â  Â  Â  Â  Â  Â  Â  Â  all_tests = work[
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (work["TCKIMLIK_NO"].astype(str) == sel_tc) &
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (work["PROTOKOL_NO"].astype(str).isin(proto_for_tc))
Â  Â  Â  Â  Â  Â  Â  Â  ].copy()
Â  Â  Â  Â  Â  Â  Â  Â  show_cols = [c for c in ["PROTOKOL_NO","TETKIK_ISMI","TEST_DEGERI","CINSIYET","SOURCE_FILE"] if c in all_tests.columns]
Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(all_tests[show_cols].sort_values(show_cols[:2]) if not all_tests.empty else all_tests, use_container_width=True)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("SeÃ§ilebilir hasta yok.")
Â  Â  Â  Â  with tabs_sel[1]:
Â  Â  Â  Â  Â  Â  if prot:
Â  Â  Â  Â  Â  Â  Â  Â  sel_p = st.selectbox("PROTOKOL_NO", options=prot, key="sel_proto_anormalhb")
Â  Â  Â  Â  Â  Â  Â  Â  all_tests = work[work["PROTOKOL_NO"].astype(str) == sel_p].copy()
Â  Â  Â  Â  Â  Â  Â  Â  show_cols = [c for c in ["PROTOKOL_NO","TETKIK_ISMI","TEST_DEGERI","CINSIYET","SOURCE_FILE","TCKIMLIK_NO"] if c in all_tests.columns]
Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(all_tests[show_cols].sort_values("TETKIK_ISMI") if not all_tests.empty else all_tests, use_container_width=True)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("SeÃ§ilebilir protokol yok.")

Â  Â  Â  Â  # Bu Ã¶zel akÄ±ÅŸta frekans/ki-kare gÃ¶stermiyoruz.
Â  Â  Â  Â  continueÂ  # >>> dÃ¶ngÃ¼nÃ¼n geri kalanÄ±nÄ± Kan Grubu/ iÃ§in Ã§alÄ±ÅŸtÄ±r

Â  Â  # ============ STANDART AKIÅ: KAN GRUBU/ (mevcut mantÄ±ÄŸÄ±nÄ±z) ============
Â  Â  # 1) Ham yazÄ±mlarÄ±n sayÄ±mÄ±
Â  Â  sub_text = raw_text[raw_text.str.contains(r"[A-Za-zÄ°Ä±Ã–Ã¶ÃœÃ¼Ã‡Ã§ÅÅŸ]", na=False)]
Â  Â  if sub_text.empty:
Â  Â  Â  Â  st.info("Harf iÃ§eren veri bulunamadÄ±.")
Â  Â  Â  Â  value_counts = pd.DataFrame(columns=["Benzersiz DeÄŸer","Frekans"])
Â  Â  else:
Â  Â  Â  Â  value_counts = (
Â  Â  Â  Â  Â  Â  sub_text.value_counts(dropna=False)
Â  Â  Â  Â  Â  Â  .rename_axis("Benzersiz DeÄŸer")
Â  Â  Â  Â  Â  Â  .reset_index(name="Frekans")
Â  Â  Â  Â  )
Â  Â  st.markdown("**Ham YazÄ±mlar**")
Â  Â  st.dataframe(value_counts, use_container_width=True)
Â  Â  st.download_button(
Â  Â  Â  Â  f"â¬‡ï¸ {test_name.strip('/')}_benzersiz_degerler.csv",
Â  Â  Â  Â  data=value_counts.to_csv(index=False).encode("utf-8-sig"),
Â  Â  Â  Â  file_name=f"{test_name.strip('/')}_benzersiz_degerler.csv",
Â  Â  Â  Â  mime="text/csv"
Â  Â  )

Â  Â  # 2) Normalize edilmiÅŸ kategorilerin sayÄ±mÄ±
Â  Â  norm_counts = (
Â  Â  Â  Â  normalized.value_counts(dropna=False)
Â  Â  Â  Â  .rename_axis("Kategori (normalize)")
Â  Â  Â  Â  .reset_index(name="N")
Â  Â  )
Â  Â  if not norm_counts.empty:
Â  Â  Â  Â  totalN = int(norm_counts["N"].sum())
Â  Â  Â  Â  norm_counts["%"] = (norm_counts["N"] / totalN * 100).round(2)
Â  Â  else:
Â  Â  Â  Â  norm_counts = pd.DataFrame(columns=["Kategori (normalize)","N","%"])

Â  Â  st.markdown("**Normalize EdilmiÅŸ Kategoriler**")
Â  Â  st.dataframe(norm_counts, use_container_width=True)
Â  Â  st.download_button(
Â  Â  Â  Â  f"â¬‡ï¸ {test_name.strip('/')}_normalize_frekans.csv",
Â  Â  Â  Â  data=norm_counts.to_csv(index=False).encode("utf-8-sig"),
Â  Â  Â  Â  file_name=f"{test_name.strip('/')}_normalize_frekans.csv",
Â  Â  Â  Â  mime="text/csv"
Â  Â  )

Â  Â  # 3) Kategorik genel frekans/ki-kare (normalize etiketle)
Â  Â  cat_name = "__CAT__"
Â  Â  sub = sub.assign(**{cat_name: normalized})
Â  Â  freq_all = (sub[cat_name].value_counts(dropna=False)
Â  Â  Â  Â  Â  Â  Â  Â  .rename_axis("Kategori").to_frame("N").reset_index())
Â  Â  totalN = int(freq_all["N"].sum()) if not freq_all.empty else 0
Â  Â  if totalN:
Â  Â  Â  Â  freq_all["%"] = (freq_all["N"]/totalN*100).round(2)
Â  Â  else:
Â  Â  Â  Â  freq_all["%"] = []
Â  Â  freq_by_sex = (sub.pivot_table(index=cat_name, columns="CINSIYET",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â values="PROTOKOL_NO", aggfunc="count", fill_value=0)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â .astype(int).reset_index().rename(columns={cat_name:"Kategori"}))
Â  Â  chi2_msg = "Ki-kare uygulanamadÄ±."
Â  Â  try:
Â  Â  Â  Â  from scipy.stats import chi2_contingency
Â  Â  Â  Â  cont = freq_by_sex.drop(columns=["Kategori"]).values
Â  Â  Â  Â  if cont.sum() > 0 and cont.shape[1] > 1:
Â  Â  Â  Â  Â  Â  chi2, p, dof, _ = chi2_contingency(cont)
Â  Â  Â  Â  Â  Â  chi2_msg = f"Chi-square: Ï‡Â²={chi2:.2f}, df={dof}, p={p:.4g}"
Â  Â  except Exception as e:
Â  Â  Â  Â  chi2_msg = f"Hata: {e}"

Â  Â  tabs = st.tabs(["Frekans", "Cinsiyet DaÄŸÄ±lÄ±mÄ±", "Ä°statistik"])
Â  Â  with tabs[0]: st.dataframe(freq_all, use_container_width=True)
Â  Â  with tabs[1]: st.dataframe(freq_by_sex, use_container_width=True)
Â  Â  with tabs[2]: st.info(chi2_msg)

# ================= Genel Bilgiler ================= #
st.subheader("ğŸ” Genel Bilgiler (BirleÅŸtirilmiÅŸ)")
c1, c2, c3, c4, c5 = st.columns(5)
c1.metric("Toplam SatÄ±r", f"{len(df):,}")
c2.metric("Benzersiz TCKIMLIK_NO", f"{df['TCKIMLIK_NO'].nunique():,}")
c3.metric("Benzersiz Tetkik", f"{df['TETKIK_ISMI'].nunique():,}")
c4.metric("Benzersiz Cinsiyet", f"{df['CINSIYET'].nunique():,}")
c5.metric("Dosya SayÄ±sÄ±", f"{df['SOURCE_FILE'].nunique():,}")

with st.expander("Ham Veri Ã–n Ä°zleme (limitli)"):
Â  Â  st.dataframe(work.head(DISPLAY_LIMIT), use_container_width=True)
Â  Â  st.caption(f"YalnÄ±zca ilk {DISPLAY_LIMIT} satÄ±r gÃ¶rÃ¼ntÃ¼lenir.")


# ================= HÄ±zlÄ± Ã–zetler ================= #
st.header("âš™ï¸ HÄ±zlÄ± Ã–zet ve KÄ±rÄ±lÄ±mlar")
colA, colB = st.columns(2)
with colA:
Â  Â  st.write("**Cinsiyete GÃ¶re TanÄ±mlayÄ±cÄ±lar (SeÃ§imdeki veri)**")
Â  Â  sex_summary = summarize_sex_counts(work)
Â  Â  st.dataframe(sex_summary, use_container_width=True)
with colB:
Â  Â  st.write("**Dosyaya GÃ¶re SatÄ±r & Hasta & Tetkik SayÄ±sÄ±**")
Â  Â  per_file = work.groupby("SOURCE_FILE").agg(
Â  Â  Â  Â  N=("PROTOKOL_NO", "size"),
Â  Â  Â  Â  Hasta_Sayisi=("TCKIMLIK_NO", "nunique"),
Â  Â  Â  Â  Tetkik_Sayisi=("TETKIK_ISMI", "nunique")
Â  Â  ).reset_index()
Â  Â  st.dataframe(per_file, use_container_width=True)
Â  Â  export_df(per_file, "dosya_bazinda_ozet_filtreli.csv")


# ================= Tetkik BazlÄ± Analiz (SeÃ§im) ================= #
st.header("ğŸ“Š Tetkik BazlÄ± Analiz (SeÃ§im)")
results_rows = []
for test_name in selected_tests:
Â  Â  # === BEGIN PATCH: overall pool for global stats ===
Â  Â  overall_pool = []
Â  Â  # === END PATCH ===
Â  Â  if test_name in CATEGORICAL_TESTS:
Â  Â  Â  Â  # Kan Grubu/ ve Anormal Hb/ yukarÄ±da Ã¶zel blokta analiz edildi
Â  Â  Â  Â  continue

Â  Â  sub = work[work["TETKIK_ISMI"].astype(str) == test_name].copy()
Â  Â  if sub.empty:Â 
Â  Â  Â  Â  continue

Â  Â  use_threshold = st.checkbox(
Â  Â  Â  Â  f"â€˜{test_name}â€™ iÃ§in eriÅŸkin eÅŸiÄŸini uygula",
Â  Â  Â  Â  value=(test_name in THRESHOLDS),
Â  Â  Â  Â  key=f"th_{test_name}"
Â  Â  )
Â  Â  use_gt_zeroÂ  = st.checkbox(
Â  Â  Â  Â  f"â€˜{test_name}â€™ iÃ§in sadece > 0 deÄŸerleri dahil et",
Â  Â  Â  Â  value=(test_name in GT_ZERO_DEFAULT),
Â  Â  Â  Â  key=f"gt0_{test_name}"
Â  Â  )
Â  Â  sub_work = sub[sub["__VAL_NUM__"].notna()].copy()
Â  Â  if use_threshold and test_name in THRESHOLDS:
Â  Â  Â  Â  sub_work = sub_work[apply_threshold(sub_work["__VAL_NUM__"], THRESHOLDS[test_name])]
Â  Â  Â  Â  st.caption(f"EÅŸik: {THRESHOLDS[test_name][0]} {THRESHOLDS[test_name][1]}")
Â  Â  elif use_gt_zero:
Â  Â  Â  Â  sub_work = sub_work[sub_work["__VAL_NUM__"] > 0]
Â  Â  Â  Â  st.caption("Filtre: > 0")
Â  Â  if sub_work.empty:
Â  Â  Â  Â  st.warning("Filtre sonrasÄ± satÄ±r bulunamadÄ±.");Â 
Â  Â  Â  Â  continue

Â  Â  stats_overall = descr_stats_fast(sub_work["__VAL_NUM__"])
Â  Â  normal_flagÂ  Â = normality_flag(sub_work["__VAL_NUM__"])
Â  Â  # Normalite testi (etiket + p)
Â  Â  norm_label, norm_p_disp = normality_test_with_p(sub_work["__VAL_NUM__"])

Â  Â  # Genel toplama havuzuna ekle
Â  Â  overall_pool.extend(pd.to_numeric(sub_work["__VAL_NUM__"], errors="coerce").dropna().tolist())



Â  Â  by_sexÂ  = (sub_work.groupby("CINSIYET", dropna=False)["__VAL_NUM__"]
Â  Â  Â  Â  Â  Â  Â  Â .agg(count="count", mean="mean", std="std", min="min", median="median", max="max")).reset_index()
Â  Â  by_file = (sub_work.groupby("SOURCE_FILE", dropna=False)["__VAL_NUM__"]
Â  Â  Â  Â  Â  Â  Â  Â .agg(count="count", mean="mean", std="std", min="min", median="median", max="max")).reset_index()
Â  Â  _msg_df = sub_work.rename(columns={"__VAL_NUM__": "VAL"})
Â  Â  msg, _ = nonparametric_test_by_group(_msg_df, "VAL", "CINSIYET")
Â  Â  # === BEGIN PATCH: collect values for global stats ===
Â  Â  overall_pool.extend(pd.to_numeric(_msg_df["VAL"], errors="coerce").dropna().tolist())
Â  Â  # === END PATCH ===


Â  Â  results_rows.append({
Â  Â  Â  Â  "TETKIK_ISMI": test_name,
Â  Â  Â  Â  "N": stats_overall["count"],
Â  Â  Â  Â  "Mean": stats_overall["mean"],
Â  Â  Â  Â  "Median": stats_overall["median"],
Â  Â  Â  Â  "Std": stats_overall["std"],
Â  Â  Â  Â  "Min": stats_overall["min"],
Â  Â  Â  Â  "Q1": stats_overall["q1"],
Â  Â  Â  Â  "Q3": stats_overall["q3"],
Â  Â  Â  Â  "Max": stats_overall["max"],
Â  Â  Â  Â  "Normalite": normal_flag,
Â  Â  Â  Â  "Test": msg
Â  Â  })

Â  Â  tabs = st.tabs(["TanÄ±mlayÄ±cÄ±", "Cinsiyet", "Dosya", "Ä°statistiksel Test", "Histogram", "Boxplot"])
Â  Â  with tabs[0]: st.table(pd.DataFrame([stats_overall]))
Â  Â  with tabs[1]: st.dataframe(by_sex, use_container_width=True)
Â  Â  with tabs[2]: st.dataframe(by_file, use_container_width=True)
Â  Â  with tabs[3]: st.info(msg)
Â  Â  with tabs[4]:
Â  Â  Â  Â  if st.checkbox(f"Histogram gÃ¶ster ({test_name})", value=False):
Â  Â  Â  Â  Â  Â  make_hist(_msg_df, "VAL", bins=30, title=f"{test_name} - Histogram")
Â  Â  with tabs[5]:
Â  Â  Â  Â  if st.checkbox(f"Boxplot gÃ¶ster ({test_name})", value=False):
Â  Â  Â  Â  Â  Â  make_boxplot(sub_work, "CINSIYET", "__VAL_NUM__", title=f"{test_name} - Cinsiyete GÃ¶re Boxplot")

Â  Â  pos_cols = ["PROTOKOL_NO", "TCKIMLIK_NO", "CINSIYET", "SOURCE_FILE"]
Â  Â  pos_cols = [c for c in pos_cols if c in sub_work.columns]
Â  Â  pos_tbl = sub_work[pos_cols + ["__VAL_NUM__"]].sort_values("__VAL_NUM__", ascending=False)
Â  Â  st.write("**Filtre sonrasÄ± kayÄ±tlar**")
Â  Â  st.dataframe(pos_tbl, use_container_width=True)
Â  Â  st.download_button(
Â  Â  Â  Â  "â¬‡ï¸ TCKIMLIK_NO listesi (CSV)",
Â  Â  Â  Â  data=pos_tbl.to_csv(index=False).encode("utf-8-sig"),
Â  Â  Â  Â  file_name=f"{test_name}_filtre_sonrasi.csv",
Â  Â  Â  Â  mime="text/csv"
Â  Â  )

if results_rows:
Â  Â  st.header("ğŸ§¾ Toplu Ã–zet Tablosu (SeÃ§ili Tetkikler)")
Â  Â  res_df = pd.DataFrame(results_rows)
Â  Â  # === BEGIN PATCH: append global total row ===
Â  Â  if len(overall_pool) > 0:
Â  Â  Â  Â  overall_stats = descr_stats_fast(pd.Series(overall_pool))
Â  Â  Â  Â  # N'yi tek tek testlerden de toplayabiliriz ama havuz zaten filtre-sonrasÄ± gerÃ§ek toplamÄ± temsil ediyor
Â  Â  Â  Â  overall_row = {
Â  Â  Â  Â  Â  Â  "TETKIK_ISMI": "GENEL TOPLAM",
Â  Â  Â  Â  Â  Â  "N": overall_stats["count"],
Â  Â  Â  Â  Â  Â  "Mean": overall_stats["mean"],
Â  Â  Â  Â  Â  Â  "Median": overall_stats["median"],
Â  Â  Â  Â  Â  Â  "Std": overall_stats["std"],
Â  Â  Â  Â  Â  Â  "Min": overall_stats["min"],
Â  Â  Â  Â  Â  Â  "Q1": overall_stats["q1"],
Â  Â  Â  Â  Â  Â  "Q3": overall_stats["q3"],
Â  Â  Â  Â  Â  Â  "Max": overall_stats["max"],
Â  Â  Â  Â  Â  Â  "Normalite": norm_label,Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  "p (normalite)": norm_p_disp,Â  Â  Â 
Â  Â  Â  Â  Â  Â  "Test": "â€”",
Â  Â  Â  Â  }
Â  Â  Â  Â  res_df = pd.concat([res_df, pd.DataFrame([overall_row])], ignore_index=True)
Â  Â  # === END PATCH ===

Â  Â Â 
Â  Â  st.dataframe(res_df, use_container_width=True)
Â  Â  export_df(res_df, name="tetkik_ozet.csv")

# ================= PIVOT: VARYANTLARA GÃ–RE PARAMETRE Ã–ZETÄ° (TABLE 2 - AKILLI v2) ================= #
st.header("ğŸ”¬ Varyantlara GÃ¶re Parametre Ã–zeti (AkÄ±llÄ± Format)")
st.caption("GÃ¶rseldeki Table 2'ye benzer pivot tablo. SÃ¼tun baÅŸlÄ±klarÄ± gruptaki protokol sayÄ±sÄ±nÄ± (n) iÃ§erir.")

# 1. 'PARAMS' sÃ¶zlÃ¼ÄŸÃ¼nde tanÄ±mlÄ± testleri (HGB, MCV, A2, F vb.) al
params_to_analyze = list(PARAMS.keys())

# 2. SÃ¼tun baÅŸlÄ±klarÄ± (n=?) iÃ§in VARYANT_TAG'a sahip benzersiz protokol sayÄ±larÄ±nÄ± HESAPLA
#Â  Â  Bunu, 'work' dataframe'i hemogram testlerine gÃ¶re FÄ°LTRELENMEDEN Ã–NCE yaparÄ±z
try:
Â  Â  variant_counts = work[
Â  Â  Â  Â  work["VARIANT_TAG"].notna() & work["PROTOKOL_NO"].notna()
Â  Â  ].groupby("VARIANT_TAG")["PROTOKOL_NO"].nunique()
Â  Â Â 
Â  Â  # { "HbS": "HbS (n=15)", "HbE": "HbE (n=13)" } gibi bir harita oluÅŸtur
Â  Â  rename_map = {
Â  Â  Â  Â  tag: f"{tag} (n={count})" for tag, count in variant_counts.items()
Â  Â  }
except KeyError:
Â  Â  st.warning("Varyant sayÄ±larÄ± (n=?) hesaplanamadÄ±. PROTOKOL_NO sÃ¼tunu eksik olabilir.")
Â  Â  rename_map = {} # Harita boÅŸ kalÄ±r, yeniden adlandÄ±rma yapÄ±lmaz

# 3. Pivot tablo iÃ§in veriyi FÄ°LTRELE
#Â  Â  - VARIANT_TAG'Ä± olanlarÄ± (Ã¶rn. HbS, HbA2â†‘)
#Â  Â  - TETKIK_ISMI, PARAMS listemizde olanlarÄ± (Ã¶rn. Hemogram/HGB, HbA2 (%))
#Â  Â  - SayÄ±sal deÄŸeri olanlarÄ±
data_for_pivot = work[
Â  Â  work["TETKIK_ISMI"].isin(params_to_analyze) &
Â  Â  work["VARIANT_TAG"].notna() &
Â  Â  work["__VAL_NUM__"].notna()
].copy()

if data_for_pivot.empty:
Â  Â  st.info("Pivot tablo iÃ§in yeterli veri bulunamadÄ± (VaryantÄ± olan ve hemogram/HPLC parametresi iÃ§eren).")
else:
Â  Â  # 4. AKILLI FORMATLAYICI (SÃ¼perskript 'a' ve 'b' eklenmiÅŸ)
Â  Â  def _format_smart_summary_superscript(s: pd.Series):
Â  Â  Â  Â  s = pd.to_numeric(s, errors="coerce").dropna()
Â  Â  Â  Â  n = len(s)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if n == 0:
Â  Â  Â  Â  Â  Â  return "â€”"
Â  Â  Â  Â  if n == 1:
Â  Â  Â  Â  Â  Â  return f"{s.iloc[0]:.2f}"
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  norm_label, _ = normality_test_with_p(s)
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  norm_label = "bilinmiyor"
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Yetersiz veri veya non-normal ise: Median [Min-Max] + 'b'
Â  Â  Â  Â  if norm_label != "normal":
Â  Â  Â  Â  Â  Â  med = s.median()
Â  Â  Â  Â  Â  Â  min_val = s.min()
Â  Â  Â  Â  Â  Â  max_val = s.max()
Â  Â  Â  Â  Â  Â  return f"{med:.2f} [{min_val:.2f}â€“{max_val:.2f}]áµ‡"
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Normal ise: Mean Â± SD + 'a'
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  mean = s.mean()
Â  Â  Â  Â  Â  Â  std = s.std(ddof=1)
Â  Â  Â  Â  Â  Â  if pd.isna(std) or std == 0:
Â  Â  Â  Â  Â  Â  Â  Â  return f"{mean:.2f}"
Â  Â  Â  Â  Â  Â  return f"{mean:.2f} Â± {std:.2f}áµƒ"

Â  Â  try:
Â  Â  Â  Â  # 5. Pivot tabloyu oluÅŸtur
Â  Â  Â  Â  pivot_table = pd.pivot_table(
Â  Â  Â  Â  Â  Â  data_for_pivot,
Â  Â  Â  Â  Â  Â  values="__VAL_NUM__",
Â  Â  Â  Â  Â  Â  index="TETKIK_ISMI",
Â  Â  Â  Â  Â  Â  columns="VARIANT_TAG",
Â  Â  Â  Â  Â  Â  aggfunc=_format_smart_summary_superscript, # GÃœNCELLENMÄ°Å fonksiyon
Â  Â  Â  Â  Â  Â  fill_value="â€”"
Â  Â  Â  Â  )

Â  Â  Â  Â  # 6. SatÄ±rlarÄ± (index) yeniden adlandÄ±r ve sÄ±rala
Â  Â  Â  Â  display_map = {k: v[0] for k, v in PARAMS.items()}
Â  Â  Â  Â  ordered_params_in_table = [
Â  Â  Â  Â  Â  Â  param_key for param_key in PARAMS.keys()Â 
Â  Â  Â  Â  Â  Â  if param_key in pivot_table.index
Â  Â  Â  Â  ]
Â  Â  Â  Â Â 
Â  Â  Â  Â  if ordered_params_in_table:
Â  Â  Â  Â  Â  Â  pivot_table_reindexed = pivot_table.loc[ordered_params_in_table]
Â  Â  Â  Â  Â  Â  pivot_table_reindexed.index = pivot_table_reindexed.index.map(display_map)
Â  Â  Â  Â  Â  Â  pivot_table_reindexed = pivot_table_reindexed.rename_axis("Parametre")

Â  Â  Â  Â  Â  Â  # 7. YENÄ°: SÃ¼tunlarÄ± (n=?) iÃ§erecek ÅŸekilde yeniden adlandÄ±r
Â  Â  Â  Â  Â  Â  if rename_map:
Â  Â  Â  Â  Â  Â  Â  Â  Â # Sadece tabloda var olan sÃ¼tunlarÄ± yeniden adlandÄ±r
Â  Â  Â  Â  Â  Â  Â  Â  existing_cols_to_rename = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  col: rename_map[col] for col in pivot_table_reindexed.columnsÂ 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if col in rename_map
Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  pivot_table_reindexed = pivot_table_reindexed.rename(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  columns=existing_cols_to_rename
Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  # 8. Ekranda gÃ¶ster
Â  Â  Â  Â  Â  Â  st.dataframe(pivot_table_reindexed, use_container_width=True)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 9. YENÄ°: AÃ§Ä±klamayÄ± (footnote) ekle
Â  Â  Â  Â  Â  Â  st.caption("""
Â  Â  Â  Â  Â  Â  Â  Â  áµƒ: Normal daÄŸÄ±lÄ±m gÃ¶steren veriler (Mean Â± SD)Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  áµ‡: Normal daÄŸÄ±lÄ±m gÃ¶stermeyen veya yetersiz veriler (Median [Minâ€“Max])
Â  Â  Â  Â  Â  Â  """)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 10. Ä°ndirme butonu (gÃ¼ncellenmiÅŸ tabloyu indirir)
Â  Â  Â  Â  Â  Â  csv_data = pivot_table_reindexed.to_csv(index=True).encode("utf-8-sig")
Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  "â¬‡ï¸ Pivot Tabloyu Ä°ndir (CSV)",
Â  Â  Â  Â  Â  Â  Â  Â  data=csv_data,
Â  Â  Â  Â  Â  _ Â  Â  Â  file_name="varyant_pivot_ozet_akilli_v2.csv",
Â  Â  Â  Â  Â  Â  Â  Â  mime="text/csv",
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.info("Pivot tablo oluÅŸturuldu ancak `PARAMS` listesiyle eÅŸleÅŸen parametre bulunamadÄ±.")

Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"Pivot tablo oluÅŸturulurken bir hata oluÅŸtu: {e}")
st.caption("Not: Kan Grubu ve Anormal Hb analizleri normalize edilerek hesaplanÄ±r; ham yazÄ±mlar ayrÄ±ca CSV olarak indirilebilir.")
