# ==== FILE START ====
# app.py
# -*- coding: utf-8 -*-
"""
ðŸ§ª Tetkik Analiz ArayÃ¼zÃ¼ â€” Ã‡oklu Dosya (Optimize, Revize)
- Ã‡oklu dosya hÄ±zlÄ± okuma (paralel + cache)
- Bellek optimizasyonu (downcast, categorical)
- Ä°steÄŸe baÄŸlÄ± Polars hÄ±zlandÄ±rma
- BÃ¼yÃ¼k tablolarÄ± gÃ¼venli gÃ¶stermeye yÃ¶nelik limitler
- Grafikler isteÄŸe baÄŸlÄ± (matplotlib; renk set edilmez)
- Kategorik analizlerde SAÄžLAM normalizasyon:
    â€¢ Kan Grubu: A/B/AB/O/0 + Rh(+/-/poz/neg/rh+/rh-) â†’ tek tipe
    â€¢ Anormal Hb: HbS/HbC/HbD/HbE/HbG/HbJ/HbO-Arab/Hb Lepore/HbH/Hb Bartâ€™s/HbA2â†‘/HbFâ†‘/Normal/Î²-thal trait/Î±-thal trait â€¦
- Hem ham yazÄ±mlar hem normalize kategoriler ayrÄ± tablolar/CSV
- Ham yazÄ±mdan hasta/protokol seÃ§erek tÃ¼m tetkikleri gÃ¶ster

Ã‡alÄ±ÅŸtÄ±rma:
    streamlit run app.py
"""

import io
import re
import math
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from scipy import stats
from concurrent.futures import ThreadPoolExecutor

# ============== Ayarlar ============== #
st.set_page_config(page_title="Tetkik Analiz â€” Optimize", layout="wide")

REQ_COLS = ["PROTOKOL_NO", "TCKIMLIK_NO", "TETKIK_ISMI", "TEST_DEGERI", "CINSIYET"]

# Kategorik (metin) testler
CATEGORICAL_TESTS = {"Kan Grubu/", "Anormal Hb/"}

# --- EriÅŸkin pozitiflik eÅŸikleri (TETKIK_ISMI anahtarlarÄ±) ---
THRESHOLDS = {
    "HbA2 (%)": (">=", 3.5),
    "A2/":      (">=", 3.5),
    "HbF (%)":  (">",  2.0),
    "F/":       (">",  2.0),
    "HbS (%)":  (">",  0.0),
    "HbC (%)":  (">",  0.0),
    "HbD (%)":  (">",  0.0),
    "HbE (%)":  (">",  0.0),
}
GT_ZERO_DEFAULT = {
    "HbS (%)","HbC (%)","HbD (%)","HbE (%)","HbF (%)","HbA2 (%)","A2/","F/",
    "C/","D/","E/","S/","G/","J/","O/"
}
VARIANT_NUMERIC_TESTS = {
    # HPLC/CE kÄ±sa test isimleri â†’ NormalleÅŸtirilmiÅŸ varyant etiketi
    "HbS (%)":"HbS","HbC (%)":"HbC","HbD (%)":"HbD","HbE (%)":"HbE","HbF (%)":"HbFâ†‘","HbA2 (%)":"HbA2â†‘",
    "C/":"HbC","D/":"HbD","E/":"HbE","S/":"HbS",
    "G/":"HbG","J/":"HbJ","O/":"HbO-Arab"
}

DISPLAY_LIMIT = 400

MALE_TOKENS   = {"e","erkek","m","male","bay"}
FEMALE_TOKENS = {"k","kadÄ±n","kadin","f","female","bayan"}

# Polars mevcut mu?
try:
    import polars as pl
    HAS_POLARS = True
except Exception:
    HAS_POLARS = False


# ============== YardÄ±mcÄ±lar ============== #
def coerce_numeric(series: pd.Series) -> pd.Series:
    s = series.astype(str).str.replace(",", ".", regex=False).str.replace(" ", "", regex=False)
    return pd.to_numeric(s, errors="coerce")


def add_numeric_copy(frame, src_col="TEST_DEGERI", out_col="__VAL_NUM__"):
    if out_col not in frame.columns:
        tmp = (frame[src_col].astype(str)
               .str.replace(",", ".", regex=False)
               .str.replace(" ", "", regex=False))
        frame[out_col] = pd.to_numeric(tmp, errors="coerce")
    return frame


def check_columns(df: pd.DataFrame):
    return [c for c in REQ_COLS if c not in df.columns]


def normalize_sex_label(value):
    if not isinstance(value, str): return None
    trimmed = value.strip()
    if not trimmed: return None
    low = trimmed.lower()
    if low in MALE_TOKENS: return "Erkek"
    if low in FEMALE_TOKENS: return "KadÄ±n"
    return trimmed


def _resolve_patient_sex(series: pd.Series) -> str:
    values = [v for v in pd.unique(series.dropna()) if isinstance(v, str) and v]
    if not values: return "Bilinmiyor"
    if len(values) == 1: return values[0]
    return "Ã‡akÄ±ÅŸma"


def summarize_sex_counts(frame: pd.DataFrame) -> pd.DataFrame:
    tmp = frame[["TCKIMLIK_NO", "CINSIYET"]].copy()
    tmp["CINSIYET"] = tmp["CINSIYET"].astype(str)
    tmp["__SEX_CANON__"] = tmp["CINSIYET"].map(normalize_sex_label).astype(object)
    s_rows = tmp["__SEX_CANON__"].where(tmp["__SEX_CANON__"].notna(), "Bilinmiyor")
    row_counts = (
        s_rows.value_counts(dropna=False)
        .rename_axis("CINSIYET").to_frame("SatÄ±r SayÄ±sÄ±")
    )
    with_id = tmp[tmp["TCKIMLIK_NO"].notna()].copy()
    if not with_id.empty:
        w = with_id.copy()
        w["__SEX_CANON__"] = w["__SEX_CANON__"].astype(object)
        patient_gender = (
            w.groupby("TCKIMLIK_NO")["__SEX_CANON__"]
             .apply(lambda s: _resolve_patient_sex(pd.Series(pd.unique(s.dropna()))))
             .reset_index(name="__SEX_RESOLVED__")
        )
        patient_counts = (
            patient_gender["__SEX_RESOLVED__"].fillna("Bilinmiyor")
            .value_counts(dropna=False)
            .rename_axis("CINSIYET").to_frame("Hasta (Benzersiz)")
        )
    else:
        patient_counts = pd.DataFrame(columns=["Hasta (Benzersiz)"])
    summary = row_counts.join(patient_counts, how="outer").fillna(0)
    summary["SatÄ±r SayÄ±sÄ±"] = summary["SatÄ±r SayÄ±sÄ±"].astype(int)
    if "Hasta (Benzersiz)" in summary.columns:
        summary["Hasta (Benzersiz)"] = summary["Hasta (Benzersiz)"].astype(int)
    else:
        summary["Hasta (Benzersiz)"] = 0
    total_rows = int(summary["SatÄ±r SayÄ±sÄ±"].sum())
    total_patients = int(summary["Hasta (Benzersiz)"].sum())
    summary["% SatÄ±r"]  = (summary["SatÄ±r SayÄ±sÄ±"] / total_rows * 100).round(2) if total_rows else np.nan
    summary["% Hasta"] = (summary["Hasta (Benzersiz)"] / total_patients * 100).round(2) if total_patients else np.nan
    summary = summary.reset_index()
    summary = summary[["CINSIYET","Hasta (Benzersiz)","% Hasta","SatÄ±r SayÄ±sÄ±","% SatÄ±r"]]
    return summary.sort_values("Hasta (Benzersiz)", ascending=False).reset_index(drop=True)


def downcast_df(df: pd.DataFrame) -> pd.DataFrame:
    if "TEST_DEGERI" in df.columns:
        df["TEST_DEGERI"] = df["TEST_DEGERI"].astype(str)
    for col in ["TETKIK_ISMI", "CINSIYET", "SOURCE_FILE"]:
        if col in df.columns:
            df[col] = df[col].astype("category")
    return df


def descr_stats_fast(x: pd.Series) -> dict:
    x = pd.to_numeric(x, errors="coerce")
    x = x[~x.isna()]
    if x.empty:
        return {"count":0,"mean":np.nan,"std":np.nan,"min":np.nan,"q1":np.nan,"median":np.nan,"q3":np.nan,"max":np.nan,"cv%":np.nan,"iqr":np.nan}
    q = np.percentile(x, [25, 50, 75])
    mean = float(x.mean())
    std = float(x.std(ddof=1)) if len(x) > 1 else 0.0
    cv  = (std/mean)*100 if mean!=0 else np.nan
    return {"count":int(x.size),"mean":mean,"std":std,"min":float(x.min()),"q1":float(q[0]),"median":float(q[1]),"q3":float(q[2]),"max":float(x.max()),"cv%":float(cv),"iqr":float(q[2]-q[0])}


# ----- P deÄŸeri yazÄ±m kuralÄ± (TÃ¼rkÃ§e ondalÄ±k) -----
def _fmt_p(p: float) -> str:
    if p is None or np.isnan(p):
        return "â€”"
    if p < 0.001:
        return "<0,001"
    if p < 0.05:
        return "<0,05"
    return f"{p:.3f}".replace(".", ",")


# ----- Normalite testi: n<=5000 Shapiro; bÃ¼yÃ¼k n KS (N(Î¼,Ïƒ)) -----
def normality_test_with_p(series: pd.Series, alpha: float = 0.05):
    x = pd.to_numeric(series, errors="coerce").dropna()
    n = len(x)
    if n < 3:
        return "yetersiz", "â€”"
    try:
        if n <= 5000:
            stat, p = stats.shapiro(x)
        else:
            mu = float(np.mean(x))
            sd = float(np.std(x, ddof=1))
            if sd == 0:
                return "yetersiz", "â€”"
            stat, p = stats.kstest(x, 'norm', args=(mu, sd))  # H0: N(mu, sd)
        label = "normal" if p >= alpha else "non-normal"
        return label, _fmt_p(p)
    except Exception:
        return "bilinmiyor", "â€”"


def normality_flag(x: pd.Series, alpha=0.05) -> str:
    x = pd.to_numeric(x, errors="coerce").dropna()
    if len(x) < 3:
        return "yetersiz"
    try:
        if len(x) <= 5000:
            _, p = stats.shapiro(x)
            return "normal" if p >= alpha else "non-normal"
        else:
            res = stats.anderson(x, dist="norm")
            crit = res.critical_values[2]  # ~%5
            return "normal" if res.statistic < crit else "non-normal"
    except Exception:
        return "bilinmiyor"


def apply_threshold(series, rule):
    op, cut = rule
    if op == ">=": return series >= cut
    if op == ">":  return series >  cut
    if op == "<=": return series <= cut
    if op == "<":  return series <  cut
    return series.notna()


def nonparametric_test_by_group(df, val_col, grp_col):
    groups = [g.dropna() for _, g in df.groupby(grp_col)[val_col]]
    groups = [pd.to_numeric(g, errors="coerce").dropna() for g in groups]
    groups = [g for g in groups if len(g) > 0]
    unique_groups = df[grp_col].dropna().unique()
    unique_groups = [g for g in unique_groups if df[df[grp_col] == g][val_col].notna().sum() > 0]
    if len(unique_groups) < 2:
        return "KarÅŸÄ±laÅŸtÄ±rma iÃ§in en az 2 grup gerekli.", None
    if len(unique_groups) == 2:
        gnames = list(unique_groups)
        x = pd.to_numeric(df[df[grp_col] == gnames[0]][val_col], errors="coerce").dropna()
        y = pd.to_numeric(df[df[grp_col] == gnames[1]][val_col], errors="coerce").dropna()
        if len(x) >= 1 and len(y) >= 1:
            stat, p = stats.mannwhitneyu(x, y, alternative="two-sided")
            return f"Mannâ€“Whitney U: U={stat:.2f}, p={p:.4g} ({gnames[0]} vs {gnames[1]})", ("MWU", stat, p, gnames[0], gnames[1])
        else:
            return "Gruplarda yeterli gÃ¶zlem yok.", None
    else:
        stat, p = stats.kruskal(*groups)
        return f"Kruskalâ€“Wallis: H={stat:.2f}, p={p:.4g} (grup sayÄ±sÄ±: {len(unique_groups)})", ("KW", stat, p, unique_groups)


def make_boxplot(df, x_col, y_col, title="Kutu GrafiÄŸi"):
    valid = df[[x_col, y_col]].copy()
    valid[y_col] = pd.to_numeric(valid[y_col], errors="coerce")
    valid = valid.dropna()
    if valid.empty:
        st.info("Grafik iÃ§in yeterli veri yok."); return
    cats = list(valid[x_col].astype(str).unique())
    data = [valid[valid[x_col].astype(str) == c][y_col].values for c in cats]
    fig, ax = plt.subplots()
    ax.boxplot(data, labels=cats, showmeans=True)
    ax.set_title(title); ax.set_xlabel(x_col); ax.set_ylabel(y_col)
    st.pyplot(fig)


def make_hist(df, col, bins=30, title="Histogram"):
    x = pd.to_numeric(df[col], errors="coerce").dropna()
    if x.empty:
        st.info("Histogram iÃ§in yeterli veri yok."); return
    fig, ax = plt.subplots()
    ax.hist(x, bins=bins)
    ax.set_title(title); ax.set_xlabel(col); ax.set_ylabel("Frekans")
    st.pyplot(fig)


def export_df(df, name="export.csv"):
    csv = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("â¬‡ï¸ CSV indir", data=csv, file_name=name, mime="text/csv")


# ======== Ã–ZEL: Kategorik normalizasyon fonksiyonlarÄ± ======== #
def normalize_blood_group(x: str | None):
    """
    'A Rh (+) Pozitif' -> 'A Rh(+)', 'O Rh -' -> 'O Rh(-)', '0 +' -> 'O Rh(+)'.
    AnlaÅŸÄ±lmazsa None dÃ¶ner.
    """
    if not isinstance(x, str): return None
    u = x.strip().upper().replace("Ä°", "I")
    if not u: return None

    # ABO (AB, A, B, O/0)
    abo = None
    if re.search(r"\bAB\b", u):
        abo = "AB"
    elif re.search(r"\bA\b", u):
        abo = "A"
    elif re.search(r"\bB\b", u):
        abo = "B"
    elif re.search(r"\bO\b|\b0\b", u):
        abo = "O"

    # Rh (+ / - / POS/POZ / NEG / RH+ / RH- / + / -)
    rh = None
    if re.search(r"\+|\bPOS(ITIVE)?\b|\bPOZ(ITIF)?\b|\bRH\+\b", u):
        rh = "Rh(+)"
    elif re.search(r"-|\bNEG(ATIVE)?\b|\bRH-\b", u):
        rh = "Rh(-)"

    if abo is None and rh is None:
        return None
    return f"{abo or ''} {rh or ''}".strip()


def _rx(pattern):  # kÃ¼Ã§Ã¼k yardÄ±mcÄ±
    return re.compile(pattern, flags=re.I)


# Normalizasyon: Anormal Hb metinlerinden geniÅŸ varyant seti
ANORMAL_HB_PATTERNS = [
    (_rx(r"(s-?\s*beta|s *Î²|s[- ]?beta[- ]?tal)"), "Hb S-Î²-thal"),
    (_rx(r"\b(hbs?|sickle)\b.*(trait|het|carrier|tasiy|taÅŸiy)"), "HbS trait"),
    (_rx(r"\b(hbs?)\b"), "HbS"),
    (_rx(r"\bhbc\b"), "HbC"),
    (_rx(r"\bhbd\b"), "HbD"),
    (_rx(r"\bhbe\b"), "HbE"),
    (_rx(r"\bhbg\b"), "HbG"),
    (_rx(r"\bhbj\b"), "HbJ"),            # <- dÃ¼zeltilen satÄ±r
    (_rx(r"o[- ]?arab|hb ?o\b"), "HbO-Arab"),
    (_rx(r"lepore"), "Hb Lepore"),
    (_rx(r"\bhb ?h\b|\bhemoglobin\s*h\b"), "HbH"),
    (_rx(r"bart'?s|barts|\bhb\s*bart"), "Hb Bartâ€™s"),
    (_rx(r"\bhba2\b|a2\b|h?b?\s*a2\s*(yuk|art|â†‘|\+)|a2\s*high"), "HbA2â†‘"),
    (_rx(r"\bhbf\b|\bf\b|h?b?\s*f\s*(yuk|art|â†‘|\+)"), "HbFâ†‘"),
    (_rx(r"beta.*(thal|talas)|Î².*thal"), "Î²-thal trait"),
    (_rx(r"alpha.*(thal|talas)|Î±.*thal"), "Î±-thal trait"),
    (_rx(r"\bnormal\b|neg(atif)?|(-)\s?bulunmadÄ±"), "Normal"),
]

def norm_anormal_hb_text(x: str | None):
    if not isinstance(x, str): return None
    s = x.strip()
    if not s: return None
    for rx, label in ANORMAL_HB_PATTERNS:
        if rx.search(s):
            return label
    return None


# ============== Cache'li Dosya Okuma ============== #
@st.cache_data(show_spinner=False)
def read_one_excel_cached(file_bytes: bytes, engine_hint: str = "openpyxl") -> pd.DataFrame:
    bio = io.BytesIO(file_bytes)
    return pd.read_excel(bio, engine=engine_hint)


def read_many_excels(files):
    def _read(upl):
        try:
            data = upl.read()
            df = read_one_excel_cached(data)
            return (upl.name, df, None)
        except Exception as e:
            return (upl.name, None, str(e))
    out = []
    with ThreadPoolExecutor(max_workers=min(8, len(files))) as ex:
        for name, df, err in ex.map(_read, files):
            out.append((name, df, err))
    return out


# ============== UI BaÅŸlangÄ±Ã§ ============== #
st.title("âš¡ Tetkik Analiz â€” Ã‡oklu Dosya (Optimize, Revize)")
st.caption("BÃ¼yÃ¼k veri ve Ã§oklu dosya iÃ§in hÄ±zlandÄ±rÄ±lmÄ±ÅŸ sÃ¼rÃ¼m (kan grubu/anormal Hb normalizasyonu dÃ¢hil).")

uploads = st.file_uploader("Excel dosyalarÄ± (.xlsx, .xls) â€” Ã‡oklu seÃ§im", type=["xlsx", "xls"], accept_multiple_files=True)

use_polars = st.checkbox("Polars hÄ±zlandÄ±rmayÄ± dene (kuruluysa)", value=('pl' in globals() and HAS_POLARS),
                         help="Polars kurulu deÄŸilse otomatik devre dÄ±ÅŸÄ± kalÄ±r.")

if not uploads:
    st.info("Birden Ã§ok dosyayÄ± aynÄ± anda seÃ§in (Ã¶rn. 12 dosya).")
    st.stop()

with st.spinner("Dosyalar okunuyor..."):
    results = read_many_excels(uploads)

frames, skipped = [], []
for name, tmp, err in results:
    if err:
        skipped.append((name, f"Okuma hatasÄ±: {err}")); continue
    miss = check_columns(tmp)
    if miss:
        skipped.append((name, f"Eksik sÃ¼tun: {miss}")); continue
    tmp["SOURCE_FILE"] = name
    frames.append(tmp)

if skipped:
    for nm, msg in skipped:
        st.warning(f"'{nm}' atlandÄ± â†’ {msg}")

if not frames:
    st.error("Uygun veri iÃ§eren dosya bulunamadÄ±."); st.stop()

df = pd.concat(frames, ignore_index=True)
df = downcast_df(df)

if use_polars and HAS_POLARS:
    try: pl_df = pl.from_pandas(df)
    except Exception: 
        use_polars, pl_df = False, None
else:
    pl_df = None


# ================= Filtreler ================= #
left, right = st.columns([3, 2])
with left:
    unique_tests = sorted([str(x) for x in df["TETKIK_ISMI"].dropna().unique()])
    selected_tests = st.multiselect("Analiz edilecek tetkikler", options=unique_tests, default=unique_tests)
with right:
    sexes = [str(x) for x in df["CINSIYET"].dropna().unique()]
    chosen_sex = st.multiselect("Cinsiyet filtresi", options=sexes, default=sexes)
    files = [str(x) for x in df["SOURCE_FILE"].dropna().unique()]
    chosen_files = st.multiselect("Dosya filtresi", options=files, default=files)

# --- 99 ile baÅŸlayan TCKN filtreleme kontrolÃ¼ ---
st.markdown("### ðŸ§¾ TCKN Filtre SeÃ§imi")

tckn_filter = st.selectbox(
    "TCKN filtrele:",
    ["Hepsi", "Sadece gerÃ§ek TCKN", "Sadece 99'lu TCKN"],
    index=1,  # VarsayÄ±lan: Sadece gerÃ§ek TCKN
    help="99 ile baÅŸlayanlar genelde geÃ§ici kayÄ±tlardÄ±r."
)

work = df.copy()
# --- TCKN filtreleme ---
if "TCKIMLIK_NO" in work.columns:
    tckn_str = work["TCKIMLIK_NO"].astype(str)

    if tckn_filter == "Sadece gerÃ§ek TCKN":
        work = work[~tckn_str.str.startswith("99", na=False)]

    elif tckn_filter == "Sadece 99'lu TCKN":
        work = work[tckn_str.str.startswith("99", na=False)]

if chosen_sex:
    work = work[work["CINSIYET"].astype(str).isin(chosen_sex)]
if chosen_files:
    work = work[work["SOURCE_FILE"].astype(str).isin(chosen_files)]
if selected_tests:
    work = work[work["TETKIK_ISMI"].astype(str).isin(selected_tests)]

# GÃ¼vence: numeric kopya olsun
work = add_numeric_copy(work)


# ================= VARYANT Ã–ZETÄ° (etiketleme) ================= #
A2_KEYS = {"A2/","HbA2","HbA2 (%)","Hb A2","Hb A2 (%)"}
F_KEYS  = {"F/","HbF","HbF (%)","Hb F","Hb F (%)"}

# Ã–ncelik sÄ±rasÄ± (varsa bu sÄ±rayla tercih edilir)
PRIORITY_ORDER = [
    "Hb S-Î²-thal","Sickle cell disease","HbS","HbS trait",
    "HbC","HbD","HbE","HbG","HbJ","HbO-Arab","Hb Lepore","HbH","Hb Bartâ€™s",
    "Î²-thal trait","Î±-thal trait",
    "HbA2â†‘","HbFâ†‘","Normal"
]

def _priority_key(tag: str) -> int:
    try: return PRIORITY_ORDER.index(tag)
    except ValueError: return len(PRIORITY_ORDER) + 1  # listede yoksa en sona

def pick_variant_tag(g: pd.DataFrame) -> str | None:
    g = add_numeric_copy(g.copy())
    g["TETKIK_ISMI"] = g["TETKIK_ISMI"].astype(str)
    tags = []

    # 1) Anormal Hb/ metinlerinden
    txt = g.loc[g["TETKIK_ISMI"] == "Anormal Hb/", "TEST_DEGERI"].dropna().astype(str)
    for v in txt:
        t = norm_anormal_hb_text(v)
        if t: tags.append(t)

    # 2) A2/F eriÅŸkin eÅŸikleri
    if g["TETKIK_ISMI"].isin(A2_KEYS).any():
        a2 = g.loc[g["TETKIK_ISMI"].isin(A2_KEYS), "__VAL_NUM__"].dropna()
        if not a2.empty and a2.max() >= 3.5: tags.append("HbA2â†‘")
    if g["TETKIK_ISMI"].isin(F_KEYS).any():
        f = g.loc[g["TETKIK_ISMI"].isin(F_KEYS), "__VAL_NUM__"].dropna()
        if not f.empty and f.max() > 2.0: tags.append("HbFâ†‘")

    # 3) HPLC/CE piklerinden (pozitifse)
    for k, var_name in VARIANT_NUMERIC_TESTS.items():
        m = g["TETKIK_ISMI"] == k
        if m.any():
            vv = g.loc[m, "__VAL_NUM__"].dropna()
            if not vv.empty and (vv > 0).any():
                tags.append(var_name)

    if not tags:
        return None

    # En Ã¶ncelikli etiketi seÃ§
    tags_unique = sorted(set(tags), key=_priority_key)
    return tags_unique[0]


if "VARIANT_TAG" not in work.columns:
    var_map = (work.groupby("PROTOKOL_NO", group_keys=False)
                  .apply(lambda g: pd.Series({"VARIANT_TAG": pick_variant_tag(g)}))
                  .reset_index())
    work = work.merge(var_map, on="PROTOKOL_NO", how="left")

st.header("ðŸ“‹ Varyant Ã–zeti â€” eriÅŸkin eÅŸikleri ile")

# <- SABÄ°T LÄ°STE YOK: Dinamik!
present = sorted([t for t in set(work["VARIANT_TAG"].dropna())], key=_priority_key)
variant_choice = st.selectbox("Varyant seÃ§:", ["(TÃ¼mÃ¼)"] + present, index=0)

base_v = work.copy()
if variant_choice != "(TÃ¼mÃ¼)":
    base_v = base_v[base_v["VARIANT_TAG"] == variant_choice]

# 1) TÃ¼mÃ¼ iÃ§in frekans
if variant_choice == "(TÃ¼mÃ¼)":
    freq = (work["VARIANT_TAG"].value_counts(dropna=True)
            .rename_axis("Varyant").to_frame("N").reset_index())
    total = int(freq["N"].sum()) if not freq.empty else 0
    if total > 0: freq["%"] = (freq["N"]/total*100).round(2)
    st.subheader("Varyant FrekanslarÄ±")
    st.dataframe(freq, use_container_width=True)
    st.download_button("â¬‡ï¸ Varyant frekanslarÄ± (CSV)",
                       data=freq.to_csv(index=False).encode("utf-8-sig"),
                       file_name="varyant_frekans.csv", mime="text/csv")

# 2) SeÃ§ilen varyant iÃ§in â™€/â™‚ Mean Â± SD tablosu
def _mean_sd(s: pd.Series):
    s = pd.to_numeric(s, errors="coerce").dropna()
    return "â€”" if s.empty else f"{s.mean():.2f} Â± {s.std(ddof=1):.2f}"

PARAMS = {
    "Hemogram/HGB":  ("Hb (g/dL)",   "F: 11â€“15; M: 12â€“17"),
    "Hemogram/HCT":  ("HCT (%)",     "F: 36â€“46; M: 40â€“53"),
    "Hemogram/RBC":  ("RBC (Ã—10â¶)",  "F: 3.9â€“5.6; M: 4.5â€“6.0"),
    "Hemogram/RDW":  ("RDW (%)",     "11â€“16"),
    "Hemogram/MCV":  ("MCV (fL)",    "80â€“100"),
    "Hemogram/MCH":  ("MCH (pg)",    "27â€“34"),
    "Hemogram/MCHC": ("MCHC (g/dL)", "32â€“36"),
    "HbA":           ("HbA (%)",     "94â€“98"),
    "HbA2 (%)":      ("HbAâ‚‚ (%)",    "2â€“3.5"),
    "A2/":           ("HbAâ‚‚ (%)",    "2â€“3.5"),
    "HbF (%)":       ("Hb F (%)",    "0â€“2"),
    "F/":            ("Hb F (%)",    "0â€“2"),
}

table_fm = pd.DataFrame()
if variant_choice != "(TÃ¼mÃ¼)":
    rows = []
    for tetkik_key, (disp, ref) in PARAMS.items():
        subp = base_v[base_v["TETKIK_ISMI"] == tetkik_key].copy()
        if subp.empty: 
            continue
        subp = add_numeric_copy(subp)  # __VAL_NUM__ gÃ¼vence
        fem = _mean_sd(subp.loc[subp["CINSIYET"].astype(str).str.lower().str.startswith(("k","f")), "__VAL_NUM__"])
        male= _mean_sd(subp.loc[subp["CINSIYET"].astype(str).str.lower().str.startswith(("e","m")), "__VAL_NUM__"])
        rows.append({"Parameter": disp, "Female (Mean Â± SD)": fem, "Male (Mean Â± SD)": male, "Reference range": ref})
    table_fm = pd.DataFrame(rows)
    st.subheader("â™€/â™‚ Mean Â± SD (seÃ§ilen varyant)")
    if table_fm.empty:
        st.info("Bu varyant iÃ§in parametrik veri bulunamadÄ±.")
    else:
        st.dataframe(table_fm, use_container_width=True)
        st.download_button("â¬‡ï¸ Tablo #1 (CSV)",
                           data=table_fm.to_csv(index=False).encode("utf-8-sig"),
                           file_name="varyant_ozet_female_male.csv", mime="text/csv")

# 3) BirleÅŸik tablo (Varyant FrekanslarÄ± + MeanÂ±SD)
if variant_choice != "(TÃ¼mÃ¼)":
    freq_part = locals().get("freq", pd.DataFrame(columns=["Varyant","N","%"])).copy()
    if not freq_part.empty:
        freq_part = freq_part.rename(columns={"Varyant":"BaÅŸlÄ±k"})
        freq_part.insert(0,"BÃ¶lÃ¼m","Varyant FrekanslarÄ±")
    msd_part = table_fm.copy()
    if not msd_part.empty:
        msd_part = msd_part.rename(columns={"Parameter":"BaÅŸlÄ±k"})
        msd_part.insert(0,"BÃ¶lÃ¼m","â™€/â™‚ Mean Â± SD")
    cols = ["BÃ¶lÃ¼m","BaÅŸlÄ±k","N","%","Female (Mean Â± SD)","Male (Mean Â± SD)","Reference range"]
    for dfc in (freq_part, msd_part):
        for c in cols:
            if c not in dfc.columns: dfc[c] = None
    combined_df = pd.concat([freq_part[cols], msd_part[cols]], ignore_index=True)
    st.subheader("ðŸ§© BirleÅŸik Tablo")
    st.dataframe(combined_df, use_container_width=True)
    st.download_button("â¬‡ï¸ BirleÅŸik tablo (CSV)",
                       data=combined_df.to_csv(index=False).encode("utf-8-sig"),
                       file_name=f"birlesik_{variant_choice}.csv",
                       mime="text/csv")


# ================= Kategorik Veri Analizi â€” Benzersiz DeÄŸerler ================= #
st.header("ðŸ§¬ Kategorik Veri Analizi â€” Benzersiz DeÄŸerler")
for test_name in ["Kan Grubu/", "Anormal Hb/"]:
    sub = work[work["TETKIK_ISMI"].astype(str) == test_name].copy()
    if sub.empty:
        st.warning(f"{test_name} verisi bulunamadÄ±.")
        continue

    st.subheader(f"ðŸ” {test_name}")

    raw_text = sub["TEST_DEGERI"].astype(str).str.strip()
    if test_name == "Kan Grubu/":
        normalized = raw_text.map(normalize_blood_group)
    else:
        normalized = raw_text.map(norm_anormal_hb_text)

    # ============ Ã–ZEL AKIÅž: ANORMAL Hb/ ============
    if test_name == "Anormal Hb/":
        # 1) Ham yazÄ±m â†’ TC listesi (Frekans yerine)
        sub_nonempty = sub[raw_text.ne("") & sub["TEST_DEGERI"].notna()].copy()
        if sub_nonempty.empty:
            st.info("Anormal Hb/ iÃ§in dolu metin bulunamadÄ±.")
        else:
            # Her ham deÄŸer iÃ§in benzersiz TCKIMLIK_NO listesini Ã§Ä±kar
            map_tc = (
                sub_nonempty
                .assign(_val=raw_text.loc[sub_nonempty.index])
                .groupby("_val", dropna=False)["TCKIMLIK_NO"]
                .apply(lambda s: ", ".join(sorted({str(x) for x in s.dropna().astype(str)})) or "â€”")
                .reset_index()
                .rename(columns={"_val": "Ham DeÄŸer", "TCKIMLIK_NO": "TCKIMLIK_NO (liste)"})
            )
            st.markdown("**Ham yazÄ±mlar (TC listeli)**")
            st.dataframe(map_tc, use_container_width=True)
            st.download_button(
                "â¬‡ï¸ AnormalHb_ham_yazim_TC_listesi.csv",
                data=map_tc.to_csv(index=False).encode("utf-8-sig"),
                file_name="AnormalHb_ham_yazim_TC_listesi.csv",
                mime="text/csv",
            )

        # 2) DÃ¼zenlenebilir tablo (CLEAN kolonu)
        edit_cols = [c for c in ["PROTOKOL_NO","TCKIMLIK_NO","CINSIYET","SOURCE_FILE","TEST_DEGERI"] if c in sub_nonempty.columns]
        edit_df = sub_nonempty[edit_cols].copy()
        clean_col = "ANORMAL_HB_CLEAN"
        # Daha Ã¶nce varsa koru; yoksa normalize Ã¶neriyi doldur
        if clean_col in sub_nonempty.columns:
            edit_df[clean_col] = sub_nonempty[clean_col].astype(str)
        else:
            edit_df[clean_col] = normalized.loc[sub_nonempty.index].fillna("").astype(str)

        st.markdown("**DÃ¼zenlenebilir tablo (CLEAN deÄŸerini yazÄ±n)**")
        edited = st.data_editor(
            edit_df,
            use_container_width=True,
            key="anormalhb_editor",
            column_config={
                "TEST_DEGERI": st.column_config.TextColumn(label="ORIGINAL", help="Ham deÄŸer", disabled=True),
                clean_col: st.column_config.TextColumn(label="CLEAN (dÃ¼zenlenebilir)"),
            },
        )
        col_apply, col_over = st.columns([1,1])
        with col_apply:
            apply_now = st.button("âœ… Uygula ve kaydet (oturum iÃ§i)", key="apply_anormalhb")
        with col_over:
            overwrite_main = st.checkbox("ORIGINAL sÃ¼tununu da CLEAN ile deÄŸiÅŸtir", value=False, key="over_anormalhb")

        if apply_now and not edited.empty:
            # PROTOKOL_NO + ORIGINAL eÅŸleÅŸmesine gÃ¶re geri yaz
            upd = edited[[c for c in ["PROTOKOL_NO","TEST_DEGERI",clean_col] if c in edited.columns]].copy()
            upd.rename(columns={clean_col: "__CLEAN_TMP__"}, inplace=True)

            key_proto = work["PROTOKOL_NO"].astype(str) if "PROTOKOL_NO" in work.columns else pd.Series("", index=work.index)
            key_test  = work["TEST_DEGERI"].astype(str).str.strip()

            for _, r in upd.iterrows():
                proto = str(r.get("PROTOKOL_NO",""))
                orig  = str(r.get("TEST_DEGERI","")).strip()
                mask = (key_proto == proto) & (key_test == orig)
                work.loc[mask, clean_col] = r["__CLEAN_TMP__"]
                if overwrite_main:
                    work.loc[mask, "TEST_DEGERI"] = r["__CLEAN_TMP__"]

            st.success("Anormal Hb/ CLEAN deÄŸerleri uygulandÄ±.")
            st.download_button(
                "â¬‡ï¸ GÃ¼ncellenmiÅŸ veri (CSV)",
                data=work.to_csv(index=False).encode("utf-8-sig"),
                file_name="guncellenmis_veri.csv",
                mime="text/csv",
            )

        # 3) SeÃ§ince hastanÄ±n/protokolÃ¼n tÃ¼m tetkikleri
        st.markdown("**HÄ±zlÄ± inceleme: bir hasta veya protokol seÃ§in**")
        tcs  = sorted({str(x) for x in sub_nonempty.get("TCKIMLIK_NO", pd.Series(dtype=object)).dropna().astype(str)})
        prot = sorted({str(x) for x in sub_nonempty.get("PROTOKOL_NO", pd.Series(dtype=object)).dropna().astype(str)})

        tabs_sel = st.tabs(["Hasta ile seÃ§", "Protokol ile seÃ§"])
        with tabs_sel[0]:
            if tcs:
                sel_tc = st.selectbox("TCKIMLIK_NO", options=tcs, key="sel_tc_anormalhb")
                proto_for_tc = (
                    sub_nonempty.loc[sub_nonempty["TCKIMLIK_NO"].astype(str) == sel_tc, "PROTOKOL_NO"]
                    .astype(str).unique().tolist()
                ) if "PROTOKOL_NO" in sub_nonempty.columns else []
                all_tests = work[
                    (work["TCKIMLIK_NO"].astype(str) == sel_tc) &
                    (work["PROTOKOL_NO"].astype(str).isin(proto_for_tc))
                ].copy()
                show_cols = [c for c in ["PROTOKOL_NO","TETKIK_ISMI","TEST_DEGERI","CINSIYET","SOURCE_FILE"] if c in all_tests.columns]
                st.dataframe(all_tests[show_cols].sort_values(show_cols[:2]) if not all_tests.empty else all_tests, use_container_width=True)
            else:
                st.info("SeÃ§ilebilir hasta yok.")
        with tabs_sel[1]:
            if prot:
                sel_p = st.selectbox("PROTOKOL_NO", options=prot, key="sel_proto_anormalhb")
                all_tests = work[work["PROTOKOL_NO"].astype(str) == sel_p].copy()
                show_cols = [c for c in ["PROTOKOL_NO","TETKIK_ISMI","TEST_DEGERI","CINSIYET","SOURCE_FILE","TCKIMLIK_NO"] if c in all_tests.columns]
                st.dataframe(all_tests[show_cols].sort_values("TETKIK_ISMI") if not all_tests.empty else all_tests, use_container_width=True)
            else:
                st.info("SeÃ§ilebilir protokol yok.")

        continue  # Kan Grubu/ akÄ±ÅŸÄ±na geÃ§

    # ============ STANDART AKIÅž: KAN GRUBU/ ============
    # 1) Ham yazÄ±mlarÄ±n sayÄ±mÄ±
    sub_text = raw_text[raw_text.str.contains(r"[A-Za-zÄ°Ä±Ã–Ã¶ÃœÃ¼Ã‡Ã§ÅžÅŸ]", na=False)]
    if sub_text.empty:
        st.info("Harf iÃ§eren veri bulunamadÄ±.")
        value_counts = pd.DataFrame(columns=["Benzersiz DeÄŸer","Frekans"])
    else:
        value_counts = (
            sub_text.value_counts(dropna=False)
            .rename_axis("Benzersiz DeÄŸer")
            .reset_index(name="Frekans")
        )
    st.markdown("**Ham YazÄ±mlar**")
    st.dataframe(value_counts, use_container_width=True)
    st.download_button(
        f"â¬‡ï¸ {test_name.strip('/')}_benzersiz_degerler.csv",
        data=value_counts.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"{test_name.strip('/')}_benzersiz_degerler.csv",
        mime="text/csv"
    )

    # 2) Normalize kategorilerin sayÄ±mÄ±
    norm_counts = (
        normalized.value_counts(dropna=False)
        .rename_axis("Kategori (normalize)")
        .reset_index(name="N")
    )
    if not norm_counts.empty:
        totalN = int(norm_counts["N"].sum())
        norm_counts["%"] = (norm_counts["N"] / totalN * 100).round(2)
    else:
        norm_counts = pd.DataFrame(columns=["Kategori (normalize)","N","%"])

    st.markdown("**Normalize EdilmiÅŸ Kategoriler**")
    st.dataframe(norm_counts, use_container_width=True)
    st.download_button(
        f"â¬‡ï¸ {test_name.strip('/')}_normalize_frekans.csv",
        data=norm_counts.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"{test_name.strip('/')}_normalize_frekans.csv",
        mime="text/csv"
    )

    # 3) Kategorik genel frekans/ki-kare
    cat_name = "__CAT__"
    sub = sub.assign(**{cat_name: normalized})
    freq_all = (sub[cat_name].value_counts(dropna=False)
                .rename_axis("Kategori").to_frame("N").reset_index())
    totalN = int(freq_all["N"].sum()) if not freq_all.empty else 0
    if totalN:
        freq_all["%"] = (freq_all["N"]/totalN*100).round(2)
    else:
        freq_all["%"] = []
    freq_by_sex = (sub.pivot_table(index=cat_name, columns="CINSIYET",
                                   values="PROTOKOL_NO", aggfunc="count", fill_value=0)
                   .astype(int).reset_index().rename(columns={cat_name:"Kategori"}))
    chi2_msg = "Ki-kare uygulanamadÄ±."
    try:
        from scipy.stats import chi2_contingency
        cont = freq_by_sex.drop(columns=["Kategori"]).values
        if cont.sum() > 0 and cont.shape[1] > 1:
            chi2, p, dof, _ = chi2_contingency(cont)
            chi2_msg = f"Chi-square: Ï‡Â²={chi2:.2f}, df={dof}, p={p:.4g}"
    except Exception as e:
        chi2_msg = f"Hata: {e}"

    tabs = st.tabs(["Frekans", "Cinsiyet DaÄŸÄ±lÄ±mÄ±", "Ä°statistik"])
    with tabs[0]: st.dataframe(freq_all, use_container_width=True)
    with tabs[1]: st.dataframe(freq_by_sex, use_container_width=True)
    with tabs[2]: st.info(chi2_msg)

# ================= Genel Bilgiler ================= #
st.subheader("ðŸ”Ž Genel Bilgiler (BirleÅŸtirilmiÅŸ)")
c1, c2, c3, c4, c5 = st.columns(5)
c1.metric("Toplam SatÄ±r", f"{len(df):,}")
c2.metric("Benzersiz TCKIMLIK_NO", f"{df['TCKIMLIK_NO'].nunique():,}")
c3.metric("Benzersiz Tetkik", f"{df['TETKIK_ISMI'].nunique():,}")
c4.metric("Benzersiz Cinsiyet", f"{df['CINSIYET'].nunique():,}")
c5.metric("Dosya SayÄ±sÄ±", f"{df['SOURCE_FILE'].nunique():,}")

with st.expander("Ham Veri Ã–n Ä°zleme (limitli)"):
    st.dataframe(work.head(DISPLAY_LIMIT), use_container_width=True)
    st.caption(f"YalnÄ±zca ilk {DISPLAY_LIMIT} satÄ±r gÃ¶rÃ¼ntÃ¼lenir.")


# ================= HÄ±zlÄ± Ã–zetler ================= #
st.header("âš™ï¸ HÄ±zlÄ± Ã–zet ve KÄ±rÄ±lÄ±mlar")
colA, colB = st.columns(2)
with colA:
    st.write("**Cinsiyete GÃ¶re TanÄ±mlayÄ±cÄ±lar (SeÃ§imdeki veri)**")
    sex_summary = summarize_sex_counts(work)
    st.dataframe(sex_summary, use_container_width=True)
with colB:
    st.write("**Dosyaya GÃ¶re SatÄ±r & Hasta & Tetkik SayÄ±sÄ±**")
    per_file = work.groupby("SOURCE_FILE").agg(
        N=("PROTOKOL_NO", "size"),
        Hasta_Sayisi=("TCKIMLIK_NO", "nunique"),
        Tetkik_Sayisi=("TETKIK_ISMI", "nunique")
    ).reset_index()
    st.dataframe(per_file, use_container_width=True)
    export_df(per_file, "dosya_bazinda_ozet_filtreli.csv")


# ================= Tetkik BazlÄ± Analiz (SeÃ§im) ================= #
st.header("ðŸ“Š Tetkik BazlÄ± Analiz (SeÃ§im)")
results_rows = []
overall_pool = []  # GENEL HAVUZ

for test_name in selected_tests:
    if test_name in CATEGORICAL_TESTS:
        continue  # Kan Grubu/ & Anormal Hb/ Ã¼stte iÅŸlendi

    sub = work[work["TETKIK_ISMI"].astype(str) == test_name].copy()
    if sub.empty: 
        continue

    use_threshold = st.checkbox(
        f"â€˜{test_name}â€™ iÃ§in eriÅŸkin eÅŸiÄŸini uygula",
        value=(test_name in THRESHOLDS),
        key=f"th_{test_name}"
    )
    use_gt_zero  = st.checkbox(
        f"â€˜{test_name}â€™ iÃ§in sadece > 0 deÄŸerleri dahil et",
        value=(test_name in GT_ZERO_DEFAULT),
        key=f"gt0_{test_name}"
    )
    sub_work = sub[sub["__VAL_NUM__"].notna()].copy()
    if use_threshold and test_name in THRESHOLDS:
        sub_work = sub_work[apply_threshold(sub_work["__VAL_NUM__"], THRESHOLDS[test_name])]
        st.caption(f"EÅŸik: {THRESHOLDS[test_name][0]} {THRESHOLDS[test_name][1]}")
    elif use_gt_zero:
        sub_work = sub_work[sub_work["__VAL_NUM__"] > 0]
        st.caption("Filtre: > 0")
    if sub_work.empty:
        st.warning("Filtre sonrasÄ± satÄ±r bulunamadÄ±."); 
        continue

    # TanÄ±mlayÄ±cÄ±lar ve normalite
    stats_overall = descr_stats_fast(sub_work["__VAL_NUM__"])
    normal_flag_label   = normality_flag(sub_work["__VAL_NUM__"])
    norm_label, norm_p_disp = normality_test_with_p(sub_work["__VAL_NUM__"])

    # GENEL havuza deÄŸer ekle
    overall_pool.extend(pd.to_numeric(sub_work["__VAL_NUM__"], errors="coerce").dropna().tolist())

    # KÄ±rÄ±lÄ±mlar
    by_sex  = (sub_work.groupby("CINSIYET", dropna=False)["__VAL_NUM__"]
               .agg(count="count", mean="mean", std="std", min="min", median="median", max="max")).reset_index()
    by_file = (sub_work.groupby("SOURCE_FILE", dropna=False)["__VAL_NUM__"]
               .agg(count="count", mean="mean", std="std", min="min", median="median", max="max")).reset_index()
    _msg_df = sub_work.rename(columns={"__VAL_NUM__": "VAL"})
    msg, _ = nonparametric_test_by_group(_msg_df, "VAL", "CINSIYET")

    # SonuÃ§ satÄ±rÄ±
    mean_pm_sd = (
        "â€”" if np.isnan(stats_overall["mean"]) or np.isnan(stats_overall["std"])
        else f"{stats_overall['mean']:.2f} Â± {stats_overall['std']:.2f}"
    )

    results_rows.append({
        "TETKIK_ISMI": test_name,
        "N": stats_overall["count"],
        "Mean": stats_overall["mean"],
        "Median": stats_overall["median"],
        "Std": stats_overall["std"],
        "Mean Â± SD": mean_pm_sd,
        "Min": stats_overall["min"],
        "Q1": stats_overall["q1"],
        "Q3": stats_overall["q3"],
        "Max": stats_overall["max"],
        "Normalite": normal_flag_label,
        "p (normalite)": norm_p_disp,
        "Test": msg
    })

    tabs = st.tabs(["TanÄ±mlayÄ±cÄ±", "Cinsiyet", "Dosya", "Ä°statistiksel Test", "Histogram", "Boxplot"])
    with tabs[0]: st.table(pd.DataFrame([stats_overall]))
    with tabs[1]: st.dataframe(by_sex, use_container_width=True)
    with tabs[2]: st.dataframe(by_file, use_container_width=True)
    with tabs[3]: st.info(msg)
    with tabs[4]:
        if st.checkbox(f"Histogram gÃ¶ster ({test_name})", value=False):
            make_hist(_msg_df, "VAL", bins=30, title=f"{test_name} - Histogram")
    with tabs[5]:
        if st.checkbox(f"Boxplot gÃ¶ster ({test_name})", value=False):
            make_boxplot(sub_work, "CINSIYET", "__VAL_NUM__", title=f"{test_name} - Cinsiyete GÃ¶re Boxplot")

    pos_cols = ["PROTOKOL_NO", "TCKIMLIK_NO", "CINSIYET", "SOURCE_FILE"]
    pos_cols = [c for c in pos_cols if c in sub_work.columns]
    pos_tbl = sub_work[pos_cols + ["__VAL_NUM__"]].sort_values("__VAL_NUM__", ascending=False)
    st.write("**Filtre sonrasÄ± kayÄ±tlar**")
    st.dataframe(pos_tbl, use_container_width=True)
    st.download_button(
        "â¬‡ï¸ TCKIMLIK_NO listesi (CSV)",
        data=pos_tbl.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"{test_name}_filtre_sonrasi.csv",
        mime="text/csv"
    )

if results_rows:
    st.header("ðŸ§¾ Toplu Ã–zet Tablosu (SeÃ§ili Tetkikler)")
    res_df = pd.DataFrame(results_rows)

    # === GENEL TOPLAM ===
    if len(overall_pool) > 0:
        overall_series = pd.Series(overall_pool)
        overall_stats = descr_stats_fast(overall_series)
        overall_norm_label, overall_norm_p = normality_test_with_p(overall_series)

        mean_pm_sd_overall = (
            "â€”" if np.isnan(overall_stats["mean"]) or np.isnan(overall_stats["std"])
            else f"{overall_stats['mean']:.2f} Â± {overall_stats['std']:.2f}"
        )

        overall_row = {
            "TETKIK_ISMI": "GENEL TOPLAM",
            "N": overall_stats["count"],
            "Mean": overall_stats["mean"],
            "Median": overall_stats["median"],
            "Std": overall_stats["std"],
            "Mean Â± SD": mean_pm_sd_overall,
            "Min": overall_stats["min"],
            "Q1": overall_stats["q1"],
            "Q3": overall_stats["q3"],
            "Max": overall_stats["max"],
            "Normalite": overall_norm_label,
            "p (normalite)": overall_norm_p,
            "Test": "â€”",
        }
        res_df = pd.concat([res_df, pd.DataFrame([overall_row])], ignore_index=True)

    st.dataframe(res_df, use_container_width=True)
    export_df(res_df, name="tetkik_ozet.csv")

st.caption("Not: Varyant listesi dinamiktir; Anormal Hb metinleri ve HPLC/CE pikleri ile tespit edilen TÃœM etiketler frekans tablosuna yansÄ±r.")
# ==== FILE END ====
