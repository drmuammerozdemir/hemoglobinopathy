# app.py
# -*- coding: utf-8 -*-
"""
ğŸ§ª Tetkik Analiz ArayÃ¼zÃ¼ â€” Ã‡oklu Dosya (Optimize, Revize)
- Ã‡oklu dosya hÄ±zlÄ± okuma (paralel + cache)
- Bellek optimizasyonu (downcast, categorical)
- Ä°steÄŸe baÄŸlÄ± Polars hÄ±zlandÄ±rma
- BÃ¼yÃ¼k tablolarÄ± gÃ¼venli gÃ¶stermeye yÃ¶nelik limitler
- Grafikler isteÄŸe baÄŸlÄ± (matplotlib; renk set edilmez)
- Kategorik analizlerde SAÄLAM normalizasyon:
    â€¢ Kan Grubu: A/B/AB/O/0 + Rh(+/-/poz/neg/rh+/rh-) â†’ tek tipe
    â€¢ Anormal Hb: HbS/HbC/HbD/HbE/HbA2â†‘/HbFâ†‘/Normal
- Hem ham yazÄ±mlar hem normalize edilmiÅŸ kategoriler ayrÄ± tablolar/CSV
- Ham yazÄ±mdan hasta/protokol seÃ§erek hastanÄ±n/protokolÃ¼n tÃ¼m tetkiklerini gÃ¶ster

Ã‡alÄ±ÅŸtÄ±rma:
    streamlit run app.py
"""

import io
import re
import math
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import psutil
from scipy import stats
from concurrent.futures import ThreadPoolExecutor


# ============== Ayarlar ============== #
st.set_page_config(page_title="Tetkik Analiz â€” Optimize", layout="wide")
# ============== SÄ°STEM Ä°ZLEME ============== #
def sistem_durumu():
    # RAM KullanÄ±mÄ±
    mem = psutil.virtual_memory()
    ram_kullanilan = mem.used / (1024 ** 3) # GB cinsinden
    ram_toplam = mem.total / (1024 ** 3)
    ram_yuzde = mem.percent

    # CPU KullanÄ±mÄ±
    cpu_yuzde = psutil.cpu_percent(interval=1) # 1 saniye Ã¶lÃ§Ã¼m yapar

    return ram_kullanilan, ram_toplam, ram_yuzde, cpu_yuzde

with st.sidebar:
    st.divider()
    st.markdown("### ğŸ–¥ï¸ Sistem Durumu")
    if st.checkbox("CanlÄ± Ä°zle", value=False):
        # AnlÄ±k deÄŸerleri al
        r_used, r_total, r_perc, c_perc = sistem_durumu()
        
        st.metric("CPU KullanÄ±mÄ±", f"%{c_perc}")
        st.metric("RAM KullanÄ±mÄ±", f"{r_used:.1f} / {r_total:.1f} GB", f"%{r_perc}")
        
        if r_perc > 90:
            st.error("âš ï¸ RAM Dolmak Ãœzere!")
        elif c_perc > 90:
            st.warning("ğŸ”¥ Ä°ÅŸlemci Tam YÃ¼kte!")
    st.divider()
    
REQ_COLS = ["PROTOKOL_NO", "TCKIMLIK_NO", "TETKIK_ISMI", "TEST_DEGERI", "CINSIYET", "YAS"]

# Kategorik (metin) testler
CATEGORICAL_TESTS = {"Kan Grubu/", "Anormal Hb/"}

# --- EriÅŸkin pozitiflik eÅŸikleri (TETKIK_ISMI anahtarlarÄ±) ---
THRESHOLDS = {
    "HbA2 (%)": (">=", 3.5),
    "A2/":      (">=", 3.5),
    "HbF (%)":  (">",  2.0),
    "F/":       (">",  2.0),
    "HbS (%)":  (">",  0.0),
    "HbC (%)":  (">",  0.0),
    "HbD (%)":  (">",  0.0),
    "HbE (%)":  (">",  0.0),
}
GT_ZERO_DEFAULT = {
    "HbS (%)","HbC (%)","HbD (%)","HbE (%)","HbF (%)","HbA2 (%)","A2/","F/",
    "C/","D/","E/","S/"
}
VARIANT_NUMERIC_TESTS = {
    "HbS (%)","HbC (%)","HbD (%)","HbE (%)","HbF (%)","HbA2 (%)","Anormal Hb/"
}

# --- YENÄ° VE GENÄ°ÅLETÄ°LMÄ°Å HALÄ° ---
PARAMS = {
    # --- YENÄ° EKLENDÄ° ---
    "YAS":             ("YaÅŸ (yÄ±l)",    "â€”"),
    # --- Hemogram Parametreleri ---
    "Hemogram/HGB":  ("Hb (g/dL)",    "F: 11â€“15; M: 12â€“17"),
    "Hemogram/HCT":  ("HCT (%)",      "F: 36â€“46; M: 40â€“53"),
    "Hemogram/RBC":  ("RBC (Ã—10â¶)",   "F: 3.9â€“5.6; M: 4.5â€“6.0"),
    "Hemogram/RDW":  ("RDW (%)",      "11â€“16"),
    "Hemogram/MCV":  ("MCV (fL)",     "80â€“100"),
    "Hemogram/MCH":  ("MCH (pg)",     "27â€“34"),
    "Hemogram/MCHC": ("MCHC (g/dL)", "32â€“36"),
    # --- Buraya diÄŸer hemogram parametrelerini ekleyin (Ã–RN) ---
    "Hemogram/PLT":  ("PLT (Ã—10Â³)",   "150-450"),
    "Hemogram/WBC":  ("WBC (Ã—10Â³)",   "4.0-11.0"),
    
    # --- HPLC Parametreleri (Mevcut) ---
    "Talasemi(HPLC) (A0)/":         ("HbA0 (%)",     "94â€“98"),
    "HbA0 (%)":      ("HbAâ‚‚ (%)",     "94â€“98"),
    "A0/":           ("HbAâ‚‚ (%)",     "94â€“98"), # A0 iÃ§in alternatif isim
    "HbA":           ("HbA (%)",      "94â€“98"),
    "HbA2 (%)":      ("HbAâ‚‚ (%)",     "2â€“3.5"),
    "A2/":           ("HbAâ‚‚ (%)",     "2â€“3.5"), # A2 iÃ§in alternatif isim
    "HbF (%)":       ("Hb F (%)",     "0â€“2"),
    "F/":            ("Hb F (%)",     "0â€“2"),   # F iÃ§in alternatif isim
    
    # --- YENÄ° EKLENEN HPLC VARYANTLARI ---
    "HbS (%)":       ("HbS (%)",      "0"),
    "S/":            ("HbS (%)",      "0"),   # S iÃ§in alternatif isim
    "HbC (%)":       ("HbC (%)",      "0"),
    "C/":            ("HbC (%)",      "0"),   # C iÃ§in alternatif isim
    "HbD (%)":       ("HbD (%)",      "0"),
    "D/":            ("HbD (%)",      "0"),   # D iÃ§in alternatif isim
    "HbE (%)":       ("HbE (%)",      "0"),
    "E/":            ("HbE (%)",      "0"),   # E iÃ§in alternatif isim
    # YENÄ° EKLENEN USV SATIRI (EÄŸer verinizde "USV/" gibi bir test ismi varsa)
    "USV/":          ("USV (%)",      "â€”"),
    "USV (%)":       ("USV (%)",      "â€”"),
}

DISPLAY_LIMIT = 400

MALE_TOKENS   = {"e","erkek","m","male","bay"}
FEMALE_TOKENS = {"k","kadÄ±n","kadin","f","female","bayan"}

# Polars mevcut mu?
try:
    import polars as pl
    HAS_POLARS = True
except Exception:
    HAS_POLARS = False


# ============== YardÄ±mcÄ±lar ============== #
def coerce_numeric(series: pd.Series) -> pd.Series:
    s = series.astype(str).str.replace(",", ".", regex=False).str.replace(" ", "", regex=False)
    return pd.to_numeric(s, errors="coerce")

# ----- P deÄŸeri yazÄ±m kuralÄ± (TÃ¼rkÃ§e ondalÄ±k) -----
def _fmt_p(p: float) -> str:
    if p is None or np.isnan(p):
        return "â€”"
    if p < 0.001:
        return "<0,001"
    if p < 0.05:
        return "<0,05"
    return f"{p:.3f}".replace(".", ",")

# ----- Normalite testi: n<=5000 Shapiro; bÃ¼yÃ¼k n KS (N(Î¼,Ïƒ)) -----
def normality_test_with_p(series: pd.Series, alpha: float = 0.05):
    x = pd.to_numeric(series, errors="coerce").dropna()
    n = len(x)
    if n < 3:
        return "yetersiz", "â€”"

    try:
        if n <= 5000:
            stat, p = stats.shapiro(x)
        else:
            mu = float(np.mean(x))
            sd = float(np.std(x, ddof=1))
            if sd == 0:
                return "yetersiz", "â€”"
            # H0: veri ~ N(mu, sd)
            stat, p = stats.kstest(x, 'norm', args=(mu, sd))

        label = "normal" if p >= alpha else "non-normal"
        return label, _fmt_p(p)
    except Exception:
        return "bilinmiyor", "â€”"

def add_numeric_copy(frame, src_col="TEST_DEGERI", out_col="__VAL_NUM__"):
    if out_col not in frame.columns:
        tmp = (frame[src_col].astype(str)
                .str.replace(",", ".", regex=False)
                .str.replace(" ", "", regex=False))
        frame[out_col] = pd.to_numeric(tmp, errors="coerce")
    return frame


def check_columns(df: pd.DataFrame):
    return [c for c in REQ_COLS if c not in df.columns]


def normalize_sex_label(value):
    if not isinstance(value, str): return None
    trimmed = value.strip()
    if not trimmed: return None
    low = trimmed.lower()
    if low in MALE_TOKENS: return "Erkek"
    if low in FEMALE_TOKENS: return "KadÄ±n"
    return trimmed


def _resolve_patient_sex(series: pd.Series) -> str:
    values = [v for v in pd.unique(series.dropna()) if isinstance(v, str) and v]
    if not values: return "Bilinmiyor"
    if len(values) == 1: return values[0]
    return "Ã‡akÄ±ÅŸma"


def summarize_sex_counts(frame: pd.DataFrame) -> pd.DataFrame:
    tmp = frame[["TCKIMLIK_NO", "CINSIYET"]].copy()
    tmp["CINSIYET"] = tmp["CINSIYET"].astype(str)
    tmp["__SEX_CANON__"] = tmp["CINSIYET"].map(normalize_sex_label).astype(object)
    s_rows = tmp["__SEX_CANON__"].where(tmp["__SEX_CANON__"].notna(), "Bilinmiyor")
    row_counts = (
        s_rows.value_counts(dropna=False)
        .rename_axis("CINSIYET").to_frame("SatÄ±r SayÄ±sÄ±")
    )
    with_id = tmp[tmp["TCKIMLIK_NO"].notna()].copy()
    if not with_id.empty:
        w = with_id.copy()
        w["__SEX_CANON__"] = w["__SEX_CANON__"].astype(object)
        patient_gender = (
            w.groupby("TCKIMLIK_NO")["__SEX_CANON__"]
             .apply(lambda s: _resolve_patient_sex(pd.Series(pd.unique(s.dropna()))))
             .reset_index(name="__SEX_RESOLVED__")
        )
        patient_counts = (
            patient_gender["__SEX_RESOLVED__"].fillna("Bilinmiyor")
            .value_counts(dropna=False)
            .rename_axis("CINSIYET").to_frame("Hasta (Benzersiz)")
        )
    else:
        patient_counts = pd.DataFrame(columns=["Hasta (Benzersiz)"])
    summary = row_counts.join(patient_counts, how="outer").fillna(0)
    summary["SatÄ±r SayÄ±sÄ±"] = summary["SatÄ±r SayÄ±sÄ±"].astype(int)
    if "Hasta (Benzersiz)" in summary.columns:
        summary["Hasta (Benzersiz)"] = summary["Hasta (Benzersiz)"].astype(int)
    else:
        summary["Hasta (Benzersiz)"] = 0
    total_rows = int(summary["SatÄ±r SayÄ±sÄ±"].sum())
    total_patients = int(summary["Hasta (Benzersiz)"].sum())
    summary["% SatÄ±r"]  = (summary["SatÄ±r SayÄ±sÄ±"] / total_rows * 100).round(2) if total_rows else np.nan
    summary["% Hasta"] = (summary["Hasta (Benzersiz)"] / total_patients * 100).round(2) if total_patients else np.nan
    summary = summary.reset_index()
    summary = summary[["CINSIYET","Hasta (Benzersiz)","% Hasta","SatÄ±r SayÄ±sÄ±","% SatÄ±r"]]
    return summary.sort_values("Hasta (Benzersiz)", ascending=False).reset_index(drop=True)


def downcast_df(df: pd.DataFrame) -> pd.DataFrame:
    if "TEST_DEGERI" in df.columns:
        df["TEST_DEGERI"] = df["TEST_DEGERI"].astype(str)
    for col in ["CINSIYET", "SOURCE_FILE"]: # "TETKIK_ISMI" buradan kaldÄ±rÄ±ldÄ±
        if col in df.columns:
            df[col] = df[col].astype("category")
    return df


def descr_stats_fast(x: pd.Series) -> dict:
    x = pd.to_numeric(x, errors="coerce")
    x = x[~x.isna()]
    if x.empty:
        return {"count":0,"mean":np.nan,"std":np.nan,"min":np.nan,"q1":np.nan,"median":np.nan,"q3":np.nan,"max":np.nan,"cv%":np.nan,"iqr":np.nan}
    q = np.percentile(x, [25, 50, 75])
    mean = float(x.mean())
    std = float(x.std(ddof=1)) if len(x) > 1 else 0.0
    cv  = (std/mean)*100 if mean!=0 else np.nan
    return {"count":int(x.size),"mean":mean,"std":std,"min":float(x.min()),"q1":float(q[0]),"median":float(q[1]),"q3":float(q[2]),"max":float(x.max()),"cv%":float(cv),"iqr":float(q[2]-q[0])}


def normality_flag(x: pd.Series, alpha=0.05) -> str:
    x = pd.to_numeric(x, errors="coerce").dropna()
    if len(x) < 3: return "yetersiz"
    try:
        if len(x) <= 5000:
            stat, p = stats.shapiro(x)
            return "normal" if p >= alpha else "non-normal"
        else:
            res = stats.anderson(x, dist="norm")
            crit = res.critical_values[2]
            return "normal" if res.statistic < crit else "non-normal"
    except Exception:
        return "bilinmiyor"


def apply_threshold(series, rule):
    op, cut = rule
    if op == ">=": return series >= cut
    if op == ">":  return series >  cut
    if op == "<=": return series <= cut
    if op == "<":  return series <  cut
    return series.notna()


def nonparametric_test_by_group(df, val_col, grp_col):
    groups = [g.dropna() for _, g in df.groupby(grp_col)[val_col]]
    groups = [pd.to_numeric(g, errors="coerce").dropna() for g in groups]
    groups = [g for g in groups if len(g) > 0]
    unique_groups = df[grp_col].dropna().unique()
    unique_groups = [g for g in unique_groups if df[df[grp_col] == g][val_col].notna().sum() > 0]
    if len(unique_groups) < 2:
        return "KarÅŸÄ±laÅŸtÄ±rma iÃ§in en az 2 grup gerekli.", None
    if len(unique_groups) == 2:
        gnames = list(unique_groups)
        x = pd.to_numeric(df[df[grp_col] == gnames[0]][val_col], errors="coerce").dropna()
        y = pd.to_numeric(df[df[grp_col] == gnames[1]][val_col], errors="coerce").dropna()
        if len(x) >= 1 and len(y) >= 1:
            stat, p = stats.mannwhitneyu(x, y, alternative="two-sided")
            return f"Mannâ€“Whitney U: U={stat:.2f}, p={p:.4g} ({gnames[0]} vs {gnames[1]})", ("MWU", stat, p, gnames[0], gnames[1])
        else:
            return "Gruplarda yeterli gÃ¶zlem yok.", None
    else:
        stat, p = stats.kruskal(*groups)
        return f"Kruskalâ€“Wallis: H={stat:.2f}, p={p:.4g} (grup sayÄ±sÄ±: {len(unique_groups)})", ("KW", stat, p, unique_groups)


def make_boxplot(df, x_col, y_col, title="Kutu GrafiÄŸi"):
    valid = df[[x_col, y_col]].copy()
    valid[y_col] = pd.to_numeric(valid[y_col], errors="coerce")
    valid = valid.dropna()
    if valid.empty:
        st.info("Grafik iÃ§in yeterli veri yok."); return
    cats = list(valid[x_col].astype(str).unique())
    data = [valid[valid[x_col].astype(str) == c][y_col].values for c in cats]
    fig, ax = plt.subplots()
    ax.boxplot(data, labels=cats, showmeans=True)
    ax.set_title(title); ax.set_xlabel(x_col); ax.set_ylabel(y_col)
    st.pyplot(fig)


def make_hist(df, col, bins=30, title="Histogram"):
    x = pd.to_numeric(df[col], errors="coerce").dropna()
    if x.empty:
        st.info("Histogram iÃ§in yeterli veri yok."); return
    fig, ax = plt.subplots()
    ax.hist(x, bins=bins)
    ax.set_title(title); ax.set_xlabel(col); ax.set_ylabel("Frekans")
    st.pyplot(fig)


def export_df(df, name="export.csv"):
    csv = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("â¬‡ï¸ CSV indir", data=csv, file_name=name, mime="text/csv")


# ======== Ã–ZEL: Kategorik normalizasyon fonksiyonlarÄ± ======== #
def normalize_blood_group(x: str | None):
    """
    'A Rh (+) Pozitif' -> 'A Rh(+)', 'O Rh -' -> 'O Rh(-)', '0 +' -> 'O Rh(+)'
    metin anlaÅŸÄ±lmazsa None dÃ¶ner.
    """
    if not isinstance(x, str): return None
    u = x.strip().upper().replace("Ä°", "I")
    if not u: return None

    # ABO (AB, A, B, O/0)
    abo = None
    if re.search(r"\bAB\b", u):
        abo = "AB"
    elif re.search(r"\bA\b", u):
        abo = "A"
    elif re.search(r"\bB\b", u):
        abo = "B"
    elif re.search(r"\bO\b|\b0\b", u):
        abo = "O"

    # Rh (+ / - / POS/POZ / NEG / RH+ / RH- / + / -)
    rh = None
    if re.search(r"\+|\bPOS(ITIVE)?\b|\bPOZ(ITIF)?\b|\bRH\+\b", u):
        rh = "Rh(+)"
    elif re.search(r"-|\bNEG(ATIVE)?\b|\bRH-\b", u):
        rh = "Rh(-)"

    if abo is None and rh is None:
        return None
    return f"{abo or ''} {rh or ''}".strip()


def norm_anormal_hb_text(x: str | None):
    if not isinstance(x, str): return None
    s = x.upper().replace("Ä°","I").strip()
    
    if re.search(r"\bUSV\b|UNIDENTIFIED|TANIMLANAMAYAN", s): return "USV"
    
    # GÃœNCELLENMÄ°Å BLOK
    if re.search(r"S-?BETA ?0|S ?Î²0", s): return "Hb S-Î²0 thal"
    if re.search(r"S-?BETA ?\+|S ?Î²\+", s): return "Hb S-Î²+ thal"
    if re.search(r"S-?BETA|S ?Î²", s): return "Hb S-Î²-thal" # Genel
    
    if re.search(r"\bHBS\b|S TRAIT|S HET|HBS HET|HBS TAS|S-TASIY", s): return "HbS"
    # ... (kalanÄ± aynÄ±) ...
    if re.search(r"\bHBC\b", s): return "HbC"
    if re.search(r"\bHBD\b", s): return "HbD"
    if re.search(r"\bHBE\b", s): return "HbE"
    if re.search(r"\bA2\b|HBA2", s): return "HbA2â†‘ (B-thal Trait)" # Etiketi standart hale getirelim
    if re.search(r"\bF\b|HBF", s): return "HbFâ†‘"
    if re.search(r"\bNORMAL\b|NEG", s): return "Normal"
    return None
    


# ============== Cache'li Dosya Okuma ============== #
@st.cache_data(show_spinner=False)
def read_one_excel_cached(file_bytes: bytes, engine_hint: str = "openpyxl") -> pd.DataFrame:
    bio = io.BytesIO(file_bytes)
    return pd.read_excel(bio, engine=engine_hint)


def read_many_excels(files):
    def _read(upl):
        try:
            data = upl.read()
            df = read_one_excel_cached(data)
            return (upl.name, df, None)
        except Exception as e:
            return (upl.name, None, str(e))
    out = []
    with ThreadPoolExecutor(max_workers=min(8, len(files))) as ex:
        for name, df, err in ex.map(_read, files):
            out.append((name, df, err))
    return out


# ============== UI BaÅŸlangÄ±Ã§ ============== #
st.title("âš¡ Tetkik Analiz â€” Ã‡oklu Dosya (Optimize, Revize)")
st.caption("BÃ¼yÃ¼k veri ve Ã§oklu dosya iÃ§in hÄ±zlandÄ±rÄ±lmÄ±ÅŸ sÃ¼rÃ¼m (kan grubu/anormal Hb normalizasyonu dÃ¢hil).")

uploads = st.file_uploader("Excel dosyalarÄ± (.xlsx, .xls) â€” Ã‡oklu seÃ§im", type=["xlsx", "xls"], accept_multiple_files=True)

use_polars = st.checkbox("Polars hÄ±zlandÄ±rmayÄ± dene (kuruluysa)", value=('pl' in globals() and HAS_POLARS),
                         help="Polars kurulu deÄŸilse otomatik devre dÄ±ÅŸÄ± kalÄ±r.")

if not uploads:
    st.info("Birden Ã§ok dosyayÄ± aynÄ± anda seÃ§in (Ã¶rn. 12 dosya).")
    st.stop()

with st.spinner("Dosyalar okunuyor..."):
    results = read_many_excels(uploads)

frames, skipped = [], []
for name, tmp, err in results:
    if err:
        skipped.append((name, f"Okuma hatasÄ±: {err}")); continue
    miss = check_columns(tmp)
    if miss:
        skipped.append((name, f"Eksik sÃ¼tun: {miss}")); continue
    tmp["SOURCE_FILE"] = name
    frames.append(tmp)

if skipped:
    for nm, msg in skipped:
        st.warning(f"'{nm}' atlandÄ± â†’ {msg}")

if not frames:
    st.error("Uygun veri iÃ§eren dosya bulunamadÄ±."); st.stop()

df = pd.concat(frames, ignore_index=True)
df = downcast_df(df)

if use_polars and HAS_POLARS:
    try: pl_df = pl.from_pandas(df)
    except Exception: 
        use_polars, pl_df = False, None
else:
    pl_df = None


# ================= Filtreler ================= #
left, right = st.columns([3, 2])
with left:
    unique_tests = sorted([str(x) for x in df["TETKIK_ISMI"].dropna().unique()])
    selected_tests = st.multiselect("Analiz edilecek tetkikler", options=unique_tests, default=unique_tests)
with right:
    sexes = [str(x) for x in df["CINSIYET"].dropna().unique()]
    chosen_sex = st.multiselect("Cinsiyet filtresi", options=sexes, default=sexes)
    files = [str(x) for x in df["SOURCE_FILE"].dropna().unique()]
    chosen_files = st.multiselect("Dosya filtresi", options=files, default=files)

# --- TCKN FÄ°LTRESÄ° ---
st.markdown("### ğŸ§¾ TCKN Filtre SeÃ§imi (GeliÅŸmiÅŸ)")

tckn_filter = st.selectbox(
    "TCKN filtrele:",
    [
        "Hepsi", 
        "Sadece GerÃ§ek TCKN (11 hane, 99'la baÅŸlamayan)", 
        "Sadece YabancÄ±/GeÃ§ici (99'lu veya 11 hane olmayan)"
    ],
    index=1,  # VarsayÄ±lan: Sadece gerÃ§ek TCKN
    help="GerÃ§ek TCKN: 11 haneli ve 99 ile baÅŸlamayan. YabancÄ±/GeÃ§ici: 99 ile baÅŸlayan VEYA 11 hane olmayan (Dosya No vb.)."
)

# --- YENÄ° YAÅ FÄ°LTRESÄ° ---
st.markdown("### ğŸ‚ YaÅŸ Filtre SeÃ§imi")
age_filter = st.selectbox(
    "YaÅŸ filtresi:",
    ["TÃ¼mÃ¼", "Sadece 18 yaÅŸ ve Ã¼stÃ¼", "Sadece 18 yaÅŸ altÄ±"],
    index=0,  # VarsayÄ±lan: TÃ¼mÃ¼
    help="18 yaÅŸ altÄ± (<18), 18 yaÅŸ ve Ã¼stÃ¼ (>=18). Evlilik taramasÄ± iÃ§in 18 yaÅŸ Ã¼stÃ¼ Ã¶nerilir."
)

work = df.copy()

# --- YAS SÃœTUNUNU FÄ°LTRELEME Ä°Ã‡Ä°N SAYISALA DÃ–NÃœÅTÃœR (VE 1'LERÄ° TEMÄ°ZLE) ---
if "YAS" in work.columns:
    # 1'leri (placeholder) NaN yap, sonra sayÄ±ya Ã§evir
    work["YAS"] = pd.to_numeric(work["YAS"], errors='coerce').replace(1, np.nan)
else:
    # YAS sÃ¼tunu yoksa, filtrelemenin hata vermemesi iÃ§in boÅŸ bir NaN sÃ¼tun oluÅŸtur
    work["YAS"] = np.nan 

# --- TCKN filtreleme ---
if "TCKIMLIK_NO" in work.columns:
    # Ã–nce str yap, NaN'larÄ± boÅŸ string yap, boÅŸluklarÄ± temizle
    tckn_str = work["TCKIMLIK_NO"].astype(str).fillna("").str.strip()
    
    # Kural 1: 11 hane mi?
    is_11_digits = tckn_str.str.len() == 11
    
    # Kural 2: 99 ile mi baÅŸlÄ±yor?
    starts_with_99 = tckn_str.str.startswith("99")
    
    # Kural 3: "GerÃ§ek TCKN" maskesi
    # 11 haneli OLMALI VE 99 ile BAÅLAMAMALI
    is_gercek_mask = is_11_digits & (~starts_with_99)
    
    # Kural 4: "YabancÄ±/GeÃ§ici" maskesi
    # 99 ile BAÅLAMALI VEYA 11 hane OLMAMALI
    # (AyrÄ±ca boÅŸ olmayanlarÄ± alalÄ±m ki NaN'lar bu gruba girmesin)
    is_yabanci_mask = (starts_with_99 | (~is_11_digits)) & (tckn_str != "")

    if tckn_filter == "Sadece GerÃ§ek TCKN (11 hane, 99'la baÅŸlamayan)":
        work = work[is_gercek_mask]

    elif tckn_filter == "Sadece YabancÄ±/GeÃ§ici (99'lu veya 11 hane olmayan)":
        work = work[is_yabanci_mask]
    
    # 'Hepsi' seÃ§eneÄŸi iÃ§in hiÃ§bir ÅŸey yapma

# --- YaÅŸ filtreleme ---
if age_filter == "Sadece 18 yaÅŸ ve Ã¼stÃ¼":
    # YAS >= 18 olanlarÄ± al (NaN olmayanlarÄ± da otomatik alÄ±r)
    work = work[work["YAS"] >= 18]
elif age_filter == "Sadece 18 yaÅŸ altÄ±":
    # YAS < 18 olanlarÄ± al (NaN olmayanlarÄ± da otomatik alÄ±r)
    work = work[work["YAS"] < 18]
# 'TÃ¼mÃ¼' seÃ§iliyse bir ÅŸey yapma


if chosen_sex:
    work = work[work["CINSIYET"].astype(str).isin(chosen_sex)]
if chosen_files:
    work = work[work["SOURCE_FILE"].astype(str).isin(chosen_files)]
if selected_tests:
    work = work[work["TETKIK_ISMI"].astype(str).isin(selected_tests)]

# GÃ¼vence: numeric kopya olsun
work = add_numeric_copy(work)


# ================= VARYANT Ã–ZETÄ° (etiketleme) ================= #
A2_KEYS = {"A2/","HbA2","HbA2 (%)","Hb A2","Hb A2 (%)"}
F_KEYS  = {"F/","HbF","HbF (%)","Hb F","Hb F (%)"}
NUMVAR_FROM_TEST = {"C/":"HbC", "D/":"HbD", "E/":"HbE", "S/":"HbS"}

def pick_variant_tag(g: pd.DataFrame) -> str | None:
    g = add_numeric_copy(g.copy())
    g["TETKIK_ISMI"] = g["TETKIK_ISMI"].astype(str)
    
    # --- KURAL 0: MANUEL DÃœZELTME ---
    clean_col = "ANORMAL_HB_CLEAN"
    if clean_col in g.columns:
        clean_values = g[clean_col].dropna().astype(str)
        clean_values = clean_values[clean_values != ""]
        if not clean_values.empty:
            return clean_values.iloc[0] 

    # --- VERÄ°LERÄ° TOPLA ---
    def get_val(df, keys):
        if isinstance(keys, str): keys = {keys}
        all_keys = set(keys)
        for k in keys:
            if k in PARAMS: 
                display_name = PARAMS[k][0]
                all_keys.update({p_key for p_key, (disp, ref) in PARAMS.items() if disp == display_name})
        s = df.loc[df["TETKIK_ISMI"].isin(all_keys), "__VAL_NUM__"].dropna()
        return s.max() if not s.empty else np.nan

    # Gerekli parametreleri al (RBC ve HGB eklendi)
    mcv = get_val(g, {"Hemogram/MCV"})
    mch = get_val(g, {"Hemogram/MCH"})
    hgb = get_val(g, {"Hemogram/HGB"}) # Anemi kontrolÃ¼ iÃ§in
    rbc = get_val(g, {"Hemogram/RBC"}) # Mentzer Ä°ndeksi iÃ§in
    a2 = get_val(g, {"A2/"}) 
    f = get_val(g, {"F/"})   
    s = get_val(g, {"S/"})   
    a = get_val(g, {"HbA"})  
    c = get_val(g, {"C/"})   
    
    # GÃ¼venli DeÄŸerler (NaN kontrolÃ¼)
    mcv_val = mcv if pd.notna(mcv) else 999.0
    mch_val = mch if pd.notna(mch) else 999.0
    hgb_val = hgb if pd.notna(hgb) else 99.0
    rbc_val = rbc if pd.notna(rbc) else 0.0
    hba2_val = a2 if pd.notna(a2) else 0.0
    hbf_val = f if pd.notna(f) else 0.0
    hbs_val = s if pd.notna(s) else 0.0
    hbc_val = c if pd.notna(c) else 0.0 
    hba_present = (a > 1.0) if pd.notna(a) else False 
    
    # --- YENÄ° MANTIKLAR ---
    
    # 1. Mikrositoz / Hipokromi
    has_micro_hypo = (mcv_val < 80) or (mch_val < 27)
    
    # 2. Anemi KontrolÃ¼ (Cinsiyete gÃ¶re HGB eÅŸiÄŸi)
    is_anemic = False
    sex_series = g["CINSIYET"].dropna().astype(str).str.upper()
    if not sex_series.empty:
        sex = sex_series.iloc[0]
        # KadÄ±n < 12, Erkek < 13 (DSÃ– Kriterleri)
        if sex.startswith(('K', 'F')): 
            is_anemic = (hgb_val < 12.0)
        elif sex.startswith(('E', 'M')): 
            is_anemic = (hgb_val < 13.0)
        else:
            is_anemic = (hgb_val < 12.0) # Bilinmiyorsa gÃ¼venli sÄ±nÄ±r

    # 3. Mentzer Ä°ndeksi (MCV / RBC)
    # < 13 : Talasemi lehine
    # > 13 : Demir EksikliÄŸi lehine
    mentzer_index = (mcv_val / rbc_val) if rbc_val > 0 else 0
    
    tags = [] 

 # --- Kural 1: Kompleks Varyantlar (S-Beta Talasemi) ---
    # Bu blok A2 YÃœKSEKLÄ°ÄÄ°NE baktÄ±ÄŸÄ± iÃ§in S-Beta'larÄ± yakalar
    if has_micro_hypo and hba2_val > 3.5 and hbs_val > 50:
        if hba_present: tags.append("Hb S-Î²+ thal")
        else: tags.append("Hb S-Î²0 thal")
        
    # --- Kural 2: Orak HÃ¼cre Anemisi (HbSS) ---
    # HbA2 NORMAL (veya dÃ¼ÅŸÃ¼k), HbS Ã‡OK YÃœKSEK
    # Limiti 50 yerine 75 yapmak daha gÃ¼venlidir, Ã§Ã¼nkÃ¼ SS hastalarÄ± genelde %85-95 S olur.
    elif hbs_val > 75 and hba2_val <= 3.5: 
        tags.append("Sickle Cell Anemia (HbSS)")
    
    if has_micro_hypo and hba2_val <= 3.5 and (hbf_val >= 5 and hbf_val <= 20):
        tags.append("Î´Î²-thal Trait")
    # --- YENÄ° KURAL 1d: BETA TALASEMÄ° INTERMEDIA / MAJOR ÅÃœPHESÄ° ---
    # Kriter: Mikrositoz VAR ve HbF Ã‡ok YÃ¼ksek (> %10)
    # (Not: S-Beta yukarÄ±da elendiÄŸi iÃ§in buraya sadece Beta tÃ¼revleri gelir)
    if hbf_val > 10.0:
         if hba2_val > 3.5:
             tags.append("B-thal Intermedia (High A2/High F)")
         else:
             tags.append("B-thal Intermedia (High F only)")
    
    # SENARYO B: SÄ±nÄ±rda Ä°ntermedia (Orta F + Ciddi Anemi + Mikrositoz)
    # HbF %5-10 arasÄ± ama hasta ciddi anemik (Hb < 9) ve mikrositik ise -> Ä°ntermedia lehine
    elif (hbf_val >= 5.0 and hbf_val <= 10.0) and (hgb_val < 9.0) and has_micro_hypo:
        tags.append("B-thal Intermedia? (Mod. F + Severe Anemia)")
        
    if (hbs_val > 0) and (hbc_val > 0) and (not hba_present):
        tags.append("Hb S/C or S/O-Arab?") 

    # --- Kural 2: Metin BazlÄ± ---
    txt = g.loc[g["TETKIK_ISMI"] == "Anormal Hb/", "TEST_DEGERI"].dropna().astype(str)
    for v in txt:
        t = norm_anormal_hb_text(v) 
        if t: tags.append(t)

    # --- Kural 3: Basit Kantitatif ---
    # A) Klasik A2 TaÅŸÄ±yÄ±cÄ± (> 3.5)
    if hba2_val > 3.5: tags.append("HbA2â†‘ (B-thal Trait)")
    
    # B) Borderline (3.3 - 3.8 arasÄ±)
    # A2 3.3-3.8 arasÄ±
    criteria_a = (hba2_val >= 3.3 and hba2_val <= 3.8) and has_micro_hypo
    if criteria_a:
        tags.append("Borderline HbA2")
      
    # C) HPFH
    if hbf_val > 2.0:
        if (not has_micro_hypo) and hbf_val > 5.0: tags.append("HPFH?")
        else: tags.append("HbFâ†‘")

    # D) DEMÄ°R EKSÄ°KLÄ°ÄÄ° ve ALFA TALASEMÄ° AYRIMI (GELÄ°ÅMÄ°Å)
    # Kriter: Mikrositik/Hipokromik VE Normal A2 VE Normal F
    if has_micro_hypo and hba2_val < 3.3 and hbf_val < 5.0:
        
        # Senaryo 1: Anemik ise (HGB DÃ¼ÅŸÃ¼k)
        if is_anemic:
            if mentzer_index > 13:
                tags.append("Iron Deficiency Anemia (Probable)")
            else:
                # Hem anemik hem mentzer < 13 ise karÄ±ÅŸÄ±k/ÅŸÃ¼pheli
                tags.append("Iron Def./Alpha-thal? (Anemic)")
        
        # Senaryo 2: Anemik DeÄŸilse (HGB Normal ama MCV dÃ¼ÅŸÃ¼k)
        # Bu durum Alfa Talasemi TaÅŸÄ±yÄ±cÄ±lÄ±ÄŸÄ± iÃ§in Ã§ok tipiktir
        else:
            tags.append("Alpha-thal Carrier? (Probable)")

    # --- DiÄŸer Varyantlar ---
    for k, var_name in NUMVAR_FROM_TEST.items():
        val = get_val(g, {k}) 
        if pd.notna(val) and val > 0.1:
            if (var_name == "HbS" or var_name == "HbC") and ("Hb S/C or S/O-Arab?" in tags): continue 
            if var_name == "HbS" and val < 50: tags.append("HbS Trait")
            else: tags.append(var_name)
    
    if not tags: return "Normal (Assumed)" 
    
# --- FÄ°NAL Ã–NCELÄ°K LÄ°STESÄ° ---
    for p in [
        "Hb S-Î²0 thal", 
        "Hb S-Î²+ thal", 
        "Hb S/C or S/O-Arab?", 
        
        # YENÄ°: Intermedia/Major'larÄ± en Ã¼ste, S-Beta'nÄ±n altÄ±na ekledik
        "B-thal Intermedia (High A2/High F)",
        "B-thal Intermedia (High F only)",
        "B-thal Intermedia? (Mod. F + Severe Anemia)",
        
        "Î´Î²-thal Trait",
        "Hb S-Î²-thal",
        "HbS", "HbC", "HbD", "HbE", "USV",
        "HbS Trait",
        
        "Borderline HbA2",
        "HbA2â†‘ (B-thal Trait)",
        
        "Iron Deficiency Anemia (Probable)",
        "Iron Def./Alpha-thal? (Anemic)",
        "Alpha-thal Carrier? (Probable)",
        
        "HPFH?", "HbFâ†‘",
        "Normal (Assumed)", "Normal"
    ]:
        if p in tags: return p
            
    return tags[0]
    
if "VARIANT_TAG" not in work.columns:
    var_map = (work.groupby("PROTOKOL_NO", group_keys=False)
                   .apply(lambda g: pd.Series({"VARIANT_TAG": pick_variant_tag(g)}))
                   .reset_index())
    work = work.merge(var_map, on="PROTOKOL_NO", how="left")

# ================= VARYANT Ã–ZETÄ° (etiketleme ve istatistik) ================= #
if "VARIANT_TAG" not in work.columns:
    var_map = (work.groupby("PROTOKOL_NO", group_keys=False)
                   .apply(lambda g: pd.Series({"VARIANT_TAG": pick_variant_tag(g)}))
                   .reset_index())
    work = work.merge(var_map, on="PROTOKOL_NO", how="left")

st.header("ğŸ“‹ Varyant Ã–zeti â€” eriÅŸkin eÅŸikleri ile")

# Mevcut varyantlarÄ± bul
present_tags = sorted([t for t in work["VARIANT_TAG"].dropna().unique()])

# --- YENÄ°: "TOPLAM BETA GRUBU" TANIMI ---
# Bu liste, toplamak istediÄŸiniz 4 ana grubu iÃ§erir
BETA_CARRIER_GROUP = [
    "HbA2â†‘ (B-thal Trait)", # 1. Klasik
    "Borderline HbA2",      # 2. SÄ±nÄ±rda
    "Î´Î²-thal Trait",        # 3. YÃ¼ksek F'li TaÅŸÄ±yÄ±cÄ±
    "Hb S-Î²0 thal",         # 4a. S-Beta
    "Hb S-Î²+ thal"          # 4b. S-Beta
    "HbFâ†‘"                  # 5. HbFâ†‘
]

# Dropdown seÃ§eneklerini oluÅŸtur
custom_options = ["(TÃ¼mÃ¼)", ">> TOPLAM BETA TAÅIYICI GRUBU (Kombine) <<"] + present_tags
variant_choice = st.selectbox("Varyant seÃ§:", custom_options, index=0)

# --- FÄ°LTRELEME MANTIÄI ---
base_v = work.copy()

if variant_choice == "(TÃ¼mÃ¼)":
    # Hepsini gÃ¶ster, filtreleme yapma
    pass
elif variant_choice == ">> TOPLAM BETA TAÅIYICI GRUBU (Kombine) <<":
    # Sadece o 4 Ã¶zel grubu filtrele
    base_v = base_v[base_v["VARIANT_TAG"].isin(BETA_CARRIER_GROUP)]
    st.info(f"Bu grup ÅŸu varyantlarÄ±n toplamÄ±ndan oluÅŸmaktadÄ±r: {', '.join(BETA_CARRIER_GROUP)}")
else:
    # Tek bir varyant seÃ§ildiyse sadece onu filtrele
    base_v = base_v[base_v["VARIANT_TAG"] == variant_choice]


# 1) Frekans Tablosu (SeÃ§ime GÃ¶re)
freq = (base_v["VARIANT_TAG"].value_counts(dropna=True)
        .rename_axis("Varyant").to_frame("N").reset_index())
total = int(freq["N"].sum()) if not freq.empty else 0
if total > 0: freq["%"] = (freq["N"]/total*100).round(2)

if variant_choice == "(TÃ¼mÃ¼)":
    st.subheader("TÃ¼m VaryantlarÄ±n FrekansÄ±")
    st.dataframe(freq, use_container_width=True)
    st.download_button("â¬‡ï¸ Varyant frekanslarÄ± (CSV)",
                      data=freq.to_csv(index=False).encode("utf-8-sig"),
                      file_name="varyant_frekans.csv", mime="text/csv")
else:
    # Kombine veya tekil seÃ§im yapÄ±ldÄ±ÄŸÄ±nda da frekanslarÄ± gÃ¶sterelim
    st.write(f"**SeÃ§ilen Gruptaki DaÄŸÄ±lÄ±m (Toplam n={total}):**")
    st.dataframe(freq, use_container_width=True)


# 2) SeÃ§ilen varyant iÃ§in â™€/â™‚ Ä°statistik Tablosu (SeÃ§meli Format & SayÄ±lar)

# --- YardÄ±mcÄ± Format FonksiyonlarÄ± ---
def fmt(val):
    if pd.isna(val): return "â€”"
    s = f"{val:.2f}"
    if s.endswith(".00"): return s[:-3]
    return s

def _mean_sd(s: pd.Series):
    s = pd.to_numeric(s, errors="coerce").dropna()
    if s.empty: return "â€”"
    mean = s.mean()
    std = s.std(ddof=1)
    if pd.isna(std) or std == 0: return fmt(mean)
    return f"{fmt(mean)} Â± {fmt(std)}"

def _median_min_max(s: pd.Series):
    s = pd.to_numeric(s, errors="coerce").dropna()
    if s.empty: return "â€”"
    med = s.median()
    min_v = s.min()
    max_v = s.max()
    return f"{fmt(med)} [{fmt(min_v)}â€“{fmt(max_v)}]"

table_fm = pd.DataFrame()
if variant_choice != "(TÃ¼mÃ¼)":
    
    # --- YENÄ°: BAÅLIK Ä°Ã‡Ä°N HASTA SAYISINI HESAPLA ---
    # base_v 'long format' olduÄŸu iÃ§in (her test bir satÄ±r), benzersiz hasta sayÄ±sÄ±nÄ± bulmalÄ±yÄ±z
    unique_pats_stats = base_v[['PROTOKOL_NO', 'CINSIYET']].drop_duplicates(subset=['PROTOKOL_NO'])
    unique_pats_stats['Gender_Clean'] = unique_pats_stats['CINSIYET'].astype(str).map(normalize_sex_label).fillna('Bilinmiyor')
    
    n_stat_total = len(unique_pats_stats)
    n_stat_fem = len(unique_pats_stats[unique_pats_stats['Gender_Clean'] == 'KadÄ±n'])
    n_stat_male = len(unique_pats_stats[unique_pats_stats['Gender_Clean'] == 'Erkek'])
    
    # BaÅŸlÄ±k Metni
    header_text = f"â™€/â™‚ Ä°statistikler (Total: {n_stat_total}) [F: {n_stat_fem}, M: {n_stat_male}]"

    st.divider()
    
    # --- Format SeÃ§ici ve BaÅŸlÄ±k ---
    col_head, col_opt = st.columns([2, 2])
    with col_head:
        st.subheader(header_text)
    with col_opt:
        stat_mode = st.radio(
            "Tablo FormatÄ±:",
            ["Ortalama Â± Standart Sapma (Mean Â± SD)", "Ortanca [Min - Max] (Median [Min-Max])"],
            index=0,
            horizontal=True,
            key="variant_summary_stat_mode",
            label_visibility="collapsed"
        )
    
    # SeÃ§ime gÃ¶re baÅŸlÄ±klarÄ± ve fonksiyonu belirle
    if "Mean" in stat_mode:
        col_label_f = "Female (Mean Â± SD)"
        col_label_m = "Male (Mean Â± SD)"
        func_stat = _mean_sd
    else:
        col_label_f = "Female (Median [Min-Max])"
        col_label_m = "Male (Median [Min-Max])"
        func_stat = _median_min_max

    rows = []
    
    # ADIM 1 - YAÅ'Ä± Ã¶zel olarak iÅŸle
    if "YAS" in base_v.columns:
        age_data = base_v[['PROTOKOL_NO', 'CINSIYET', 'YAS']].dropna(subset=['PROTOKOL_NO', 'YAS']).drop_duplicates(subset=['PROTOKOL_NO'])
        # 1 yaÅŸ temizliÄŸi
        age_data['YAS'] = pd.to_numeric(age_data['YAS'], errors='coerce').replace(1, np.nan)
        age_data['Gender_Clean'] = age_data['CINSIYET'].astype(str).map(normalize_sex_label).fillna('Bilinmiyor')
        
        fem_age = age_data.loc[age_data['Gender_Clean'] == 'KadÄ±n', "YAS"]
        male_age = age_data.loc[age_data['Gender_Clean'] == 'Erkek', "YAS"]
        
        rows.append({
            "Parameter": "YaÅŸ (yÄ±l)", 
            col_label_f: func_stat(fem_age), 
            col_label_m: func_stat(male_age), 
            "Reference range": PARAMS.get("YAS", ("YaÅŸ", "â€”"))[1]
        })

    # ADIM 2 - Kalan PARAMS'larÄ± (Hemogram, HPLC) iÅŸle
    for tetkik_key, (disp, ref) in PARAMS.items():
        if tetkik_key == "YAS": continue 
            
        subp = base_v[base_v["TETKIK_ISMI"] == tetkik_key].copy()
        if subp.empty: continue
            
        subp = add_numeric_copy(subp)
        subp['Gender_Clean'] = subp['CINSIYET'].astype(str).map(normalize_sex_label).fillna('Bilinmiyor')
        
        fem = subp.loc[subp['Gender_Clean'] == 'KadÄ±n', "__VAL_NUM__"]
        male = subp.loc[subp['Gender_Clean'] == 'Erkek', "__VAL_NUM__"]
        
        rows.append({
            "Parameter": disp, 
            col_label_f: func_stat(fem), 
            col_label_m: func_stat(male), 
            "Reference range": ref
        })
    
    table_fm = pd.DataFrame(rows)
    
    if table_fm.empty:
        st.info("Bu varyant iÃ§in parametrik veri bulunamadÄ±.")
    else:
        st.dataframe(table_fm, use_container_width=True)
        
        file_suffix = "mean_sd" if "Mean" in stat_mode else "median_minmax"
        st.download_button(
            f"â¬‡ï¸ Tabloyu Ä°ndir (CSV - {file_suffix})",
            data=table_fm.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"varyant_ozet_{variant_choice}_{file_suffix}.csv", 
            mime="text/csv"
        )

# ================= Kategorik Veri Analizi â€” Benzersiz DeÄŸerler ================= #
st.header("ğŸ§¬ Kategorik Veri Analizi â€” Benzersiz DeÄŸerler")
for test_name in ["Kan Grubu/", "Anormal Hb/"]:
    sub = work[work["TETKIK_ISMI"].astype(str) == test_name].copy()
    if sub.empty:
        st.warning(f"{test_name} verisi bulunamadÄ±.")
        continue

    st.subheader(f"ğŸ” {test_name}")

    raw_text = sub["TEST_DEGERI"].astype(str).str.strip()
    if test_name == "Kan Grubu/":
        normalized = raw_text.map(normalize_blood_group)
    else:
        normalized = raw_text.map(norm_anormal_hb_text)

    # ============ Ã–ZEL AKIÅ: ANORMAL Hb/ (GÃœNCELLENMÄ°Å v2) ============
    if test_name == "Anormal Hb/":
        
        # 1. YENÄ° FÄ°LTRE: KullanÄ±cÄ±nÄ±n istediÄŸi gibi, hem 'Anormal Hb/' hem de 'USV/' olanlarÄ± getir
        #    (ve 'Anormal Hb/'den dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ olabilecek diÄŸerlerini)
        #    Bunu, 'work' dataframe'indeki 'Anormal Hb/'den tÃ¼reyen tÃ¼m varyantlarÄ± bularak yapalÄ±m
        
        # 'norm_anormal_hb_text' fonksiyonunun dÃ¶ndÃ¼rebileceÄŸi tÃ¼m olasÄ± metin etiketleri
        # (Bu, 'pick_variant_tag' iÃ§indeki Ã¶ncelik listesinden alÄ±nabilir)
        known_hb_variants = {"Hb S-Î²-thal","HbS","HbC","HbD","HbE","USV","HbA2â†‘","HbFâ†‘","Normal"}
        
        # 'TETKIK_ISMI'si 'Anormal Hb/' OLAN veya 'Anormal Hb/'den DÃ–NÃœÅTÃœRÃœLMÃœÅ olabilecek
        # (Ã¶rn. 'USV/') satÄ±rlarÄ± gÃ¶ster.
        # En gÃ¼venli yol, 'Anormal Hb/' testinin metin iÃ§erdiÄŸi bilinen satÄ±rlarÄ± almaktÄ±r.
        
        # Orijinal 'sub' filtresini koruyalÄ±m ve 'sub_nonempty'yi geniÅŸletelim
        # sub = work[work["TETKIK_ISMI"].astype(str) == test_name].copy()
        
        # YENÄ° FÄ°LTRE: Sadece "Anormal Hb/" deÄŸil, "USV/" gibi elle dÃ¼zeltilmiÅŸ olanlarÄ± da gÃ¶ster
        target_tests = {"Anormal Hb/", "USV/"} 
        
        # EÄŸer 'work' iÃ§inde 'ANORMAL_HB_CLEAN' varsa, oradaki deÄŸerleri de hedef listeye ekle
        if "ANORMAL_HB_CLEAN" in work.columns:
             target_tests.update(work["ANORMAL_HB_CLEAN"].dropna().unique())
             
        # 'pick_variant_tag' iÃ§inde 'USV/' gibi etiketlenen testleri de dahil et
        # Bu Ã§ok karmaÅŸÄ±k olacaÄŸÄ± iÃ§in ÅŸimdilik 'Anormal Hb/' ve 'USV/'ye odaklanalÄ±m:
        
        filter_list = {"Anormal Hb/", "USV/"}
        sub = work[work["TETKIK_ISMI"].astype(str).isin(filter_list)].copy()

        # 1) Ham yazÄ±m â†’ TC listesi (Frekans yerine) - BU KISIM AYNI KALABÄ°LÄ°R
        sub_nonempty = sub[sub["TEST_DEGERI"].notna() & (sub["TEST_DEGERI"].astype(str).str.strip() != "")].copy()
        
        if sub_nonempty.empty:
            st.info("DÃ¼zenlenecek 'Anormal Hb/' veya 'USV/' satÄ±rÄ± bulunamadÄ±.")
            # HÄ±zlÄ± inceleme ve diÄŸerleri iÃ§in bu bloÄŸu atla
        else:
            # 2) DÃ¼zenlenebilir tablo (DOÄRUDAN DÃœZENLEME)
            # 'CLEAN' sÃ¼tununu kaldÄ±rÄ±yoruz
            edit_cols = [c for c in ["PROTOKOL_NO","TCKIMLIK_NO","CINSIYET","SOURCE_FILE","TETKIK_ISMI","TEST_DEGERI"] if c in sub_nonempty.columns]
            edit_df = sub_nonempty[edit_cols].copy()
            
            # YENÄ°: DeÄŸiÅŸiklikleri takip etmek iÃ§in ana 'work' index'ini bir sÃ¼tun olarak ekle
            edit_df["__ORIG_INDEX__"] = edit_df.index
            
            st.markdown("**DÃ¼zenlenebilir tablo (TETKIK_ISMI ve TEST_DEGERI)**")
            st.caption("Burada 'AnormalHb/' ismini 'USV/' olarak veya 'TEST_DEGERI'ni (Ã¶rn. 'HBS D LOS ANGELES') 'USV' olarak deÄŸiÅŸtirebilirsiniz.")
            
            edited = st.data_editor(
                edit_df,
                use_container_width=True,
                key="anormalhb_editor_v2",
                column_config={
                    # YENÄ°: Bu iki sÃ¼tun artÄ±k dÃ¼zenlenebilir
                    "TETKIK_ISMI": st.column_config.TextColumn(label="TETKIK_ISMI (dÃ¼zenlenebilir)"),
                    "TEST_DEGERI": st.column_config.TextColumn(label="TEST_DEGERI (dÃ¼zenlenebilir)"),
                    
                    # Bu sÃ¼tunlarÄ± kilitle
                    "PROTOKOL_NO": st.column_config.TextColumn(disabled=True),
                    "TCKIMLIK_NO": st.column_config.TextColumn(disabled=True),
                    "CINSIYET": st.column_config.TextColumn(disabled=True),
                    "SOURCE_FILE": st.column_config.TextColumn(disabled=True),
                    
                    # YENÄ°: Index sÃ¼tununu gizle
                    "__ORIG_INDEX__": None, 
                }
            )
            
            apply_now = st.button("âœ… Uygula ve kaydet (oturum iÃ§i)", key="apply_anormalhb_v2")

            if apply_now and not edited.empty:
                st.info("DeÄŸiÅŸiklikler uygulanÄ±yor...")
                
                # 1. DeÄŸiÅŸiklikleri bulmak iÃ§in 'edited' ve 'edit_df'yi karÅŸÄ±laÅŸtÄ±r
                # (Daha basit yÃ¶ntem: 'edited'deki her satÄ±rÄ± 'orig_index' kullanarak 'df' ve 'work'e geri yaz)
                
                update_count = 0
                for _, changed_row in edited.iterrows():
                    orig_index = changed_row["__ORIG_INDEX__"]
                    
                    # Orijinal satÄ±rÄ±n 'df' ve 'work'te hala var olduÄŸunu kontrol et
                    if orig_index not in df.index or orig_index not in work.index:
                        continue
                        
                    # Yeni deÄŸerleri al
                    new_tetkik_ismi = changed_row["TETKIK_ISMI"]
                    new_test_degeri = changed_row["TEST_DEGERI"]
                    
                    # Orijinal deÄŸerlerle karÅŸÄ±laÅŸtÄ±r (gereksiz yazmayÄ± Ã¶nle)
                    orig_tetkik = work.loc[orig_index, "TETKIK_ISMI"]
                    orig_test_val = work.loc[orig_index, "TEST_DEGERI"]
                    
                    if (orig_tetkik != new_tetkik_ismi) or (orig_test_val != new_test_degeri):
                        update_count += 1
                        
                        # 2. DeÄŸiÅŸiklikleri ANA 'df'ye uygula (KalÄ±cÄ±lÄ±k iÃ§in)
                        df.loc[orig_index, "TETKIK_ISMI"] = new_tetkik_ismi
                        df.loc[orig_index, "TEST_DEGERI"] = new_test_degeri
                        
                        # 3. DeÄŸiÅŸiklikleri GEÃ‡Ä°CÄ° 'work'e uygula (Bu anki gÃ¶rÃ¼nÃ¼m iÃ§in)
                        work.loc[orig_index, "TETKIK_ISMI"] = new_tetkik_ismi
                        work.loc[orig_index, "TEST_DEGERI"] = new_test_degeri
                        
                        # 4. YENÄ°: DeÄŸiÅŸen satÄ±rÄ±n sayÄ±sal deÄŸerini de gÃ¼ncelle
                        #    (coerce_numeric fonksiyonu yukarÄ±da tanÄ±mlÄ± olmalÄ±)
                        new_val_num = coerce_numeric(pd.Series([new_test_degeri])).iloc[0]
                        work.loc[orig_index, "__VAL_NUM__"] = new_val_num
                        df.loc[orig_index, "__VAL_NUM__"] = new_val_num # Ana df'i de gÃ¼ncelle

                st.info(f"{update_count} satÄ±r gÃ¼ncellendi.")

                # 5. YENÄ° ve Ã–NEMLÄ°: VARIANT_TAG'Ä° YENÄ°DEN HESAPLA
                st.info("TÃ¼m VARIANT_TAG'ler yeniden hesaplanÄ±yor...")
                if "VARIANT_TAG" in work.columns:
                    work = work.drop(columns="VARIANT_TAG") # Eski tag'leri sil
                if "VARIANT_TAG" in df.columns:
                    df = df.drop(columns="VARIANT_TAG") # Ana df'ten de sil
                
                # 'work' Ã¼zerinden tag'leri yeniden hesapla
                var_map_work = (work.groupby("PROTOKOL_NO", group_keys=False)
                               .apply(lambda g: pd.Series({"VARIANT_TAG": pick_variant_tag(g)}))
                               .reset_index())
                work = work.merge(var_map_work, on="PROTOKOL_NO", how="left")
                
                # 'df' Ã¼zerinden tag'leri yeniden hesapla
                var_map_df = (df.groupby("PROTOKOL_NO", group_keys=False)
                               .apply(lambda g: pd.Series({"VARIANT_TAG": pick_variant_tag(g)}))
                               .reset_index())
                df = df.merge(var_map_df, on="PROTOKOL_NO", how="left")
                
                st.success("VARIANT_TAG'ler baÅŸarÄ±yla gÃ¼ncellendi! Pivot tabloyu kontrol edebilirsiniz.")
                
                # 6. GÃ¼ncellenmiÅŸ veriyi indirme
                st.download_button(
                    "â¬‡ï¸ GÃ¼ncellenmiÅŸ veri (CSV)",
                    data=work.to_csv(index=False).encode("utf-8-sig"),
                    file_name="guncellenmis_veri_v2.csv",
                    mime="text/csv",
                    key="download_v2"
                )

        # 3) SeÃ§ince hastanÄ±n/protokolÃ¼n tÃ¼m tetkikleri (Bu kÄ±sÄ±m aynÄ± kalÄ±r)
        st.markdown("**HÄ±zlÄ± inceleme: bir hasta veya protokol seÃ§in**")
        tcs  = sorted({str(x) for x in sub_nonempty.get("TCKIMLIK_NO", pd.Series(dtype=object)).dropna().astype(str)})
        prot = sorted({str(x) for x in sub_nonempty.get("PROTOKOL_NO", pd.Series(dtype=object)).dropna().astype(str)})

        tabs_sel = st.tabs(["Hasta ile seÃ§", "Protokol ile seÃ§"])
        with tabs_sel[0]:
            if tcs:
                sel_tc = st.selectbox("TCKIMLIK_NO", options=tcs, key="sel_tc_anormalhb")
                proto_for_tc = (
                    sub_nonempty.loc[sub_nonempty["TCKIMLIK_NO"].astype(str) == sel_tc, "PROTOKOL_NO"]
                    .astype(str).unique().tolist()
                ) if "PROTOKOL_NO" in sub_nonempty.columns else []
                all_tests = work[
                    (work["TCKIMLIK_NO"].astype(str) == sel_tc) &
                    (work["PROTOKOL_NO"].astype(str).isin(proto_for_tc))
                ].copy()
                show_cols = [c for c in ["PROTOKOL_NO","TETKIK_ISMI","TEST_DEGERI","CINSIYET","SOURCE_FILE"] if c in all_tests.columns]
                st.dataframe(all_tests[show_cols].sort_values(show_cols[:2]) if not all_tests.empty else all_tests, use_container_width=True)
            else:
                st.info("SeÃ§ilebilir hasta yok.")
        with tabs_sel[1]:
            if prot:
                sel_p = st.selectbox("PROTOKOL_NO", options=prot, key="sel_proto_anormalhb")
                all_tests = work[work["PROTOKOL_NO"].astype(str) == sel_p].copy()
                show_cols = [c for c in ["PROTOKOL_NO","TETKIK_ISMI","TEST_DEGERI","CINSIYET","SOURCE_FILE","TCKIMLIK_NO"] if c in all_tests.columns]
                st.dataframe(all_tests[show_cols].sort_values("TETKIK_ISMI") if not all_tests.empty else all_tests, use_container_width=True)
            else:
                st.info("SeÃ§ilebilir protokol yok.")
                
        # --- YENÄ°: Anormal Hb/ Olan HastalarÄ±n TÃœM Verilerini Ä°ndir ---
        st.divider()
        st.markdown("### ğŸ“¥ Anormal Hb/ KaydÄ± OlanlarÄ±n TÃ¼m Verisi")
        st.caption("AÅŸaÄŸÄ±daki buton, 'Anormal Hb/' testi Ã§alÄ±ÅŸÄ±lmÄ±ÅŸ (sonucu ne olursa olsun) tÃ¼m protokollerin, hemogram ve diÄŸer HPLC dahil BÃœTÃœN sonuÃ§larÄ±nÄ± indirir.")
        
        # 1. Anormal Hb/ testi olan protokolleri bul
        # (sub_nonempty zaten bu filtreyi iÃ§eriyor ama garantilemek iÃ§in ana 'work'ten Ã§ekelim)
        anormal_hb_protocols = work.loc[work["TETKIK_ISMI"] == "Anormal Hb/", "PROTOKOL_NO"].unique()
        
        if len(anormal_hb_protocols) > 0:
            # 2. Bu protokollerin TÃœM verilerini ana 'work' tablosundan Ã§ek
            full_patient_data = work[work["PROTOKOL_NO"].isin(anormal_hb_protocols)].copy()
            
            # 3. OkunaklÄ± bir sÄ±ralama yap (Ã–nce Protokol, Sonra Tetkik Ä°smi)
            full_patient_data = full_patient_data.sort_values(by=["PROTOKOL_NO", "TETKIK_ISMI"])
            
            # 4. Ä°ndirme butonu
            csv_full_data = full_patient_data.to_csv(index=False).encode("utf-8-sig")
            
            st.download_button(
                label=f"â¬‡ï¸ {len(anormal_hb_protocols)} HastanÄ±n TÃ¼m Tetkiklerini Ä°ndir (CSV)",
                data=csv_full_data,
                file_name="anormal_hb_hastalarinin_tum_verileri.csv",
                mime="text/csv",
                key="btn_download_full_anormal_hb_data"
            )
        else:
            st.info("Anormal Hb/ kaydÄ± olan protokol bulunamadÄ±.")
            
        # -------------------------------------------------------------
        
        # Bu Ã¶zel akÄ±ÅŸta frekans/ki-kare gÃ¶stermiyoruz.
        
        continue  # >>> dÃ¶ngÃ¼nÃ¼n geri kalanÄ±nÄ± Kan Grubu/ iÃ§in Ã§alÄ±ÅŸtÄ±r
    # ============ STANDART AKIÅ: KAN GRUBU/ (mevcut mantÄ±ÄŸÄ±nÄ±z) ============
    # 1) Ham yazÄ±mlarÄ±n sayÄ±mÄ±
    sub_text = raw_text[raw_text.str.contains(r"[A-Za-zÄ°Ä±Ã–Ã¶ÃœÃ¼Ã‡Ã§ÅÅŸ]", na=False)]
    if sub_text.empty:
        st.info("Harf iÃ§eren veri bulunamadÄ±.")
        value_counts = pd.DataFrame(columns=["Benzersiz DeÄŸer","Frekans"])
    else:
        value_counts = (
            sub_text.value_counts(dropna=False)
            .rename_axis("Benzersiz DeÄŸer")
            .reset_index(name="Frekans")
        )
    st.markdown("**Ham YazÄ±mlar**")
    st.dataframe(value_counts, use_container_width=True)
    st.download_button(
        f"â¬‡ï¸ {test_name.strip('/')}_benzersiz_degerler.csv",
        data=value_counts.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"{test_name.strip('/')}_benzersiz_degerler.csv",
        mime="text/csv"
    )

    # 2) Normalize edilmiÅŸ kategorilerin sayÄ±mÄ±
    norm_counts = (
        normalized.value_counts(dropna=False)
        .rename_axis("Kategori (normalize)")
        .reset_index(name="N")
    )
    if not norm_counts.empty:
        totalN = int(norm_counts["N"].sum())
        norm_counts["%"] = (norm_counts["N"] / totalN * 100).round(2)
    else:
        norm_counts = pd.DataFrame(columns=["Kategori (normalize)","N","%"])

    st.markdown("**Normalize EdilmiÅŸ Kategoriler**")
    st.dataframe(norm_counts, use_container_width=True)
    st.download_button(
        f"â¬‡ï¸ {test_name.strip('/')}_normalize_frekans.csv",
        data=norm_counts.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"{test_name.strip('/')}_normalize_frekans.csv",
        mime="text/csv"
    )

    # 3) Kategorik genel frekans/ki-kare (normalize etiketle)
    cat_name = "__CAT__"
    sub = sub.assign(**{cat_name: normalized})
    freq_all = (sub[cat_name].value_counts(dropna=False)
                .rename_axis("Kategori").to_frame("N").reset_index())
    totalN = int(freq_all["N"].sum()) if not freq_all.empty else 0
    if totalN:
        freq_all["%"] = (freq_all["N"]/totalN*100).round(2)
    else:
        freq_all["%"] = []
    freq_by_sex = (sub.pivot_table(index=cat_name, columns="CINSIYET",
                                   values="PROTOKOL_NO", aggfunc="count", fill_value=0)
                   .astype(int).reset_index().rename(columns={cat_name:"Kategori"}))
    chi2_msg = "Ki-kare uygulanamadÄ±."
    try:
        from scipy.stats import chi2_contingency
        cont = freq_by_sex.drop(columns=["Kategori"]).values
        if cont.sum() > 0 and cont.shape[1] > 1:
            chi2, p, dof, _ = chi2_contingency(cont)
            chi2_msg = f"Chi-square: Ï‡Â²={chi2:.2f}, df={dof}, p={p:.4g}"
    except Exception as e:
        chi2_msg = f"Hata: {e}"

    tabs = st.tabs(["Frekans", "Cinsiyet DaÄŸÄ±lÄ±mÄ±", "Ä°statistik"])
    with tabs[0]: st.dataframe(freq_all, use_container_width=True)
    with tabs[1]: st.dataframe(freq_by_sex, use_container_width=True)
    with tabs[2]: st.info(chi2_msg)

# ================= Genel Bilgiler ================= #
st.subheader("ğŸ” Genel Bilgiler (BirleÅŸtirilmiÅŸ)")
c1, c2, c3, c4, c5 = st.columns(5)
c1.metric("Toplam SatÄ±r", f"{len(df):,}")
c2.metric("Benzersiz TCKIMLIK_NO", f"{df['TCKIMLIK_NO'].nunique():,}")
c3.metric("Benzersiz Tetkik", f"{df['TETKIK_ISMI'].nunique():,}")
c4.metric("Benzersiz Cinsiyet", f"{df['CINSIYET'].nunique():,}")
c5.metric("Dosya SayÄ±sÄ±", f"{df['SOURCE_FILE'].nunique():,}")

with st.expander("Ham Veri Ã–n Ä°zleme (limitli)"):
    st.dataframe(work.head(DISPLAY_LIMIT), use_container_width=True)
    st.caption(f"YalnÄ±zca ilk {DISPLAY_LIMIT} satÄ±r gÃ¶rÃ¼ntÃ¼lenir.")


# ================= HÄ±zlÄ± Ã–zetler ================= #
st.header("âš™ï¸ HÄ±zlÄ± Ã–zet ve KÄ±rÄ±lÄ±mlar")
colA, colB = st.columns(2)
with colA:
    st.write("**Cinsiyete GÃ¶re TanÄ±mlayÄ±cÄ±lar (SeÃ§imdeki veri)**")
    sex_summary = summarize_sex_counts(work)
    st.dataframe(sex_summary, use_container_width=True)
with colB:
    st.write("**Dosyaya GÃ¶re SatÄ±r & Hasta & Tetkik SayÄ±sÄ±**")
    per_file = work.groupby("SOURCE_FILE").agg(
        N=("PROTOKOL_NO", "size"),
        Hasta_Sayisi=("TCKIMLIK_NO", "nunique"),
        Tetkik_Sayisi=("TETKIK_ISMI", "nunique")
    ).reset_index()
    st.dataframe(per_file, use_container_width=True)
    export_df(per_file, "dosya_bazinda_ozet_filtreli.csv")


# ================= Tetkik BazlÄ± Analiz (SeÃ§im) ================= #
st.header("ğŸ“Š Tetkik BazlÄ± Analiz (SeÃ§im)")
results_rows = []
for test_name in selected_tests:
    # === BEGIN PATCH: overall pool for global stats ===
    overall_pool = []
    # === END PATCH ===
    if test_name in CATEGORICAL_TESTS:
        # Kan Grubu/ ve Anormal Hb/ yukarÄ±da Ã¶zel blokta analiz edildi
        continue

    sub = work[work["TETKIK_ISMI"].astype(str) == test_name].copy()
    if sub.empty: 
        continue

    use_threshold = st.checkbox(
        f"â€˜{test_name}â€™ iÃ§in eriÅŸkin eÅŸiÄŸini uygula",
        value=(test_name in THRESHOLDS),
        key=f"th_{test_name}"
    )
    use_gt_zero  = st.checkbox(
        f"â€˜{test_name}â€™ iÃ§in sadece > 0 deÄŸerleri dahil et",
        value=(test_name in GT_ZERO_DEFAULT),
        key=f"gt0_{test_name}"
    )
    sub_work = sub[sub["__VAL_NUM__"].notna()].copy()
    if use_threshold and test_name in THRESHOLDS:
        sub_work = sub_work[apply_threshold(sub_work["__VAL_NUM__"], THRESHOLDS[test_name])]
        st.caption(f"EÅŸik: {THRESHOLDS[test_name][0]} {THRESHOLDS[test_name][1]}")
    elif use_gt_zero:
        sub_work = sub_work[sub_work["__VAL_NUM__"] > 0]
        st.caption("Filtre: > 0")
    if sub_work.empty:
        st.warning("Filtre sonrasÄ± satÄ±r bulunamadÄ±."); 
        continue

    stats_overall = descr_stats_fast(sub_work["__VAL_NUM__"])
    normal_flag   = normality_flag(sub_work["__VAL_NUM__"])
    # Normalite testi (etiket + p)
    norm_label, norm_p_disp = normality_test_with_p(sub_work["__VAL_NUM__"])

    # Genel toplama havuzuna ekle
    overall_pool.extend(pd.to_numeric(sub_work["__VAL_NUM__"], errors="coerce").dropna().tolist())



    by_sex  = (sub_work.groupby("CINSIYET", dropna=False)["__VAL_NUM__"]
               .agg(count="count", mean="mean", std="std", min="min", median="median", max="max")).reset_index()
    by_file = (sub_work.groupby("SOURCE_FILE", dropna=False)["__VAL_NUM__"]
               .agg(count="count", mean="mean", std="std", min="min", median="median", max="max")).reset_index()
    _msg_df = sub_work.rename(columns={"__VAL_NUM__": "VAL"})
    msg, _ = nonparametric_test_by_group(_msg_df, "VAL", "CINSIYET")
    # === BEGIN PATCH: collect values for global stats ===
    overall_pool.extend(pd.to_numeric(_msg_df["VAL"], errors="coerce").dropna().tolist())
    # === END PATCH ===


    results_rows.append({
        "TETKIK_ISMI": test_name,
        "N": stats_overall["count"],
        "Mean": stats_overall["mean"],
        "Median": stats_overall["median"],
        "Std": stats_overall["std"],
        "Min": stats_overall["min"],
        "Q1": stats_overall["q1"],
        "Q3": stats_overall["q3"],
        "Max": stats_overall["max"],
        "Normalite": normal_flag,
        "Test": msg
    })

    tabs = st.tabs(["TanÄ±mlayÄ±cÄ±", "Cinsiyet", "Dosya", "Ä°statistiksel Test", "Histogram", "Boxplot"])
    with tabs[0]: st.table(pd.DataFrame([stats_overall]))
    with tabs[1]: st.dataframe(by_sex, use_container_width=True)
    with tabs[2]: st.dataframe(by_file, use_container_width=True)
    with tabs[3]: st.info(msg)
    with tabs[4]:
        if st.checkbox(f"Histogram gÃ¶ster ({test_name})", value=False):
            make_hist(_msg_df, "VAL", bins=30, title=f"{test_name} - Histogram")
    with tabs[5]:
        if st.checkbox(f"Boxplot gÃ¶ster ({test_name})", value=False):
            make_boxplot(sub_work, "CINSIYET", "__VAL_NUM__", title=f"{test_name} - Cinsiyete GÃ¶re Boxplot")

    pos_cols = ["PROTOKOL_NO", "TCKIMLIK_NO", "CINSIYET", "SOURCE_FILE"]
    pos_cols = [c for c in pos_cols if c in sub_work.columns]
    pos_tbl = sub_work[pos_cols + ["__VAL_NUM__"]].sort_values("__VAL_NUM__", ascending=False)
    st.write("**Filtre sonrasÄ± kayÄ±tlar**")
    st.dataframe(pos_tbl, use_container_width=True)
    st.download_button(
        "â¬‡ï¸ TCKIMLIK_NO listesi (CSV)",
        data=pos_tbl.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"{test_name}_filtre_sonrasi.csv",
        mime="text/csv"
    )

if results_rows:
    st.header("ğŸ§¾ Toplu Ã–zet Tablosu (SeÃ§ili Tetkikler)")
    res_df = pd.DataFrame(results_rows)
    # === BEGIN PATCH: append global total row ===
    if len(overall_pool) > 0:
        overall_stats = descr_stats_fast(pd.Series(overall_pool))
        # N'yi tek tek testlerden de toplayabiliriz ama havuz zaten filtre-sonrasÄ± gerÃ§ek toplamÄ± temsil ediyor
        overall_row = {
            "TETKIK_ISMI": "GENEL TOPLAM",
            "N": overall_stats["count"],
            "Mean": overall_stats["mean"],
            "Median": overall_stats["median"],
            "Std": overall_stats["std"],
            "Min": overall_stats["min"],
            "Q1": overall_stats["q1"],
            "Q3": overall_stats["q3"],
            "Max": overall_stats["max"],
            "Normalite": norm_label,        
            "p (normalite)": norm_p_disp,     
            "Test": "â€”",
        }
        res_df = pd.concat([res_df, pd.DataFrame([overall_row])], ignore_index=True)
    # === END PATCH ===

    
    st.dataframe(res_df, use_container_width=True)
    export_df(res_df, name="tetkik_ozet.csv")

# ================= PIVOT: VARYANTLARA GÃ–RE PARAMETRE Ã–ZETÄ° (TABLE 2 - v11 - Ã‡Ä°FTE SAYIM + DETAYLI TOPLAM) ================= #
st.header("ğŸ”¬ Varyantlara GÃ¶re Parametre Ã–zeti")
st.caption("GÃ¶rseldeki Table 2'ye benzer pivot tablo. NOT: HbA2 deÄŸeri >3.5 olan 'Borderline' hastalar, hem Borderline hem de TaÅŸÄ±yÄ±cÄ± sÃ¼tununa dahil edilmiÅŸtir.")

# 1. 'PARAMS' sÃ¶zlÃ¼ÄŸÃ¼nde tanÄ±mlÄ± testleri al
params_to_analyze = list(PARAMS.keys())

# --- VERÄ° HAZIRLIÄI VE Ã‡Ä°FTE SAYIM MANTIÄI ---
# 1. Ana veriyi al
data_for_pivot_raw = work[
    work["TETKIK_ISMI"].isin(params_to_analyze) &
    work["VARIANT_TAG"].notna() &
    work["__VAL_NUM__"].notna()
].copy()

# 2. YAS verisini ekle
age_data_to_add = pd.DataFrame()
if "YAS" in work.columns:
    age_data = work[['PROTOKOL_NO', 'VARIANT_TAG', 'YAS']].dropna(subset=['PROTOKOL_NO', 'YAS']).drop_duplicates(subset=['PROTOKOL_NO'])
    age_data['TETKIK_ISMI'] = "YAS"
    age_data = age_data.rename(columns={'YAS': '__VAL_NUM__'})
    age_data_to_add = age_data.dropna(subset=['__VAL_NUM__'])

# 3. Ham veriyi birleÅŸtir (Ã‡oÄŸaltÄ±lmamÄ±ÅŸ hali)
data_for_pivot_base = pd.concat([age_data_to_add, data_for_pivot_raw])

if data_for_pivot_base.empty:
    st.info("Pivot tablo iÃ§in yeterli veri bulunamadÄ±.")
else:
    # --- BORDERLINE HASTALARINI BUL VE KOPYALA ---
    borderline_protocols = work[work["VARIANT_TAG"] == "Borderline HbA2"]["PROTOKOL_NO"].unique()
    a2_tests = ["A2/", "HbA2 (%)", "Hb A2", "Hb A2 (%)"]
    borderline_a2_values = work[
        (work["PROTOKOL_NO"].isin(borderline_protocols)) & 
        (work["TETKIK_ISMI"].isin(a2_tests))
    ]
    
    double_count_protocols = []
    if not borderline_a2_values.empty:
        double_count_protocols = borderline_a2_values[
            pd.to_numeric(borderline_a2_values["__VAL_NUM__"], errors='coerce') > 3.5
        ]["PROTOKOL_NO"].unique()
    
    if len(double_count_protocols) > 0:
        rows_to_duplicate = data_for_pivot_base[data_for_pivot_base["PROTOKOL_NO"].isin(double_count_protocols)].copy()
        rows_to_duplicate["VARIANT_TAG"] = "HbA2â†‘ (B-thal Trait)"
        data_for_pivot = pd.concat([data_for_pivot_base, rows_to_duplicate])
        st.info(f"Bilgi: {len(double_count_protocols)} adet 'Borderline' hasta, A2>3.5 olduÄŸu iÃ§in 'Beta Talasemi TaÅŸÄ±yÄ±cÄ±sÄ±' grubuna da eklendi.")
    else:
        data_for_pivot = data_for_pivot_base

    # --- BAÅLIKLARI HAZIRLA (n=?, %...) ---
    rename_map = {}
    try:
        total_unique_patients = work[work["VARIANT_TAG"].notna()]["PROTOKOL_NO"].nunique()
        if total_unique_patients == 0: total_unique_patients = 1
        
        grouped_counts = data_for_pivot.groupby(['VARIANT_TAG', 'CINSIYET'])['PROTOKOL_NO'].nunique().unstack(fill_value=0)
        
        for tag in data_for_pivot["VARIANT_TAG"].unique():
            if tag in grouped_counts.index:
                sub_grp = data_for_pivot[data_for_pivot["VARIANT_TAG"] == tag][["PROTOKOL_NO", "CINSIYET"]].drop_duplicates()
                sub_grp["Sex"] = sub_grp["CINSIYET"].astype(str).map(normalize_sex_label).fillna("Bilinmiyor")
                
                f_count = len(sub_grp[sub_grp["Sex"] == "KadÄ±n"])
                m_count = len(sub_grp[sub_grp["Sex"] == "Erkek"])
                n_grp = len(sub_grp)
                
                pct = (n_grp / total_unique_patients) * 100
                rename_map[tag] = f"{tag} (n={n_grp}, {pct:.1f}%) (F: {f_count}, M: {m_count})"
    except Exception as e:
        st.warning(f"BaÅŸlÄ±k oluÅŸturma hatasÄ±: {e}")

    # --- FORMATLAYICILAR (GÃœNCELLENMÄ°Å - .00 SÄ°LÄ°CÄ°) ---
    def fmt(val):
        if pd.isna(val): return "â€”"
        s = f"{val:.2f}"
        if s.endswith(".00"): return s[:-3]
        return s

    def _format_smart_summary_default(s: pd.Series):
        s = pd.to_numeric(s, errors="coerce").dropna()
        n = len(s)
        if n == 0: return "â€”"
        if n == 1: return fmt(s.iloc[0])
        try: norm_label, _ = normality_test_with_p(s)
        except: norm_label = "bilinmiyor"
        if norm_label != "normal": return f"{fmt(s.median())} [{fmt(s.min())}â€“{fmt(s.max())}]áµ‡"
        else: 
            mean = s.mean(); std = s.std(ddof=1)
            if pd.isna(std) or std == 0: return fmt(mean)
            return f"{fmt(mean)} Â± {fmt(std)}áµƒ"

    def _format_smart_summary_inverted(s: pd.Series):
        s = pd.to_numeric(s, errors="coerce").dropna()
        n = len(s)
        if n == 0: return "â€”"
        if n == 1: return fmt(s.iloc[0])
        try: norm_label, _ = normality_test_with_p(s)
        except: norm_label = "bilinmiyor"
        if norm_label != "normal":
            mean = s.mean(); std = s.std(ddof=1)
            if pd.isna(std) or std == 0: return fmt(mean)
            return f"{fmt(mean)} Â± {fmt(std)}"
        else: return f"{fmt(s.median())} [{fmt(s.min())}â€“{fmt(s.max())}]"

    def _process_and_display_pivot(pivot_df, table_title, table_key, file_name_suffix):
        display_map = {k: v[0] for k, v in PARAMS.items()}
        ordered_params_in_table = [k for k in PARAMS.keys() if k in pivot_df.index]
        if not ordered_params_in_table: return
        
        final_pivot_table = pivot_df.loc[ordered_params_in_table]
        final_pivot_table.index = final_pivot_table.index.map(display_map)
        final_pivot_table = final_pivot_table.rename_axis("Parametre")
        
        if rename_map:
            cols_to_rename = {c: rename_map[c] for c in final_pivot_table.columns if c in rename_map}
            final_pivot_table = final_pivot_table.rename(columns=cols_to_rename)
            
        # SÄ±ralama: Toplam en saÄŸa
        cols = list(final_pivot_table.columns)
        total_cols = [c for c in cols if "TOPLAM" in str(c)]
        other_cols = [c for c in cols if "TOPLAM" not in str(c)]
        other_cols.sort()
        final_pivot_table = final_pivot_table[other_cols + total_cols]

        st.subheader(table_title)
        st.dataframe(final_pivot_table, use_container_width=True, key=table_key)
        csv_data = final_pivot_table.to_csv(index=True).encode("utf-8-sig")
        st.download_button(f"â¬‡ï¸ {table_title} Ä°ndir (CSV)", data=csv_data, file_name=f"varyant_pivot_ozet_{file_name_suffix}.csv", mime="text/csv", key=f"download_{table_key}")

    try:
        # --- TOPLAM SÃœTUNU HESAPLAMA (DETAYLI CÄ°NSÄ°YET Ä°LE) ---
        
        # 1. Benzersiz hasta listesini 'base' (Ã§oÄŸaltÄ±lmamÄ±ÅŸ) veriden Ã§ek
        unique_patients_all = data_for_pivot_base[['PROTOKOL_NO', 'CINSIYET']].drop_duplicates(subset=['PROTOKOL_NO'])
        
        # 2. Cinsiyetleri normalize et
        unique_patients_all['Sex_Clean'] = unique_patients_all['CINSIYET'].astype(str).map(normalize_sex_label).fillna("Bilinmiyor")
        
        # 3. SayÄ±larÄ± hesapla
        total_n_all = len(unique_patients_all)
        total_f = len(unique_patients_all[unique_patients_all['Sex_Clean'] == 'KadÄ±n'])
        total_m = len(unique_patients_all[unique_patients_all['Sex_Clean'] == 'Erkek'])
        
        # 4. BaÅŸlÄ±ÄŸÄ± oluÅŸtur
        total_col_label = f"TOPLAM (n={total_n_all}) (F: {total_f}, M: {total_m})"
        
        # Tablo 1
        pivot_table_default = pd.pivot_table(data_for_pivot, values="__VAL_NUM__", index="TETKIK_ISMI", columns="VARIANT_TAG", aggfunc=_format_smart_summary_default, fill_value="â€”")
        total_series_1 = data_for_pivot_base.groupby("TETKIK_ISMI")["__VAL_NUM__"].apply(_format_smart_summary_default)
        pivot_table_default[total_col_label] = total_series_1
        
        _process_and_display_pivot(pivot_table_default, "Tablo 1: AkÄ±llÄ± Format (Normal=SDáµƒ, Non-Normal=Medianáµ‡)", "akilli_format_varsayilan", "akilli")
        st.caption("""áµƒ: Normal daÄŸÄ±lÄ±m gÃ¶steren veriler (Mean Â± SD) \náµ‡: Normal daÄŸÄ±lÄ±m gÃ¶stermeyen veya yetersiz veriler (Median [Minâ€“Max])""")
        
        st.divider()
        
        # Tablo 2
        pivot_table_inverted = pd.pivot_table(data_for_pivot, values="__VAL_NUM__", index="TETKIK_ISMI", columns="VARIANT_TAG", aggfunc=_format_smart_summary_inverted, fill_value="â€”")
        total_series_2 = data_for_pivot_base.groupby("TETKIK_ISMI")["__VAL_NUM__"].apply(_format_smart_summary_inverted)
        pivot_table_inverted[total_col_label] = total_series_2
        
        _process_and_display_pivot(pivot_table_inverted, "Tablo 2: Ä°nvert EdilmiÅŸ Format (Normal=Median, Non-Normal=SD)", "invert_edilmis_format", "inverted")
        
    except Exception as e:
        st.error(f"Pivot tablo oluÅŸturulurken bir hata oluÅŸtu: {e}")
        
# ================= PIVOT HAM VERÄ° Ä°NDÄ°RME ================= #
st.subheader("ğŸ§¬ Ham Veri Listesi (Pivot Tablo GruplarÄ±)")
st.caption("YukarÄ±daki pivot tabloda gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z varyant gruplarÄ±nÄ±n (Ã¶rn. 'HbA2â†‘ (B-thal Trait)', 'HPFH?') ham hasta listesini (TCKN ve tÃ¼m parametreler) indirin.")

# 1. Pivot tablolar iÃ§in kullandÄ±ÄŸÄ±mÄ±z ana 'work' verisini alalÄ±m
#    Bu veri 'YAS' sÃ¼tununu ve tÃ¼m filtreleri iÃ§erir
#    'data_for_pivot'u kullanamayÄ±z Ã§Ã¼nkÃ¼ o 'long' formatta
#    ve sadece PARAMS'taki testleri iÃ§erir. Bize 'work' lazÄ±m.

# 2. 'work' dataframe'i tÃ¼m gerekli bilgileri (TCKN, VARIANT_TAG, CINSIYET) iÃ§erir
#    'VARIANT_TAG' sÃ¼tunu olmayan satÄ±rlarÄ± (Ã¶rn. gruplanmamÄ±ÅŸ) Ã§Ä±karalÄ±m
download_df = work[work["VARIANT_TAG"].notna()].copy()

if download_df.empty:
    st.info("Ä°ndirilecek etiketlenmiÅŸ ham veri bulunamadÄ±.")
else:
    # 3. Ä°ndirme iÃ§in sÃ¼tunlarÄ± sÄ±ralayalÄ±m (Daha okunaklÄ± olmasÄ± iÃ§in)
    cols_to_show = [
        "VARIANT_TAG", 
        "PROTOKOL_NO", 
        "TCKIMLIK_NO", 
        "CINSIYET", 
        "YAS", # 'YAÅ' -> 'YAS' olarak dÃ¼zeltildi
        "TETKIK_ISMI", 
        "TEST_DEGERI", 
        "SOURCE_FILE"
    ]
    # Sadece 'download_df' iÃ§inde var olan sÃ¼tunlarÄ± seÃ§
    existing_cols = [c for c in cols_to_show if c in download_df.columns]
    
    # Kalan diÄŸer sÃ¼tunlarÄ± da sona ekle (Ã¶rn. __VAL_NUM__)
    other_cols = [c for c in download_df.columns if c not in existing_cols]
    
    final_download_df = download_df[existing_cols + other_cols]
    
    # 4. Varyant Tag'e ve Protokol No'ya gÃ¶re sÄ±rala
    final_download_df = final_download_df.sort_values(by=["VARIANT_TAG", "PROTOKOL_NO"])
    
    # 5. Ä°ndirme butonunu oluÅŸtur
    csv_data_ham_veri = final_download_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "â¬‡ï¸ TÃ¼m Varyant GruplarÄ±nÄ±n Ham Listesini Ä°ndir (CSV)",
        data=csv_data_ham_veri,
        file_name="varyant_gruplari_ham_veri_listesi.csv",
        mime="text/csv",
        key="download_ham_veri_pivot"
    )

# ================= BLOK SONU ================= #
# ================= PREVALANS HESAPLAYICI (Epidemiyolojik KarÅŸÄ±laÅŸtÄ±rma) ================= #
st.divider()
st.subheader("ğŸ“ˆ GeniÅŸletilmiÅŸ Beta Talasemi PrevalansÄ±")
st.caption("LiteratÃ¼rdeki %2'lik orana kÄ±yasla sizin verinizin durumu. Sadece 'HbA2 YÃ¼ksekliÄŸi' deÄŸil, iliÅŸkili diÄŸer gruplar da hesaba katÄ±lÄ±r.")

# 1. Toplam 'GerÃ§ek' Protokol SayÄ±sÄ± (Payda)
total_n = work["PROTOKOL_NO"].nunique()

if total_n > 0:
    # 2. Alt GruplarÄ±n SayÄ±larÄ±nÄ± Al
    # A. Klasik TaÅŸÄ±yÄ±cÄ±lar (Sizin %1.4'Ã¼nÃ¼z)
    n_classic = len(work[work["VARIANT_TAG"] == "HbA2â†‘ (B-thal Trait)"])
    
    # B. SÄ±nÄ±rda (Borderline) Olanlar
    # (Not: Kodunuzda "Borderline HbA2" etiketi varsa)
    n_border = len(work[work["VARIANT_TAG"] == "Borderline HbA2"])
    
    # C. Delta-Beta Talasemi (Normal A2, YÃ¼ksek F)
    n_db = len(work[work["VARIANT_TAG"] == "Î´Î²-thal Trait"])
    
    # D. HbS-Beta Talasemi (Hem S hem Beta geni var)
    n_sb = len(work[work["VARIANT_TAG"].astype(str).str.contains("Hb S-Î²", na=False)])
    
    # 3. ToplamlarÄ± Hesapla
    total_carriers = n_classic + n_border + n_db + n_sb
    prevalence = (total_carriers / total_n) * 100
    
    # 4. SonuÃ§larÄ± GÃ¶ster
    c1, c2, c3 = st.columns(3)
    
    with c1:
        st.metric(
            label="Klasik TaÅŸÄ±yÄ±cÄ± (HbA2 > 3.5)", 
            value=f"{n_classic} kiÅŸi", 
            delta=f"%{(n_classic/total_n)*100:.2f}"
        )
        
    with c2:
        st.metric(
            label="GeniÅŸletilmiÅŸ Toplam (TÃ¼mÃ¼)", 
            value=f"{total_carriers} kiÅŸi", 
            delta=f"%{prevalence:.2f}",
            help="Klasik + Borderline + Î´Î²-thal + S-Î² thal toplamÄ±"
        )
        
    with c3:
        st.info(f"""
        **ToplamÄ±n Ä°Ã§eriÄŸi:**
        - Klasik HbA2â†‘: {n_classic}
        - Borderline: {n_border}
        - Î´Î²-thal: {n_db}
        - S-Î² thal: {n_sb}
        """)
        
    if prevalence < 1.8:
        st.warning("Toplam oran hala %2'nin altÄ±nda. 'Iron Def./Alpha-thal?' grubundaki bazÄ± hastalar, A2 deÄŸeri baskÄ±lanmÄ±ÅŸ (demir eksikliÄŸi yÃ¼zÃ¼nden) Beta Talasemi taÅŸÄ±yÄ±cÄ±larÄ± olabilir.")
    else:
        st.success("GeniÅŸletilmiÅŸ oran literatÃ¼rdeki (~%2) beklentiyle uyumlu gÃ¶rÃ¼nÃ¼yor.")

else:
    st.write("Hesaplanacak veri yok.")
# ================= EK ANALÄ°Z: Beta Talasemi ve HbF KesiÅŸimi (Venn Analizi) ================= #
st.divider()
st.subheader("ğŸ“Š Beta Talasemi TaÅŸÄ±yÄ±cÄ±larÄ±nda HbA2 ve HbF KesiÅŸimi")
st.caption("Bu analiz, HbA2 (>3.5) ve HbF (>2.0) yÃ¼ksekliÄŸinin birlikteliÄŸini gÃ¶sterir.")

# 1. Gerekli Veriyi HazÄ±rla (Pivotlama)
# Sadece A2 ve F testlerini iÃ§eren satÄ±rlarÄ± al
target_tests_a2 = ["A2/", "HbA2 (%)", "Hb A2", "Hb A2 (%)"]
target_tests_f  = ["F/", "HbF (%)", "Hb F", "Hb F (%)"]
all_targets = target_tests_a2 + target_tests_f

subset = work[work["TETKIK_ISMI"].isin(all_targets) & work["__VAL_NUM__"].notna()].copy()

if not subset.empty:
    # Her protokol iÃ§in tek satÄ±r olacak ÅŸekilde pivotla
    # (SÃ¼tunlar: Test isimleri, DeÄŸerler: SonuÃ§lar)
    pivot_data = subset.pivot_table(
        index="PROTOKOL_NO", 
        columns="TETKIK_ISMI", 
        values="__VAL_NUM__"
    )
    
    # SÃ¼tunlarÄ± birleÅŸtir (Birden fazla A2 ismi varsa tek sÃ¼tunda topla)
    # A2 sÃ¼tunu oluÅŸtur (Mevcut olanlarÄ±n maksimumunu al)
    cols_a2 = [c for c in pivot_data.columns if c in target_tests_a2]
    pivot_data["FINAL_A2"] = pivot_data[cols_a2].max(axis=1)
    
    # F sÃ¼tunu oluÅŸtur
    cols_f = [c for c in pivot_data.columns if c in target_tests_f]
    pivot_data["FINAL_F"] = pivot_data[cols_f].max(axis=1)
    
    # Sadece her iki verisi de (veya en az biri) olanlarÄ± al
    analysis_df = pivot_data[["FINAL_A2", "FINAL_F"]].dropna(how='all')
    
    # 2. GruplandÄ±rma MantÄ±ÄŸÄ±
    # EÅŸik DeÄŸerler
    CUTOFF_A2 = 3.5
    CUTOFF_F = 2.0
    
    # MantÄ±ksal Kontroller
    has_high_a2 = analysis_df["FINAL_A2"] > CUTOFF_A2
    has_high_f  = analysis_df["FINAL_F"]  > CUTOFF_F
    
    # 3. SayÄ±mlarÄ± Yap
    # Grup 1: Sadece YÃ¼ksek A2 (F normal)
    group_only_a2 = analysis_df[has_high_a2 & (~has_high_f)]
    n_only_a2 = len(group_only_a2)
    
    # Grup 2: Sadece YÃ¼ksek F (A2 normal)
    group_only_f = analysis_df[(~has_high_a2) & has_high_f]
    n_only_f = len(group_only_f)
    
    # Grup 3: HER Ä°KÄ°SÄ° DE YÃ¼ksek
    group_both = analysis_df[has_high_a2 & has_high_f]
    n_both = len(group_both)
    
    # Toplam "Anormal" SayÄ±sÄ± (Bu 3 grubun toplamÄ±)
    total_abnormal = n_only_a2 + n_only_f + n_both
    
    if total_abnormal > 0:
        # 4. SonuÃ§ Tablosu
        venn_df = pd.DataFrame({
            "Grup TanÄ±mÄ±": [
                f"Sadece YÃ¼ksek HbA2 (>{CUTOFF_A2})",
                f"Sadece YÃ¼ksek HbF (>{CUTOFF_F})",
                f"HER Ä°KÄ°SÄ° DE YÃ¼ksek (A2>{CUTOFF_A2} ve F>{CUTOFF_F})",
                "TOPLAM (Anormal Bulgusu Olanlar)"
            ],
            "KiÅŸi SayÄ±sÄ± (n)": [n_only_a2, n_only_f, n_both, total_abnormal],
            "YÃ¼zde (%)": [
                f"%{(n_only_a2/total_abnormal)*100:.1f}",
                f"%{(n_only_f/total_abnormal)*100:.1f}",
                f"%{(n_both/total_abnormal)*100:.1f}",
                "100%"
            ]
        })
        
        st.table(venn_df)
        
        # 5. Yorum CÃ¼mlesi (Otomatik OluÅŸturulur)
        st.info(f"**Yorum:** Beta talasemi taÅŸÄ±yÄ±cÄ±lÄ±ÄŸÄ± veya iliÅŸkili hemoglobinopati ÅŸÃ¼phesi olan {total_abnormal} kiÅŸi arasÄ±nda; "
                f"{n_only_a2} kiÅŸi (%{(n_only_a2/total_abnormal)*100:.1f}) sadece yÃ¼ksek HbA2'ye, "
                f"{n_only_f} kiÅŸi (%{(n_only_f/total_abnormal)*100:.1f}) sadece yÃ¼ksek HbF'ye sahipken, "
                f"**{n_both} kiÅŸi (%{(n_both/total_abnormal)*100:.1f}) hem yÃ¼ksek HbA2 hem de yÃ¼ksek HbF deÄŸerine sahiptir.**")

        # 6. Ä°ndirme Butonu (KesiÅŸim KÃ¼mesi Ä°Ã§in)
        if n_both > 0:
            both_high_protocols = group_both.index.tolist()
            both_high_data = work[work["PROTOKOL_NO"].isin(both_high_protocols)].copy()
            
            csv_both = both_high_data.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                "â¬‡ï¸ Hem A2 Hem F YÃ¼ksek OlanlarÄ± Ä°ndir (CSV)",
                data=csv_both,
                file_name="hem_a2_hem_f_yuksek_hastalar.csv",
                mime="text/csv"
            )

    else:
        st.info("Veri setinde A2 veya F yÃ¼ksekliÄŸi olan kayÄ±t bulunamadÄ±.")
else:
    st.warning("Analiz iÃ§in gerekli A2 veya F verisi bulunamadÄ±.")
# ================= DEBUG: HbA2 Grubunda HbF Dedektifi ================= #
st.divider()
st.subheader("ğŸ•µï¸ HbA2â†‘ Grubunda HbF Dedektifi")

target_group = "HbA2â†‘ (B-thal Trait)"
# HbF iÃ§in olasÄ± isimleri kontrol et
f_cols = ["HbF (%)", "F/", "Hb F", "Hb F (%)"]

# Bu gruba girenleri bul
indices = work[work["VARIANT_TAG"] == target_group].index
subset = work.loc[indices].copy()

# Long formatta (sizin yapÄ±nÄ±zda) analiz:
hbf_data = subset[subset["TETKIK_ISMI"].isin(f_cols)].copy()

if not hbf_data.empty:
    hbf_values = pd.to_numeric(hbf_data["__VAL_NUM__"], errors='coerce').dropna()
    
    if not hbf_values.empty:
        # YENÄ°: %2'den yÃ¼ksek olanlarÄ± say
        high_f_count = (hbf_values > 2.0).sum()
        total_count_f = len(hbf_values)
        high_f_ratio = (high_f_count / total_count_f) * 100
        
        col1, col2 = st.columns(2)
        with col1:
            st.write(f"**Ä°statistikler ({target_group}):**")
            st.write(f"Min: {hbf_values.min()}")
            st.write(f"Max: {hbf_values.max()}")
            st.write(f"Medyan: {hbf_values.median()}")
            st.write(f"Ortalama: {hbf_values.mean():.2f}")
            # YENÄ° GÃ–STERÄ°M:
            st.metric(label="HbF > %2 Olan Hasta SayÄ±sÄ±", value=f"{high_f_count} / {total_count_f}", delta=f"%{high_f_ratio:.1f}")
        
        with col2:
            st.write("**En YÃ¼ksek 10 HbF DeÄŸeri:**")
            st.dataframe(hbf_values.sort_values(ascending=False).head(10))
            
        if hbf_values.max() > 5.0:
            st.warning(f"âš ï¸ Dikkat: Bu grupta %{hbf_values.max()} gibi yÃ¼ksek HbF deÄŸerleri var. Bu hastalar standart sapmayÄ± yÃ¼kseltiyor.")
            st.info("Bu hastalarÄ±n hem A2'si yÃ¼ksek hem F'si yÃ¼ksek olduÄŸu iÃ§in 'HbA2â†‘' grubuna dÃ¼ÅŸtÃ¼ler.")
    else:
        st.warning("HbF deÄŸerleri sayÄ±ya Ã§evrilemedi.")
else:
    st.warning("Bu grupta HbF tetkiki bulunamadÄ±.")

# ================= EK ANALÄ°Z: Beta Talasemi TaÅŸÄ±yÄ±cÄ±larÄ±nda (HbA2â†‘) Normal MCV & MCH Analizi ================= #
st.divider()
st.subheader("ğŸ©¸ Beta Talasemi TaÅŸÄ±yÄ±cÄ±larÄ±nda (HbA2â†‘) Normal MCV & MCH Analizi")

# 1. Hedef kitleyi belirle (Sadece A2 TaÅŸÄ±yÄ±cÄ±larÄ±)
target_tag = "HbA2â†‘ (B-thal Trait)"
subset_indices = work[work["VARIANT_TAG"] == target_tag].index

if not subset_indices.empty:
    # 2. Sadece bu gruba ait ve sadece MCV/MCH satÄ±rlarÄ±nÄ± al
    relevant_tests = ["Hemogram/MCV", "Hemogram/MCH"]
    
    # Ana veriden (work) ilgili satÄ±rlarÄ± Ã§ek
    subset_data = work.loc[subset_indices]
    subset_data = subset_data[subset_data["TETKIK_ISMI"].isin(relevant_tests) & subset_data["__VAL_NUM__"].notna()]

    if not subset_data.empty:
        # 3. Veriyi Pivotla: Her protokol bir satÄ±r olsun, MCV ve MCH sÃ¼tun olsun
        try:
            pivot_check = subset_data.pivot_table(
                index="PROTOKOL_NO", 
                columns="TETKIK_ISMI", 
                values="__VAL_NUM__"
            )
            
            # Kolon isimlerini kontrol et ve sadeleÅŸtir
            # (DosyanÄ±zdaki isimler 'Hemogram/MCV' ÅŸeklindeyse)
            if "Hemogram/MCV" in pivot_check.columns and "Hemogram/MCH" in pivot_check.columns:
                pivot_check = pivot_check.rename(columns={"Hemogram/MCV": "MCV", "Hemogram/MCH": "MCH"})
                
                # Sadece HER Ä°KÄ°SÄ° DE (MCV ve MCH) Ã¶lÃ§Ã¼lmÃ¼ÅŸ olanlarÄ± al
                valid_data = pivot_check.dropna(subset=["MCV", "MCH"])
                
                if not valid_data.empty:
                    # 4. KuralÄ± Uygula: Normal = MCV >= 80 VE MCH >= 27
                    normal_indices = valid_data[
                        (valid_data["MCV"] >= 80) & 
                        (valid_data["MCH"] >= 27)
                    ].index # Bu indexler PROTOKOL_NO'dur
                    
                    count_normal = len(normal_indices)
                    count_total = len(valid_data)
                    count_micro_hypo = count_total - count_normal
                    
                    # 5. Tabloyu OluÅŸtur
                    summary_df = pd.DataFrame({
                        "Durum": [
                            "Normal Ä°ndeksler (MCVâ‰¥80 ve MCHâ‰¥27)", 
                            "Mikrositik/Hipokromik (MCV<80 veya MCH<27)", 
                            "TOPLAM (Verisi Tam Olanlar)"
                        ],
                        "Hasta SayÄ±sÄ±": [count_normal, count_micro_hypo, count_total],
                        "Oran (%)": [
                            f"{(count_normal/count_total)*100:.1f}%", 
                            f"{(count_micro_hypo/count_total)*100:.1f}%", 
                            "100%"
                        ]
                    })

                    st.write(f"**{target_tag}** grubunda hem MCV hem MCH deÄŸeri bulunan hastalarÄ±n analizi:")
                    st.table(summary_df)
                    
                    # 6. Ä°ndirme Butonu (Sadece Normal Olanlar)
                    if count_normal > 0:
                        normal_patients_full = work[work["PROTOKOL_NO"].isin(normal_indices)].copy()
                        
                        csv_normal = normal_patients_full.to_csv(index=False).encode("utf-8-sig")
                        st.download_button(
                            "â¬‡ï¸ Normal Ä°ndeksli (Sessiz?) TaÅŸÄ±yÄ±cÄ±larÄ± Ä°ndir (CSV)",
                            data=csv_normal,
                            file_name="normal_indeksli_b_thal_tasiyicilari.csv",
                            mime="text/csv"
                        )
                else:
                    st.warning("HbA2 taÅŸÄ±yÄ±cÄ±larÄ±nda eÅŸleÅŸen MCV ve MCH verisi bulunamadÄ±.")
            else:
                st.warning("Bu grupta MCV veya MCH testlerinden biri eksik veya isimleri farklÄ±.")
                
        except Exception as e:
            st.error(f"Ä°ndeks analizi sÄ±rasÄ±nda hata: {e}")
    else:
        st.warning(f"'{target_tag}' grubu iÃ§in MCV/MCH verisi bulunamadÄ±.")
else:
    st.info(f"Veri setinde '{target_tag}' grubuna giren hasta bulunamadÄ±.")
# ================= YENÄ°: INTERMEDIA vs TRAIT AYRIM GRAFÄ°ÄÄ° (Ã–ZELLEÅTÄ°RÄ°LEBÄ°LÄ°R) ================= #
st.divider()
st.subheader("ğŸ“‰ Klinik AyrÄ±m: Beta Talasemi MinÃ¶r vs Ä°ntermedia")
st.caption("Bu grafik, 'MinÃ¶r' ve 'Ä°ntermedia' ÅŸÃ¼phesi olan hastalarÄ± **Hemoglobin (HGB)** ve **Fetal Hemoglobin (HbF)** dÃ¼zeylerine gÃ¶re ayÄ±rÄ±r.")

# --- GRAFÄ°K AYARLARI (BURAYI DEÄÄ°ÅTÄ°REBÄ°LÄ°RSÄ°NÄ°Z) ---
GRAPH_TITLE = "Beta Talasemi AyrÄ±mÄ±: HbF vs Hemoglobin DaÄŸÄ±lÄ±mÄ±"
X_LABEL = "Fetal Hemoglobin (HbF) %"
Y_LABEL = "Toplam Hemoglobin (HGB) g/dL"

# Renk Paleti (Ä°stediÄŸiniz renkleri buraya yazabilirsiniz)
CUSTOM_COLORS = {
    "HbA2â†‘ (B-thal Trait)": "blue",        # Klasik TaÅŸÄ±yÄ±cÄ±lar (Mavi)
    "Borderline HbA2": "cyan",             # SÄ±nÄ±rda Olanlar (AÃ§Ä±k Mavi/Turkuaz)
    
    "B-thal Intermedia (High A2/High F)": "red",      # Ä°ntermedia ÅÃ¼phesi (KÄ±rmÄ±zÄ±)
    "B-thal Intermedia (High F only)": "darkred",     # Ä°ntermedia ÅÃ¼phesi (Koyu KÄ±rmÄ±zÄ±)
    "B-thal Intermedia? (Mod. F + Severe Anemia)": "orange", # SÄ±nÄ±rda Ä°ntermedia (Turuncu)
    
    "Î´Î²-thal Trait": "green"               # Delta-Beta (YeÅŸil)
}
# -------------------------------------------------------

# 1. Analiz edilecek gruplarÄ± belirle (Renk listesinden otomatik alÄ±r)
target_variants = list(CUSTOM_COLORS.keys())

# 2. Veriyi HazÄ±rla
hgb_tests = ["Hemogram/HGB"]
f_tests   = ["F/", "HbF (%)", "Hb F", "Hb F (%)"]
relevant_tests = hgb_tests + f_tests

# Ä°lgili verileri ana tablodan Ã§ek
subset_graph = work[
    work["TETKIK_ISMI"].isin(relevant_tests) & 
    work["VARIANT_TAG"].isin(target_variants)
].copy()

if not subset_graph.empty:
    # Pivotla (Her protokol tek satÄ±r: SÃ¼tunlar -> HGB, HbF)
    subset_graph["TYPE"] = subset_graph["TETKIK_ISMI"].apply(lambda x: "HGB" if x in hgb_tests else "HbF")
    
    # SayÄ±sal deÄŸere Ã§evir (Garanti olsun)
    subset_graph["__VAL_NUM__"] = pd.to_numeric(subset_graph["__VAL_NUM__"], errors='coerce')
    
    pivot_graph = subset_graph.pivot_table(
        index=["PROTOKOL_NO", "VARIANT_TAG"], 
        columns="TYPE", 
        values="__VAL_NUM__"
    ).reset_index()

    # Hem HGB hem HbF verisi olanlarÄ± al (Yoksa grafik Ã§izilemez)
    graph_data = pivot_graph.dropna(subset=["HGB", "HbF"])

    if not graph_data.empty:
        # 3. GrafiÄŸi Ã‡iz (Matplotlib)
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Her grup iÃ§in ayrÄ± ayrÄ± noktalarÄ± (scatter) Ã§iz
        for var_name in graph_data["VARIANT_TAG"].unique():
            sub_g = graph_data[graph_data["VARIANT_TAG"] == var_name]
            
            # Renkleri ayarlarÄ±mÄ±zdan al
            c = CUSTOM_COLORS.get(var_name, "gray")
            
            # MinÃ¶rleri biraz daha ÅŸeffaf ve kÃ¼Ã§Ã¼k yap (KalabalÄ±k olduklarÄ± iÃ§in)
            is_trait = "Trait" in var_name or "Borderline" in var_name
            alpha = 0.5 if is_trait else 0.9
            size = 30 if is_trait else 80 
            
            ax.scatter(sub_g["HbF"], sub_g["HGB"], label=var_name, color=c, alpha=alpha, s=size, edgecolors='w')

        # 4. Referans Ã‡izgileri ve YazÄ±lar
        
        # HGB < 10 Ã§izgisi (Yatay - Anemi SÄ±nÄ±rÄ±)
        ax.axhline(y=10, color='black', linestyle='--', linewidth=1)
        
        # HbF > 10 Ã§izgisi (Dikey - Ä°ntermedia SÄ±nÄ±rÄ±)
        ax.axvline(x=10, color='black', linestyle='--', linewidth=1)
        
        # BÃ¶lgeleri Ä°simlendir
        # SaÄŸ Alt KÃ¶ÅŸe: DÃ¼ÅŸÃ¼k HGB, YÃ¼ksek F -> INTERMEDIA
        ax.text(graph_data["HbF"].max(), graph_data["HGB"].min(), "Ä°NTERMEDÄ°A BÃ–LGESÄ°", 
                ha='right', va='bottom', fontsize=10, fontweight='bold', color='red', alpha=0.5)
        
        # Sol Ãœst KÃ¶ÅŸe: YÃ¼ksek HGB, DÃ¼ÅŸÃ¼k F -> MINOR
        ax.text(0, graph_data["HGB"].max(), "MÄ°NÃ–R (TAÅIYICI) BÃ–LGESÄ°", 
                ha='left', va='top', fontsize=10, fontweight='bold', color='blue', alpha=0.5)

        # Eksen Ä°simleri ve BaÅŸlÄ±k (Sizin AyarlarÄ±nÄ±zdan)
        ax.set_xlabel(X_LABEL)
        ax.set_ylabel(Y_LABEL)
        ax.set_title(GRAPH_TITLE)
        
        # --- Ä°STEÄE BAÄLI: Eksen AralÄ±klarÄ±nÄ± Elle Ayarlamak Ä°Ã§in Yorumu KaldÄ±rÄ±n ---
        # ax.set_xlim(0, 20)  # HbF 0 ile 20 arasÄ±
        # ax.set_ylim(5, 18)  # HGB 5 ile 18 arasÄ±
        # --------------------------------------------------------------------------

        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left') # LejantÄ± dÄ±ÅŸarÄ± al
        ax.grid(True, alpha=0.3) # Izgara Ã§izgileri

        st.pyplot(fig)
        
        st.info("""
        **Grafik Yorumu:**
        * **KÄ±rmÄ±zÄ±/Turuncu Noktalar:** Genellikle saÄŸ alt kÃ¶ÅŸede toplanÄ±r (HbF YÃ¼ksek, Hb DÃ¼ÅŸÃ¼k). Bunlar klinik olarak daha ÅŸiddetli (Ä°ntermedia) olgulardÄ±r.
        * **Mavi/YeÅŸil Noktalar:** Sol Ã¼stte toplanÄ±r. Bunlar klasik taÅŸÄ±yÄ±cÄ±lardÄ±r.
        """)
    else:
        st.warning("Grafik Ã§izmek iÃ§in seÃ§ilen gruplarda hem HGB hem de HbF sonucu olan hasta bulunamadÄ±.")
else:
    st.info("Listede grafiklenecek varyant grubu (A2, Borderline, Intermedia, Delta-Beta) verisi bulunamadÄ±.")
st.caption("Not: Kan Grubu ve Anormal Hb analizleri normalize edilerek hesaplanÄ±r; ham yazÄ±mlar ayrÄ±ca CSV olarak indirilebilir.")

# ================================================================================= #
#             ğŸ¤– MAKÄ°NE Ã–ÄRENMESÄ° (ML) MODÃœLÃœ (HEDEF SEÃ‡Ä°MÄ° + GÃœVEN SKORU)          #
# ================================================================================= #
st.divider()
st.header("ğŸ¤– Yapay Zeka (ML) LaboratuvarÄ±")
st.caption("Modelin girdilerini (Parametreler) ve Ã§Ä±ktÄ±larÄ±nÄ± (HastalÄ±k TanÄ±larÄ±) seÃ§erek Ã¶zelleÅŸtirilmiÅŸ eÄŸitim yapÄ±n.")

# --- ML ModÃ¼lÃ¼nÃ¼ Aktif Et ---
if st.checkbox("Yapay Zeka LaboratuvarÄ±nÄ± AÃ§", value=False):
    
    # 1. GENÄ°ÅLETÄ°LMÄ°Å PARAMETRE LÄ°STESÄ°
    HEMO_PARAMS = [
        "Hemogram/HGB", "Hemogram/RBC", "Hemogram/MCV", "Hemogram/MCH", "Hemogram/MCHC",
        "Hemogram/RDW", "Hemogram/RDW-SD", 
        "Hemogram/HCT", "Hemogram/PLT", "Hemogram/WBC", "Hemogram/MPV", "Hemogram/PCT", "Hemogram/PDW",
        "Hemogram/NEU", "Hemogram/NEU%", "Hemogram/LYM", "Hemogram/LYM%",
        "Hemogram/MONO", "Hemogram/MONO%", "Hemogram/EOS", "Hemogram/EOS%",
        "Hemogram/BASO", "Hemogram/BASO%"
    ]
    
    HPLC_PARAMS = [
        "HbA2 (%)", "A2/", 
        "HbF (%)", "F/", 
        "HbS (%)", "S/", 
        "HbC (%)", "C/", 
        "HbD (%)", "D/", 
        "HbE (%)", "E/",
        "Talasemi(HPLC) (A0)/", "HbA", "HbA (%)"
    ]
    
    OTHER_PARAMS = ["YAS", "CINSIYET"] 
    ALL_AVAILABLE_PARAMS = HEMO_PARAMS + HPLC_PARAMS

    # 2. KullanÄ±cÄ± ArayÃ¼zÃ¼
    col_ml_settings, col_ml_main = st.columns([1, 2])
    
    with col_ml_settings:
        st.subheader("âš™ï¸ Model AyarlarÄ±")
        
        # A) Algoritma SeÃ§imi
        algo_choice = st.radio(
            "Algoritma SeÃ§in:",
            ["Random Forest", "XGBoost", "LightGBM", "CatBoost"],
            index=1
        )
        
        st.divider()
        
        # B) Parametre SeÃ§imi (GÄ°RDÄ°LER / X)
        st.write("**1. GÄ°RDÄ°LER: Hangi verilerle tahmin yapÄ±lsÄ±n?**")
        feature_mode = st.radio(
            "Parametre Grubu:",
            ["TÃ¼mÃ¼ (Full Hemogram + HPLC + YaÅŸ/Cinsiyet)", "Sadece Hemogram", "Sadece HPLC", "Ã–zel SeÃ§im"],
            index=0
        )
        
        if feature_mode == "TÃ¼mÃ¼ (Full Hemogram + HPLC + YaÅŸ/Cinsiyet)":
            selected_features = ALL_AVAILABLE_PARAMS
        elif feature_mode == "Sadece Hemogram":
            selected_features = HEMO_PARAMS
        elif feature_mode == "Sadece HPLC":
            selected_features = HPLC_PARAMS
        else:
            selected_features = st.multiselect("Parametreleri Ä°ÅŸaretleyin:", ALL_AVAILABLE_PARAMS, default=ALL_AVAILABLE_PARAMS)
            
        st.divider()

        # C) Hedef SeÃ§imi (Ã‡IKTILAR / y) -- YENÄ° Ã–ZELLÄ°K --
        st.write("**2. Ã‡IKTILAR: Hangi tanÄ±larÄ± tahmin edeyim?**")
        
        # Mevcut tÃ¼m etiketleri bul
        if "VARIANT_TAG" in work.columns:
            available_targets = sorted(work["VARIANT_TAG"].dropna().unique().tolist())
            
            # VarsayÄ±lan olarak hepsini seÃ§, ama kullanÄ±cÄ± Ã§Ä±karabilir
            selected_targets = st.multiselect(
                "Tahmin Edilecek HastalÄ±k GruplarÄ±nÄ± SeÃ§in:",
                options=available_targets,
                default=available_targets,
                help="Listeden Ã§Ä±kardÄ±ÄŸÄ±nÄ±z hastalÄ±k grubuna sahip hastalar, model eÄŸitimine ve testine DAHÄ°L EDÄ°LMEYECEKTÄ°R."
            )
            
            st.caption(f"SeÃ§ili Gruplar: {len(selected_targets)} / {len(available_targets)}")
        else:
            st.error("Ã–nce yukarÄ±daki analizlerin tamamlanmasÄ± gerekir.")
            selected_targets = []
        
        st.divider()
        # D) BaÅŸlat Butonu
        start_training = st.button("ğŸš€ Modeli EÄŸit ve Test Et", type="primary", use_container_width=True)

    # 3. EÄŸitim ve Analiz SÃ¼reci
    with col_ml_main:
        if start_training:
            if not selected_features:
                st.error("LÃ¼tfen en az bir parametre seÃ§in.")
            elif not selected_targets:
                st.error("LÃ¼tfen en az bir hastalÄ±k grubu seÃ§in.")
            elif len(selected_targets) < 2:
                st.error("SÄ±nÄ±flandÄ±rma yapabilmek iÃ§in en az 2 farklÄ± grup seÃ§melisiniz.")
            elif "VARIANT_TAG" not in work.columns:
                st.error("Veri hazÄ±rlanmamÄ±ÅŸ.")
            else:
                # --- VERÄ° HAZIRLIÄI ---
                # Sadece seÃ§ili hedef gruplara (selected_targets) ait satÄ±rlarÄ± al
                labeled_data = work[work["VARIANT_TAG"].isin(selected_targets)].copy()
                
                if labeled_data.empty:
                    st.error("SeÃ§ilen filtrelere uygun veri kalmadÄ±.")
                else:
                    try:
                        # KÃ¼tÃ¼phaneler
                        from sklearn.model_selection import train_test_split
                        from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
                        from sklearn.preprocessing import LabelEncoder
                        import seaborn as sns
                        
                        # Algoritma YÃ¼kleyicileri
                        models = {}
                        try: from sklearn.ensemble import RandomForestClassifier; models["Random Forest"] = RandomForestClassifier
                        except: pass
                        try: from xgboost import XGBClassifier; models["XGBoost"] = XGBClassifier
                        except: pass
                        try: from lightgbm import LGBMClassifier; models["LightGBM"] = LGBMClassifier
                        except: pass
                        try: from catboost import CatBoostClassifier; models["CatBoost"] = CatBoostClassifier
                        except: pass

                        if algo_choice not in models:
                            st.error(f"{algo_choice} kÃ¼tÃ¼phanesi yÃ¼klÃ¼ deÄŸil.")
                            st.stop()

                        with st.spinner("Veri hazÄ±rlanÄ±yor..."):
                            # 1. Pivotlama (Uzun -> GeniÅŸ)
                            ml_subset = labeled_data[
                                labeled_data["TETKIK_ISMI"].isin(selected_features) & 
                                labeled_data["__VAL_NUM__"].notna()
                            ].copy()
                            
                            X = ml_subset.pivot_table(index="PROTOKOL_NO", columns="TETKIK_ISMI", values="__VAL_NUM__")
                            
                            # 2. YAÅ Ekleme
                            if "YAS" in work.columns:
                                age_series = labeled_data.drop_duplicates("PROTOKOL_NO").set_index("PROTOKOL_NO")["YAS"]
                                X = X.join(age_series, how="left")
                                X["YAS"] = X["YAS"].fillna(0)

                            # 3. CÄ°NSÄ°YET Ekleme (0/1 DÃ¶nÃ¼ÅŸÃ¼mÃ¼)
                            if "CINSIYET" in work.columns:
                                sex_series = labeled_data.drop_duplicates("PROTOKOL_NO").set_index("PROTOKOL_NO")["CINSIYET"]
                                sex_series = sex_series.astype(str).map(lambda x: 1 if x.lower().startswith(('e','m')) else 0)
                                X["CINSIYET_CODE"] = sex_series

                            # Eksikleri 0 ile doldur
                            X = X.fillna(0)
                            X = X.astype(float)
                            
                            # SÃ¼tun Ä°simlerini Temizle
                            new_cols = []
                            for col in X.columns:
                                clean_col = str(col).replace("%", "Pct").replace("/", "_").replace(" ", "_").replace("-", "_")
                                clean_col = re.sub(r'[^A-Za-z0-9_]', '', clean_col)
                                new_cols.append(clean_col)
                            X.columns = new_cols

                            # Hedef (y)
                            y_raw = labeled_data.drop_duplicates("PROTOKOL_NO").set_index("PROTOKOL_NO")["VARIANT_TAG"]
                            
                            # EÅŸleÅŸtirme
                            common = X.index.intersection(y_raw.index)
                            X = X.loc[common]
                            y_raw = y_raw.loc[common]
                            
                            # Yetersiz SÄ±nÄ±flarÄ± Temizle (<2 Ã¶rnek)
                            vc = y_raw.value_counts()
                            valid_classes = vc[vc >= 2].index
                            if len(vc[vc < 2]) > 0:
                                st.warning(f"âš ï¸ Åu nadir tanÄ±lar (<2 hasta) eÄŸitimden Ã§Ä±karÄ±ldÄ±: {list(vc[vc < 2].index)}")
                            
                            X = X[y_raw.isin(valid_classes)]
                            y_raw = y_raw[y_raw.isin(valid_classes)]
                            
                            # Label Encoding
                            le = LabelEncoder()
                            y = le.fit_transform(y_raw)
                            class_names = le.classes_
                        
                        # --- MODEL EÄÄ°TÄ°MÄ° ---
                        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
                        
                        clf = None
                        if algo_choice == "Random Forest":
                            clf = models[algo_choice](n_estimators=100, random_state=42)
                        elif algo_choice == "XGBoost":
                            clf = models[algo_choice](eval_metric='mlogloss', random_state=42)
                        elif algo_choice == "LightGBM":
                            clf = models[algo_choice](random_state=42, verbose=-1)
                        elif algo_choice == "CatBoost":
                            clf = models[algo_choice](verbose=0, random_state=42)

                        with st.spinner(f"{algo_choice} modeli eÄŸitiliyor..."):
                            clf.fit(X_train, y_train)
                            y_pred = clf.predict(X_test)
                            
                            # OlasÄ±lÄ±klarÄ± al (GÃ¼ven Skoru iÃ§in)
                            try:
                                y_proba = clf.predict_proba(X_test)
                                confidence = np.max(y_proba, axis=1) * 100
                            except:
                                confidence = [0] * len(y_pred)

                            acc = accuracy_score(y_test, y_pred)
                        
                        # --- SONUÃ‡ EKRANI ---
                        st.success(f"âœ… **{algo_choice}** BaÅŸarÄ± OranÄ±: **%{acc*100:.2f}**")
                        
                        # Sekmeler
                        tab_imp, tab_cm, tab_rep, tab_pred = st.tabs([
                            "ğŸ“Š Ã–zellik Ã–nemi", 
                            "ğŸ¯ KarmaÅŸÄ±klÄ±k Matrisi", 
                            "ğŸ“ DetaylÄ± Rapor",
                            "ğŸ” Tahmin SonuÃ§larÄ± (GÃ¼ven Skoru)"
                        ])
                        
                        with tab_imp:
                            try:
                                importances = clf.feature_importances_
                                feature_imp = pd.Series(importances, index=X.columns).sort_values(ascending=False)
                                fig_imp, ax_imp = plt.subplots(figsize=(10, 8))
                                feature_imp.head(20).plot.bar(ax=ax_imp, color="#87CEEB")
                                ax_imp.set_title(f"{algo_choice} Ä°Ã§in En Ã–nemli 20 Parametre")
                                plt.xticks(rotation=45, ha='right')
                                st.pyplot(fig_imp)
                            except: st.warning("Ã‡izilemedi.")

                        with tab_cm:
                            unique_indices = sorted(list(set(y_test) | set(y_pred)))
                            unique_names_present = [class_names[i] for i in unique_indices]
                            fig_cm, ax_cm = plt.subplots(figsize=(12, 8))
                            cm = confusion_matrix(y_test, y_pred, labels=unique_indices)
                            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                                        xticklabels=unique_names_present, yticklabels=unique_names_present, ax=ax_cm)
                            plt.ylabel('GerÃ§ek TanÄ±')
                            plt.xlabel('Modelin Tahmini')
                            plt.xticks(rotation=90)
                            st.pyplot(fig_cm)

                        with tab_rep:
                            report = classification_report(y_test, y_pred, labels=unique_indices, target_names=unique_names_present, output_dict=True, zero_division=0)
                            st.dataframe(pd.DataFrame(report).transpose())
                            
                        with tab_pred:
                            # --- GÃœVEN SKORU ANALÄ°ZÄ° (GÃœNCELLENDÄ°) ---
                            st.markdown("### ğŸ¯ GÃ¼ven Skoru ve Klinik GÃ¼venilirlik Analizi")
                            
                            pred_df = pd.DataFrame({
                                "Protokol No": X_test.index,
                                "GerÃ§ek TanÄ±": [class_names[i] for i in y_test],
                                "YZ Tahmini": [class_names[i] for i in y_pred],
                                "GÃ¼ven Skoru (%)": confidence
                            })
                            
                            pred_df["Durum"] = np.where(pred_df["GerÃ§ek TanÄ±"] == pred_df["YZ Tahmini"], "âœ… DoÄŸru", "âŒ HatalÄ±")
                            
                            # --- Ä°STATÄ°STÄ°K HESAPLAMA ---
                            threshold = 90.0 # %90 ve Ã¼zeri gÃ¼ven
                            
                            # 1. YÃ¼ksek GÃ¼venli Hastalar
                            high_conf_df = pred_df[pred_df["GÃ¼ven Skoru (%)"] >= threshold]
                            n_high = len(high_conf_df)
                            n_total_test = len(pred_df)
                            ratio_high = (n_high / n_total_test) * 100 if n_total_test > 0 else 0
                            
                            # 2. Bu gruptaki DoÄŸruluk (Accuracy)
                            if n_high > 0:
                                n_correct_high = len(high_conf_df[high_conf_df["Durum"] == "âœ… DoÄŸru"])
                                acc_high = (n_correct_high / n_high) * 100
                            else:
                                acc_high = 0
                                
                            # 3. Kalan %20'lik dilim (Test Seti) iÃ§in Genel DoÄŸruluk
                            general_accuracy = acc * 100

                            # --- METRÄ°KLERÄ° GÃ–STER ---
                            m1, m2, m3, m4 = st.columns(4)
                            m1.metric(label="Toplam Test Edilen Hasta", value=n_total_test)
                            m2.metric(label="Genel DoÄŸruluk (TÃ¼m Test Seti)", value=f"%{general_accuracy:.2f}")
                            m3.metric(label=f"YÃ¼ksek GÃ¼venli (>%{threshold})", value=f"{n_high} kiÅŸi", delta=f"%{ratio_high:.1f} Kapsama")
                            m4.metric(label="YÃ¼ksek GÃ¼venli Grubun DoÄŸruluÄŸu", value=f"%{acc_high:.2f}", help="Modelin '%90'dan fazla eminim' dediÄŸi vakalardaki baÅŸarÄ±sÄ±.")
                            
                            st.success(f"""
                            **ğŸ“ Makale Ä°Ã§in Bulgular CÃ¼mlesi:**
                            
                            "GeliÅŸtirilen yapay zeka modeli, test setindeki vakalarÄ±n **%{ratio_high:.1f}**'ini (n={n_high}) **%{threshold}** ve Ã¼zeri bir gÃ¼ven skoru (confidence score) ile sÄ±nÄ±flandÄ±rmÄ±ÅŸtÄ±r. 
                            Modelin kendinden emin olduÄŸu bu yÃ¼ksek gÃ¼venli grupta, tanÄ±sal doÄŸruluk oranÄ± (accuracy) **%{acc_high:.2f}** olarak tespit edilmiÅŸtir. TÃ¼m test seti Ã¼zerindeki genel doÄŸruluk oranÄ± ise **%{general_accuracy:.2f}**'dir."
                            """)
                            
                            st.divider()
                            st.write("#### DetaylÄ± Hasta Listesi")
                            pred_df = pred_df.sort_values("Durum", ascending=True)
                            st.dataframe(pred_df.style.apply(lambda x: ['background-color: #ffcccc' if x['Durum'] == 'âŒ HatalÄ±' else '' for i in x], axis=1), use_container_width=True)
                            
                            csv_pred = pred_df.to_csv(index=False).encode("utf-8-sig")
                            st.download_button("â¬‡ï¸ Tahmin SonuÃ§larÄ±nÄ± Ä°ndir (CSV)", csv_pred, "yz_tahmin_sonuclari.csv", "text/csv")

                    except Exception as e:
                        st.error(f"Hata oluÅŸtu: {e}")
                        st.info("Hata DetayÄ±: Veri tipleri veya sÃ¼tun isimleri uyumsuz olabilir.")
